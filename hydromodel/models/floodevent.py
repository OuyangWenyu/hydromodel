"""
Author: Wenyu Ouyang
Date: 2025-01-19 18:05:00
LastEditTime: 2025-08-01 14:26:02
LastEditors: Wenyu Ouyang
Description: æµåŸŸåœºæ¬¡æ•°æ®å¤„ç†ç±» - ç»§æ‰¿è‡ªSelfMadeHydroDataset
FilePath: \hydromodel\hydromodel\models\floodevent.py
Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.
"""

import pandas as pd
import numpy as np
import os
import xarray as xr
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from hydrodatasource.utils.utils import streamflow_unit_conv
from hydrodatasource.reader.data_source import SelfMadeHydroDataset
from hydrodatasource.configs.config import CACHE_DIR
from hydromodel.models.consts import OBS_FLOW, NET_RAIN
from hydromodel.models.common_utils import (
    read_basin_area_safe,
)


class FloodEventDatasource(SelfMadeHydroDataset):
    """
    Flood event dataset processing class

    Inherits from SelfMadeHydroDataset, specifically designed for
    processing individual flood event data, including event extraction functions.
    """

    def __init__(
        self,
        data_path: str,
        dataset_name: str = "songliaorrevents",
        time_unit: Optional[List[str]] = None,
        **kwargs,
    ):
        """
        Initialize the flood event dataset.

        Parameters
        ----------
        data_path : str
            Path to the data.
        dataset_name : str, optional
            Name of the dataset.
        time_unit : list of str, optional
            List of time units, default is ["3h"].
        **kwargs
            Additional keyword arguments passed to the parent class.
        """
        if time_unit is None:
            time_unit = ["3h"]
        super().__init__(
            data_path=data_path,
            download=False,
            time_unit=time_unit,
            dataset_name=dataset_name,
            **kwargs,
        )

    def extract_flood_events(
        self, df: pd.DataFrame
    ) -> List[Tuple[np.ndarray, np.ndarray, str]]:
        """
        ä»æ•°æ®æ¡†ä¸­æå–æ´ªæ°´äº‹ä»¶ï¼Œè¿”å›å‡€é›¨ã€å¾„æµæ•°ç»„å’Œæ´ªå³°æ—¥æœŸ

        Args:
            df: ç«™ç‚¹æ•°æ®æ¡†
            station_id: ç«™ç‚¹IDï¼ˆç”¨äºæ‰“å°ä¿¡æ¯ï¼‰

        Returns:
            List[Tuple[np.ndarray, np.ndarray, str]]: (å‡€é›¨æ•°ç»„, å¾„æµæ•°ç»„, æ´ªå³°æ—¥æœŸ) åˆ—è¡¨
        """
        events: List[Tuple[np.ndarray, np.ndarray, str]] = []
        # æ‰¾åˆ°è¿ç»­çš„flood_event > 0åŒºé—´
        flood_mask = df["flood_event"] > 0

        if not flood_mask.any():
            return events

        # æ‰¾è¿ç»­åŒºé—´
        in_event = False
        start_idx = None

        for idx, is_flood in enumerate(flood_mask):
            if is_flood and not in_event:
                start_idx = idx
                in_event = True
            elif not is_flood and in_event:
                # äº‹ä»¶ç»“æŸï¼Œæå–æ•°æ®
                event_data = df.iloc[start_idx:idx]
                net_rain = event_data["net_rain"].values
                inflow = event_data["inflow"].values
                event_times = event_data["time"].values

                # åŸºæœ¬éªŒè¯
                if (
                    len(net_rain) > 0
                    and len(inflow) > 0
                    and np.nansum(inflow) > 1e-6
                ):
                    # è·å–åœºæ¬¡å¼€å§‹å’Œç»“æŸæ—¶é—´
                    start_time = event_times[0]
                    end_time = event_times[-1]

                    # è½¬æ¢ä¸ºåä½æ•°å­—æ ¼å¼ (YYYYMMDDHH)
                    def time_to_ten_digits(time_obj):
                        """å°†æ—¶é—´å¯¹è±¡è½¬æ¢ä¸ºåä½æ•°å­—æ ¼å¼ YYYYMMDDHH"""
                        if isinstance(time_obj, np.datetime64):
                            # å¦‚æœæ˜¯numpy datetime64å¯¹è±¡
                            return (
                                time_obj.astype("datetime64[h]")
                                .astype(str)
                                .replace("-", "")
                                .replace("T", "")
                                .replace(":", "")
                            )
                        elif hasattr(time_obj, "strftime"):
                            # å¦‚æœæ˜¯datetimeå¯¹è±¡
                            return time_obj.strftime("%Y%m%d%H")
                        else:
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
                            try:
                                from datetime import datetime

                                if isinstance(time_obj, str):
                                    dt = datetime.fromisoformat(
                                        time_obj.replace("Z", "+00:00")
                                    )
                                    return dt.strftime("%Y%m%d%H")
                                else:
                                    return "0000000000"  # é»˜è®¤å€¼
                            except Exception:
                                return "0000000000"  # é»˜è®¤å€¼

                    start_digits = time_to_ten_digits(start_time)
                    end_digits = time_to_ten_digits(end_time)

                    # ç»„åˆæˆåœºæ¬¡åç§°ï¼šèµ·å§‹æ—¶é—´_ç»“æŸæ—¶é—´
                    event_name = f"{start_digits}_{end_digits}"

                    events.append((net_rain, inflow, event_name))

                in_event = False
        return events

    def create_event_dict(
        self,
        net_rain: np.ndarray,
        inflow: np.ndarray,
        event_name: str,
        include_peak_obs: bool = True,
    ) -> Optional[Dict]:
        """
        å°†å‡€é›¨å’Œå¾„æµæ•°ç»„è½¬æ¢ä¸ºæ ‡å‡†äº‹ä»¶å­—å…¸æ ¼å¼

        Parameters
        ----------
        net_rain: np.ndarray
            å‡€é›¨æ•°ç»„
        inflow: np.ndarray
            å¾„æµæ•°ç»„
        event_name: str
            æ´ªå³°æ—¥æœŸï¼ˆ8ä½æ•°å­—æ ¼å¼ï¼‰
        include_peak_obs: bool
            æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼

        Returns
        -------
            Dict: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸ï¼Œä¸uh_utils.pyå®Œå…¨å…¼å®¹
        """
        try:
            # è®¡ç®—æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
            valid_rain_mask = ~np.isnan(net_rain) & (net_rain > 0)
            m_eff = np.sum(valid_rain_mask)

            if m_eff == 0:
                return None

            # éªŒè¯å¾„æµæ•°æ®
            if np.nansum(inflow) < 1e-6:
                return None

            # åˆ›å»ºæ ‡å‡†æ ¼å¼å­—å…¸ï¼ˆä¸uh_utils.pyæœŸæœ›çš„keyå®Œå…¨ä¸€è‡´ï¼‰
            event_dict = {
                NET_RAIN: net_rain,  # æœ‰æ•ˆé™é›¨ï¼ˆå‡€é›¨ï¼‰
                OBS_FLOW: inflow,  # è§‚æµ‹å¾„æµ
                "m_eff": m_eff,  # æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
                "n_specific": len(net_rain),  # å•ä½çº¿é•¿åº¦
                "filepath": f"event_{event_name}.csv",  # é¿å…KeyError
            }

            # æ·»åŠ æ´ªå³°è§‚æµ‹å€¼
            if include_peak_obs:
                peak_flow = np.nanmax(inflow)
                if peak_flow < 1e-6:
                    return {}
                event_dict["peak_obs"] = peak_flow

            return event_dict

        except Exception:
            return {}

    def _load_1basin_flood_events(
        self,
        station_id: Optional[str] = None,
        flow_unit: str = "mm/3h",
        include_peak_obs: bool = True,
        verbose: bool = True,
    ) -> Optional[List[Dict]]:
        """
        åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®

        Parameters
        ----------
        station_id:
            æŒ‡å®šç«™ç‚¹IDï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰ç«™ç‚¹
        flow_unit
            Unit of streamflow, default is "mm/3h".
        include_peak_obs:
            æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼
        verbose:
            æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns
        -------
            List[Dict]: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼Œä¸ç°æœ‰ç®—æ³•å®Œå…¨å…¼å®¹
        """
        # è·å–æµåŸŸé¢ç§¯
        basin_area_km2 = None

        if station_id:
            basin_area_km2 = read_basin_area_safe(self, station_id, verbose)
        else:
            basin_area_km2 = None

        try:
            if verbose:
                print("ğŸ”„ æ­£åœ¨åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®...")
                if station_id:
                    print(f"   æŒ‡å®šç«™ç‚¹: {station_id}")

            all_events = []
            total_events = 0

            xr_ds = self.read_ts_xrdataset(
                gage_id_lst=[station_id],
                t_range=["1960-01-01", "2024-12-31"],
                var_lst=["inflow", "net_rain", "flood_event"],
                # recache=True,
            )["3h"]

            xr_ds["inflow"] = streamflow_unit_conv(
                xr_ds[["inflow"]],
                target_unit=flow_unit,
                area=basin_area_km2,
            )["inflow"]
            df = xr_ds.to_dataframe()
            if df is None:
                return None

            # æå–æ´ªæ°´äº‹ä»¶
            flood_events = self.extract_flood_events(
                df.loc[station_id].reset_index()
            )

            if not flood_events:
                if verbose:
                    print(f"  âš ï¸  {station_id}: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ´ªæ°´äº‹ä»¶")
                return None

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            station_event_count = 0
            for net_rain, inflow, event_name in flood_events:
                event_dict = self.create_event_dict(
                    net_rain, inflow, event_name, include_peak_obs
                )
                if event_dict is not None:
                    all_events.append(event_dict)
                    station_event_count += 1

            if verbose and station_event_count > 0:
                print(
                    f"  âœ… {station_id}: æˆåŠŸå¤„ç† {station_event_count} ä¸ªæ´ªæ°´äº‹ä»¶"
                )
                total_events += station_event_count

            if not all_events:
                if verbose:
                    print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ´ªæ°´äº‹ä»¶æ•°æ®")
                return None

            if verbose:
                print(f"âœ… æ€»å…±æˆåŠŸåŠ è½½ {len(all_events)} ä¸ªæ´ªæ°´äº‹ä»¶")

            return all_events

        except Exception as e:
            if verbose:
                print(f"âŒ åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def parse_augmented_file_metadata(self, augmented_file_path: str) -> Dict:
        """
        è§£æå¢å¼ºæ–‡ä»¶çš„å…ƒä¿¡æ¯

        Parameters
        ----------
        augmented_file_path : str
            å¢å¼ºæ–‡ä»¶çš„è·¯å¾„

        Returns
        -------
        Dict
            åŒ…å«æºåœºæ¬¡ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…æ‹¬èµ·å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´ã€æºæ–‡ä»¶åç­‰
        """
        metadata = {}

        with open(augmented_file_path, "r", encoding="utf-8") as f:
            for line in f:
                if line.startswith("#"):
                    if "Source:" in line:
                        source_file = line.split("Source:")[1].strip()
                        metadata["source_file"] = source_file
                        # ä»æºæ–‡ä»¶åæå–èµ·å§‹æ—¶é—´
                        if "event_" in source_file and ".csv" in source_file:
                            time_part = source_file.replace(
                                "event_", ""
                            ).replace(".csv", "")
                            if "_" in time_part:
                                start_time_str, end_time_str = time_part.split(
                                    "_"
                                )
                                metadata["original_start_time"] = (
                                    start_time_str
                                )
                                metadata["original_end_time"] = end_time_str
                    elif "Start Time:" in line:
                        metadata["augmented_start_time"] = line.split(
                            "Start Time:"
                        )[1].strip()
                    elif "End Time:" in line:
                        metadata["augmented_end_time"] = line.split(
                            "End Time:"
                        )[1].strip()
                    elif "Scale Factor:" in line:
                        metadata["scale_factor"] = float(
                            line.split("Scale Factor:")[1].strip()
                        )
                    elif "Sample ID:" in line:
                        metadata["sample_id"] = int(
                            line.split("Sample ID:")[1].strip()
                        )
                else:
                    break

        return metadata

    def get_warmup_period_data(
        self,
        original_start_time: str,
        original_end_time: str,
        station_id: str,
        warmup_hours: int = 240,
    ) -> Optional[pd.DataFrame]:
        """
        è·å–åŸå§‹åœºæ¬¡å‰é¢çš„é¢„çƒ­æœŸæ•°æ®

        Parameters
        ----------
        original_start_time : str
            åŸå§‹åœºæ¬¡èµ·å§‹æ—¶é—´ (YYYYMMDDHHæ ¼å¼)
        original_end_time : str
            åŸå§‹åœºæ¬¡ç»“æŸæ—¶é—´ (YYYYMMDDHHæ ¼å¼)
        station_id : str
            ç«™ç‚¹ID
        warmup_hours : int, optional
            é¢„çƒ­æœŸå°æ—¶æ•°ï¼Œé»˜è®¤240å°æ—¶(10å¤©)

        Returns
        -------
        Optional[pd.DataFrame]
            é¢„çƒ­æœŸæ•°æ®ï¼ŒåŒ…å«time, net_rain, inflowåˆ—
        """
        try:
            # è§£ææ—¶é—´
            start_dt = datetime.strptime(original_start_time, "%Y%m%d%H")
            warmup_start = start_dt - timedelta(hours=warmup_hours)
            warmup_end = start_dt - timedelta(hours=3)

            # è¯»å–é¢„çƒ­æœŸæ•°æ®
            xr_ds = self.read_ts_xrdataset(
                gage_id_lst=[station_id],
                t_range=[
                    warmup_start.strftime("%Y-%m-%d %H"),
                    warmup_end.strftime("%Y-%m-%d %H"),
                ],
                var_lst=["inflow", "net_rain"],
            )["3h"]

            if xr_ds is None:
                return None

            # è½¬æ¢ä¸ºDataFrame
            df = xr_ds.to_dataframe().reset_index()
            df = df[df["basin"] == station_id].copy()

            # é‡å‘½ååˆ—ä»¥åŒ¹é…å¢å¼ºæ–‡ä»¶æ ¼å¼
            df = df.rename(columns={"inflow": "obs_discharge"})
            df["gen_discharge"] = df["obs_discharge"]

            return df[["time", "net_rain", "gen_discharge", "obs_discharge"]]
        except Exception as e:
            print(f"è·å–é¢„çƒ­æœŸæ•°æ®å¤±è´¥: {e}")
            return None

    def adjust_warmup_time_to_augmented_year(
        self, warmup_df: pd.DataFrame, augmented_start_time: str
    ) -> pd.DataFrame:
        """
        è°ƒæ•´é¢„çƒ­æœŸæ•°æ®çš„å¹´ä»½åˆ°å¢å¼ºæ•°æ®çš„å¹´ä»½

        Parameters
        ----------
        warmup_df : pd.DataFrame
            é¢„çƒ­æœŸæ•°æ®
        augmented_start_time : str
            å¢å¼ºæ•°æ®çš„èµ·å§‹æ—¶é—´ (YYYYMMDDHHæ ¼å¼)

        Returns
        -------
        pd.DataFrame
            è°ƒæ•´å¹´ä»½åçš„é¢„çƒ­æœŸæ•°æ®
        """
        df = warmup_df.copy()

        # è·å–å¢å¼ºæ•°æ®çš„å¹´ä»½
        aug_year = int(augmented_start_time[:4])

        # è°ƒæ•´æ—¶é—´åˆ—çš„å¹´ä»½
        df["time"] = pd.to_datetime(df["time"])
        df["time"] = df["time"].apply(lambda x: x.replace(year=aug_year))

        return df

    def concatenate_warmup_and_augmented_data(
        self, warmup_df: pd.DataFrame, augmented_file_path: str
    ) -> pd.DataFrame:
        """
        æ‹¼æ¥é¢„çƒ­æœŸæ•°æ®å’Œå¢å¼ºåœºæ¬¡æ•°æ®

        Parameters
        ----------
        warmup_df : pd.DataFrame
            é¢„çƒ­æœŸæ•°æ®
        augmented_file_path : str
            å¢å¼ºæ–‡ä»¶è·¯å¾„

        Returns
        -------
        pd.DataFrame
            æ‹¼æ¥åçš„å®Œæ•´æ•°æ®
        """
        # è¯»å–å¢å¼ºæ•°æ®
        aug_df = pd.read_csv(augmented_file_path, comment="#")
        aug_df["time"] = pd.to_datetime(aug_df["time"])

        # æ‹¼æ¥æ•°æ®
        combined_df = pd.concat([warmup_df, aug_df], ignore_index=True)
        combined_df = combined_df.sort_values("time").reset_index(drop=True)

        return combined_df

    def process_augmented_files_to_timeseries(
        self,
        station_id: str,
        augmented_file_indices: List[int],
        augmented_files_dir: str,
        warmup_hours: int = 240,
        time_unit: str = "3h",
    ) -> Optional[str]:
        """
        æ‰¹é‡å¤„ç†å¢å¼ºæ–‡ä»¶ï¼Œç”Ÿæˆé•¿æ—¶é—´åºåˆ—æ•°æ®å¹¶ä¿å­˜ä¸ºncæ–‡ä»¶

        Parameters
        ----------
        station_id : str
            ç«™ç‚¹ID
        augmented_file_indices : List[int]
            è¦å¤„ç†çš„å¢å¼ºæ–‡ä»¶ç¼–å·åˆ—è¡¨
        augmented_files_dir : str
            å¢å¼ºæ–‡ä»¶æ‰€åœ¨ç›®å½•
        warmup_hours : int, optional
            é¢„çƒ­æœŸå°æ—¶æ•°ï¼Œé»˜è®¤240å°æ—¶
        time_unit : str, optional
            æ—¶é—´å•ä½ï¼Œé»˜è®¤"3h"

        Returns
        -------
        Optional[str]
            ç”Ÿæˆçš„ncæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        all_timeseries_data = []

        # è·å–ç›®å½•ä¸‹æ‰€æœ‰å¢å¼ºæ–‡ä»¶
        aug_files = [
            f
            for f in os.listdir(augmented_files_dir)
            if f.endswith(".csv") and "aug_" in f
        ]
        aug_files.sort()

        # ç­›é€‰æŒ‡å®šç¼–å·çš„æ–‡ä»¶
        selected_files = []
        for idx in augmented_file_indices:
            matching_files = [
                f for f in aug_files if f"aug_{idx:04d}.csv" in f
            ]
            selected_files.extend(matching_files)

        if not selected_files:
            print(f"æœªæ‰¾åˆ°æŒ‡å®šç¼–å·çš„å¢å¼ºæ–‡ä»¶: {augmented_file_indices}")
            return None

        print(f"å¤„ç† {len(selected_files)} ä¸ªå¢å¼ºæ–‡ä»¶...")

        for file_name in selected_files:
            file_path = os.path.join(augmented_files_dir, file_name)

            try:
                # è§£æå…ƒä¿¡æ¯
                metadata = self.parse_augmented_file_metadata(file_path)

                if "original_start_time" not in metadata:
                    print(f"è·³è¿‡æ–‡ä»¶ {file_name}: æ— æ³•è§£æåŸå§‹æ—¶é—´ä¿¡æ¯")
                    continue

                # è·å–é¢„çƒ­æœŸæ•°æ®
                warmup_df = self.get_warmup_period_data(
                    metadata["original_start_time"],
                    metadata["original_end_time"],
                    station_id,
                    warmup_hours,
                )

                if warmup_df is None:
                    print(f"è·³è¿‡æ–‡ä»¶ {file_name}: æ— æ³•è·å–é¢„çƒ­æœŸæ•°æ®")
                    continue

                # è°ƒæ•´é¢„çƒ­æœŸæ—¶é—´
                warmup_df = self.adjust_warmup_time_to_augmented_year(
                    warmup_df, metadata["augmented_start_time"]
                )

                # æ‹¼æ¥æ•°æ®
                combined_df = self.concatenate_warmup_and_augmented_data(
                    warmup_df, file_path
                )

                all_timeseries_data.append(combined_df)

            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")
                continue

        if not all_timeseries_data:
            print("æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ•°æ®")
            return None

        # åˆå¹¶æ‰€æœ‰æ—¶é—´åºåˆ—æ•°æ®
        full_timeseries = pd.concat(all_timeseries_data, ignore_index=True)
        full_timeseries = full_timeseries.sort_values("time").reset_index(
            drop=True
        )

        # è½¬æ¢ä¸ºxarray Dataset
        xr_ds = self.create_xarray_dataset_from_timeseries(
            full_timeseries, station_id, time_unit
        )

        # ä¿å­˜åˆ°cacheç›®å½•
        cache_file_path = self.save_augmented_timeseries_to_cache(
            xr_ds, station_id, time_unit
        )

        return cache_file_path

    def create_xarray_dataset_from_timeseries(
        self, df: pd.DataFrame, station_id: str, time_unit: str = "3h"
    ) -> xr.Dataset:
        """
        å°†æ—¶é—´åºåˆ—DataFrameè½¬æ¢ä¸ºxarray Datasetæ ¼å¼

        Parameters
        ----------
        df : pd.DataFrame
            æ—¶é—´åºåˆ—æ•°æ®
        station_id : str
            ç«™ç‚¹ID
        time_unit : str, optional
            æ—¶é—´å•ä½ï¼Œé»˜è®¤"3h"

        Returns
        -------
        xr.Dataset
            xarrayæ ¼å¼çš„æ•°æ®é›†
        """
        # åˆ›å»ºxarray Dataset
        ds = xr.Dataset(
            {
                "inflow": (
                    ["time", "basin"],
                    df[["obs_discharge"]].values.reshape(-1, 1),
                ),
                "net_rain": (
                    ["time", "basin"],
                    df[["net_rain"]].values.reshape(-1, 1),
                ),
            },
            coords={"time": df["time"].values, "basin": [station_id]},
        )

        # æ·»åŠ å±æ€§
        ds.attrs["description"] = "Augmented hydrological time series data"
        ds.attrs["station_id"] = station_id
        ds.attrs["time_unit"] = time_unit
        ds.attrs["creation_time"] = datetime.now().isoformat()

        return ds

    def save_augmented_timeseries_to_cache(
        self, ds: xr.Dataset, station_id: str, time_unit: str = "3h"
    ) -> str:
        """
        å°†å¢å¼ºæ—¶é—´åºåˆ—æ•°æ®ä¿å­˜åˆ°cacheç›®å½•

        Parameters
        ----------
        ds : xr.Dataset
            è¦ä¿å­˜çš„æ•°æ®é›†
        station_id : str
            ç«™ç‚¹ID
        time_unit : str, optional
            æ—¶é—´å•ä½ï¼Œé»˜è®¤"3h"

        Returns
        -------
        str
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # æ„é€ æ–‡ä»¶åï¼Œå‚è€ƒåŸæœ‰çš„å‘½åè§„åˆ™ï¼ŒåŠ ä¸Šdataaugmentå‰ç¼€
        prefix = f"{self.dataset_name}_dataaugment_"
        cache_file_name = f"{prefix}timeseries_{time_unit}_batch_{station_id}_{station_id}.nc"
        cache_file_path = os.path.join(CACHE_DIR, cache_file_name)

        # ä¿å­˜æ•°æ®
        ds.to_netcdf(cache_file_path)

        print(f"å¢å¼ºæ—¶é—´åºåˆ—æ•°æ®å·²ä¿å­˜åˆ°: {cache_file_path}")
        return cache_file_path

    def read_ts_xrdataset_augmented(
        self,
        gage_id_lst: Optional[List[str]] = None,
        t_range: Optional[List[str]] = None,
        var_lst: Optional[List[str]] = None,
        time_unit: str = "3h",
        **kwargs,
    ) -> Dict:
        """
        è¯»å–å¢å¼ºçš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œä¼˜å…ˆä»dataaugmentç¼“å­˜æ–‡ä»¶è¯»å–

        Parameters
        ----------
        gage_id_lst : Optional[List[str]], optional
            ç«™ç‚¹IDåˆ—è¡¨
        t_range : Optional[List[str]], optional
            æ—¶é—´èŒƒå›´
        var_lst : Optional[List[str]], optional
            å˜é‡åˆ—è¡¨
        time_unit : str, optional
            æ—¶é—´å•ä½ï¼Œé»˜è®¤"3h"
        **kwargs
            å…¶ä»–å‚æ•°

        Returns
        -------
        Dict
            åŒ…å«å¢å¼ºæ•°æ®çš„å­—å…¸ï¼Œæ ¼å¼ä¸read_ts_xrdatasetä¸€è‡´
        """
        if gage_id_lst is None or len(gage_id_lst) == 0:
            return self.read_ts_xrdataset(
                gage_id_lst, t_range, var_lst, **kwargs
            )

        station_id = gage_id_lst[0]

        # æ„é€ å¢å¼ºæ•°æ®ç¼“å­˜æ–‡ä»¶è·¯å¾„
        prefix = f"{self.dataset_name}_dataaugment_"
        cache_file_name = f"{prefix}timeseries_{time_unit}_batch_{station_id}_{station_id}.nc"
        cache_file_path = os.path.join(CACHE_DIR, cache_file_name)

        # æ£€æŸ¥å¢å¼ºæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(cache_file_path):
            try:
                # è¯»å–å¢å¼ºæ•°æ®
                ds = xr.open_dataset(cache_file_path)

                # åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
                if t_range is not None and len(t_range) >= 2:
                    start_time = pd.to_datetime(t_range[0])
                    end_time = pd.to_datetime(t_range[1])
                    ds = ds.sel(time=slice(start_time, end_time))

                # åº”ç”¨å˜é‡è¿‡æ»¤
                if var_lst is not None:
                    available_vars = [
                        var for var in var_lst if var in ds.data_vars
                    ]
                    if available_vars:
                        ds = ds[available_vars]

                print(f"æˆåŠŸä»å¢å¼ºæ•°æ®ç¼“å­˜è¯»å–: {cache_file_path}")
                return {time_unit: ds}

            except Exception as e:
                print(f"è¯»å–å¢å¼ºæ•°æ®ç¼“å­˜å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®: {e}")

        # å¦‚æœå¢å¼ºæ•°æ®ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®
        return self.read_ts_xrdataset(gage_id_lst, t_range, var_lst, **kwargs)

    def generate_augmented_file_indices(
        self, start_idx: int = 1, end_idx: int = 100, step: int = 1
    ) -> List[int]:
        """
        ç”Ÿæˆè¦å¤„ç†çš„å¢å¼ºæ–‡ä»¶ç¼–å·åˆ—è¡¨

        Parameters
        ----------
        start_idx : int, optional
            èµ·å§‹ç¼–å·ï¼Œé»˜è®¤1
        end_idx : int, optional
            ç»“æŸç¼–å·ï¼Œé»˜è®¤100
        step : int, optional
            æ­¥é•¿ï¼Œé»˜è®¤1

        Returns
        -------
        List[int]
            æ–‡ä»¶ç¼–å·åˆ—è¡¨
        """
        return list(range(start_idx, end_idx + 1, step))


def _calculate_event_characteristics(
    event: Dict, delta_t_hours: float = 3.0
) -> Dict:
    """
    è®¡ç®—æ´ªæ°´äº‹ä»¶çš„è¯¦ç»†ç‰¹å¾æŒ‡æ ‡ï¼Œç”¨äºç”»å›¾å’Œåˆ†æ

    Parameters
    ----------
        event: dict
            äº‹ä»¶å­—å…¸ï¼ŒåŒ…å« 'P_eff' (å‡€é›¨) å’Œ 'Q_obs_eff' (å¾„æµ) æ•°ç»„
        delta_t_hours: float
            æ—¶æ®µé•¿åº¦ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤3å°æ—¶

    Returns
    -------
        Dict: åŒ…å«è®¡ç®—å‡ºçš„æ°´æ–‡ç‰¹å¾æŒ‡æ ‡

    Calculated metrics:
        - peak_obs: æ´ªå³°æµé‡ (mÂ³/s)
        - runoff_volume_m3: æ´ªé‡ (mÂ³)
        - runoff_duration_hours: æ´ªæ°´å†æ—¶ (å°æ—¶)
        - total_net_rain: æ€»å‡€é›¨é‡ (mm)
        - lag_time_hours: æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
    """
    try:
        # æå–æ•°æ®
        net_rain = event.get(NET_RAIN, [])
        direct_runoff = event.get(OBS_FLOW, [])

        net_rain = np.array(net_rain)
        direct_runoff = np.array(direct_runoff)

        # è½¬æ¢ä¸ºç§’
        delta_t_seconds = delta_t_hours * 3600.0

        # 1. è®¡ç®—æ´ªå³°æµé‡
        peak_obs = np.max(direct_runoff)
        if peak_obs < 1e-6:
            return None

        # 2. è®¡ç®—æ´ªé‡ (mÂ³)
        runoff_volume_m3 = np.sum(direct_runoff) * delta_t_seconds

        # 3. è®¡ç®—æ´ªæ°´å†æ—¶ (å°æ—¶)
        runoff_indices = np.where(direct_runoff > 1e-6)[0]
        if len(runoff_indices) < 2:
            return None
        runoff_duration_hours = (
            runoff_indices[-1] - runoff_indices[0] + 1
        ) * delta_t_hours

        # 4. è®¡ç®—æ€»å‡€é›¨é‡ (mm)
        total_net_rain = np.sum(net_rain)

        # 5. è®¡ç®—æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
        t_peak_flow_idx = np.argmax(direct_runoff)
        t_peak_rain_idx = np.argmax(net_rain)
        lag_time_hours = (t_peak_flow_idx - t_peak_rain_idx) * delta_t_hours

        # 6. è®¡ç®—æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
        m_eff = len(net_rain)

        # 7. è®¡ç®—å¾„æµæ—¶æ®µæ•°
        n_obs = len(direct_runoff)

        # 8. è®¡ç®—å•ä½çº¿é•¿åº¦
        n_specific = n_obs - m_eff + 1

        # è¿”å›è®¡ç®—ç»“æœ
        characteristics = {
            "peak_obs": peak_obs,  # æ´ªå³°æµé‡ (mÂ³/s)
            "runoff_volume_m3": runoff_volume_m3,  # æ´ªé‡ (mÂ³)
            "runoff_duration_hours": runoff_duration_hours,  # æ´ªæ°´å†æ—¶ (å°æ—¶)
            "total_net_rain": total_net_rain,  # æ€»å‡€é›¨é‡ (mm)
            "lag_time_hours": lag_time_hours,  # æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
            "m_eff": m_eff,  # æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
            "n_obs": n_obs,  # å¾„æµæ—¶æ®µæ•°
            "n_specific": n_specific,  # å•ä½çº¿é•¿åº¦
            "delta_t_hours": delta_t_hours,  # æ—¶æ®µé•¿åº¦
        }

        return characteristics

    except Exception as e:
        print(f"è®¡ç®—äº‹ä»¶ç‰¹å¾æ—¶å‡ºé”™: {e}")
        return None


def calculate_events_characteristics(
    events: List[Dict], delta_t_hours: float = 3.0
) -> List[Dict]:
    """
    æ‰¹é‡è®¡ç®—å¤šä¸ªæ´ªæ°´äº‹ä»¶çš„ç‰¹å¾æŒ‡æ ‡

    Args:
        events: äº‹ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å« 'P_eff' å’Œ 'Q_obs_eff' æ•°ç»„
        delta_t_hours: æ—¶æ®µé•¿åº¦ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤3å°æ—¶

    Returns:
        List[Dict]: åŒ…å«è®¡ç®—å‡ºçš„æ°´æ–‡ç‰¹å¾æŒ‡æ ‡çš„äº‹ä»¶åˆ—è¡¨
    """
    enhanced_events = []

    for i, event in enumerate(events):
        # è®¡ç®—ç‰¹å¾æŒ‡æ ‡
        characteristics = _calculate_event_characteristics(
            event, delta_t_hours
        )

        if characteristics:
            # å°†ç‰¹å¾æŒ‡æ ‡æ·»åŠ åˆ°åŸäº‹ä»¶å­—å…¸ä¸­
            enhanced_event = event.copy()
            enhanced_event.update(characteristics)
            enhanced_events.append(enhanced_event)
        else:
            print(f"âš ï¸ äº‹ä»¶ {i+1} ç‰¹å¾è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡")

    return enhanced_events


def load_and_preprocess_events_unified(
    data_dir: str,
    station_id: Optional[str] = None,
    include_peak_obs: bool = True,
    verbose: bool = True,
    flow_unit: str = "mm/3h",
) -> Optional[List[Dict]]:
    """
    Unified backward-compatible interface function.

    Parameters
    ----------
    data_dir : str
        Path to the data directory.
    station_id : Optional[str], optional
        Basin station ID (default is None).
    include_peak_obs : bool, optional
        Whether to include observed flood peak values (default is True).
    verbose : bool, optional
        Whether to print detailed information (default is True).
    flow_unit : str, optional
        Unit of flow data (default is "mm/3h").

    Returns
    -------
    Optional[List[Dict]]
        List of event dictionaries in standard format, fully compatible with existing unit hydrograph algorithms.
    """
    # åˆ›å»ºæ•°æ®é›†å®ä¾‹
    dataset = FloodEventDatasource(
        data_dir,
        flow_unit=flow_unit,
        trange4cache=["1960-01-01 02", "2024-12-31 23"],
    )
    return dataset._load_1basin_flood_events(
        station_id, flow_unit, include_peak_obs, verbose
    )


def check_event_data_nan(all_event_data: List[Dict]):
    """
    æ£€æŸ¥æ‰€æœ‰æ´ªæ°´äº‹ä»¶æ•°æ®ä¸­çš„é™é›¨å’Œå¾„æµæ˜¯å¦æœ‰ç©ºå€¼ï¼Œè‹¥æœ‰åˆ™æŠ¥é”™å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯ã€‚
    Args:
        all_event_data: äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼ˆæ¯ä¸ªå­—å…¸åŒ…å«P_effã€Q_obs_effã€filepathç­‰ï¼‰
    Raises:
        ValueError: å¦‚æœå‘ç°ç©ºå€¼ï¼ŒæŠ›å‡ºå¼‚å¸¸å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    for event in all_event_data:
        event_name = event.get("filepath", "unknown")
        p_eff = event.get(NET_RAIN)
        q_obs = event.get(OBS_FLOW)
        # æ£€æŸ¥é™é›¨
        if p_eff is not None and np.any(np.isnan(p_eff)):
            nan_idx = np.where(np.isnan(p_eff))[0]
            print(f"âŒ åœºæ¬¡ {event_name} çš„ P_eff å­˜åœ¨ç©ºå€¼ï¼Œç´¢å¼•: {nan_idx}")
            raise ValueError(
                f"Event {event_name} has NaN in P_eff at index {nan_idx}"
            )
        # æ£€æŸ¥å¾„æµ
        if q_obs is not None and np.any(np.isnan(q_obs)):
            nan_idx = np.where(np.isnan(q_obs))[0]
            print(
                f"âŒ åœºæ¬¡ {event_name} çš„ {OBS_FLOW} å­˜åœ¨ç©ºå€¼ï¼Œç´¢å¼•: {nan_idx}"
            )
            raise ValueError(
                f"Event {event_name} has NaN in {OBS_FLOW} at index {nan_idx}"
            )
