"""
Author: Wenyu Ouyang
Date: 2025-08-07
LastEditTime: 2025-08-07
LastEditors: Wenyu Ouyang
Description: Unit hydrograph model with unified interface
FilePath: /hydromodel/hydromodel/models/unit_hydrograph.py
Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.
"""

import logging
import numpy as np
from typing import Union, Dict, Any
from scipy.stats import gamma


def uh_conv(x, uh, truncate=True):
    """
    Function for convolution calculation supporting different array dimensions

    Parameters
    ----------
    x
        input array for convolution:
        - 1D: [seq] - sequence data
        - 2D: [seq, batch] - sequence data with batch dimension
        - 3D: [seq, batch, feature] - sequence data with batch and feature dims
    uh
        unit hydrograph array:
        - 1D: [len_uh] - for 1D input
        - 2D: [len_uh, batch] - for 2D input
        - 3D: [len_uh, batch, feature] - for 3D input
    truncate : bool, optional
        If True (default), truncate convolution result to original sequence length.
        If False, return full convolution result (changes output shape).

    Returns
    -------
    np.array
        convolution result. If truncate=True, same shape as x.
        If truncate=False, sequence dimension length = len(x) + len(uh) - 1.
    """
    x = np.asarray(x)
    uh = np.asarray(uh)

    if x.ndim == 1:
        # 1D case: [seq]
        if uh.ndim != 1:
            logging.error("For 1D input x, uh should also be 1D")
            return np.zeros_like(x)
        # Handle empty arrays
        if len(x) == 0 or len(uh) == 0:
            return np.zeros_like(x)

        conv_result = np.convolve(x, uh)
        return conv_result[: len(x)] if truncate else conv_result

    elif x.ndim == 2:
        return _uh_conv_2d(x, uh, truncate)
    elif x.ndim == 3:
        return _uh_conv_3d(x, uh, truncate)
    else:
        logging.error(
            f"Unsupported array dimension: {x.ndim}D. "
            f"Only 1D, 2D, 3D are supported."
        )
        return np.zeros_like(x)


def _uh_conv_2d(x, uh, truncate=True):
    """2D case: [seq, batch]"""
    seq_length, batch_size = x.shape
    if uh.ndim != 2 or uh.shape[1] != batch_size:
        logging.error(
            "For 2D input x [seq, batch], uh should be [len_uh, batch]"
        )
        return np.zeros_like(x)

    # Handle empty arrays
    if seq_length == 0 or uh.shape[0] == 0:
        return np.zeros_like(x)

    # Calculate output shape
    if truncate:
        output_shape = x.shape
        outputs = np.zeros_like(x)
    else:
        conv_length = seq_length + uh.shape[0] - 1
        output_shape = (conv_length, batch_size)
        outputs = np.zeros(output_shape, dtype=x.dtype)

    for i in range(batch_size):
        conv_result = np.convolve(x[:, i], uh[:, i])
        outputs[:, i] = conv_result[:seq_length] if truncate else conv_result
    return outputs


def _uh_conv_3d(x, uh, truncate=True):
    """3D case: [seq, batch, feature]"""
    seq_length, batch_size, feature_size = x.shape
    if uh.ndim != 3 or uh.shape[1:] != (batch_size, feature_size):
        logging.error(
            "For 3D input x [seq, batch, feature], "
            "uh should be [len_uh, batch, feature]"
        )
        return np.zeros_like(x)

    # Handle empty arrays
    if seq_length == 0 or uh.shape[0] == 0:
        return np.zeros_like(x)

    # Calculate output shape
    if truncate:
        output_shape = x.shape
        outputs = np.zeros_like(x)
    else:
        conv_length = seq_length + uh.shape[0] - 1
        output_shape = (conv_length, batch_size, feature_size)
        outputs = np.zeros(output_shape, dtype=x.dtype)

    for i, j in itertools.product(range(batch_size), range(feature_size)):
        conv_result = np.convolve(x[:, i, j], uh[:, i, j])
        outputs[:, i, j] = (
            conv_result[:seq_length] if truncate else conv_result
        )
    return outputs


# --- Ê†∏ÂøÉËÆ°ÁÆóÂáΩÊï∞ ---
def objective_function_multi_event(
    U_params,
    list_of_event_data_for_opt,
    lambda_smooth,
    lambda_peak_violation,
    apply_peak_penalty_flag,
    common_n_uh,
    net_rain_name="P_eff",
    obs_flow_name="Q_obs_eff",
):
    """
    Objective function for multi-event unit hydrograph optimization.

    Parameters
    ----------
    U_params : np.ndarray
        Unit hydrograph parameters (array of length common_n_uh).
    list_of_event_data_for_opt : list of dict
        List of event data dictionaries for optimization. Each dict should contain
    lambda_smooth : float
        Weight for the smoothness penalty term.
    lambda_peak_violation : float
        Weight for the unimodality (single-peak) violation penalty.
    apply_peak_penalty_flag : bool
        Whether to apply the unimodality penalty.
    common_n_uh : int
        Length of the unit hydrograph.
    net_rain_name : str, optional
        Name of the effective rainfall column in event data.
    obs_flow_name : str, optional
        Name of the observed flow column in event data.

    Returns
    -------
    float
        Value of the objective function (to be minimized).
    """
    total_fit_loss = 0  # ÊÄªÊãüÂêàÊçüÂ§±
    if len(U_params) != common_n_uh:
        return 1e18
    for event_data in list_of_event_data_for_opt:
        P_event, Q_event_obs = (
            event_data[net_rain_name],
            event_data[obs_flow_name],
        )  # Âú∫Ê¨°ÈôçÈõ®ÂíåËßÇÊµãÂæÑÊµÅ
        Q_sim_full_event = uh_conv(
            P_event, U_params, truncate=False
        )  # Ê®°ÊãüÂæÑÊµÅ
        Q_sim_compare_event = Q_sim_full_event[
            : len(Q_event_obs)
        ]  # Áî®‰∫éÊØîËæÉÁöÑÊ®°ÊãüÂæÑÊµÅ
        total_fit_loss += np.sum(
            (Q_sim_compare_event - Q_event_obs) ** 2
        )  # Á¥ØÂä†ÂùáÊñπËØØÂ∑Æ
    loss_smooth_val = (
        np.sum(np.diff(U_params) ** 2) if len(U_params) > 1 else 0
    )  # Âπ≥ÊªëÊÄßÊÉ©ÁΩöÈ°π
    peak_violation_penalty_val = 0  # ÂçïÂ≥∞ËøùÂèçÊÉ©ÁΩöÈ°π
    if apply_peak_penalty_flag and len(U_params) > 2:
        actual_k_peak = np.argmax(U_params)  # Âçï‰ΩçÁ∫øÂ≥∞ÂÄº‰ΩçÁΩÆ
        for j in range(actual_k_peak):
            if U_params[j + 1] < U_params[j] - 1e-6:
                peak_violation_penalty_val += (
                    U_params[j] - U_params[j + 1]
                ) ** 2
        for j in range(actual_k_peak, len(U_params) - 1):
            if U_params[j + 1] > U_params[j] + 1e-6:
                peak_violation_penalty_val += (
                    U_params[j + 1] - U_params[j]
                ) ** 2
    return (
        total_fit_loss
        + lambda_smooth * loss_smooth_val
        + lambda_peak_violation * peak_violation_penalty_val
    )


def optimize_shared_unit_hydrograph(
    all_event_data,
    common_n_uh,
    smoothing_factor,
    peak_violation_weight,
    apply_peak_penalty,
    max_iterations=500,
    verbose=True,
):
    """
    Optimize shared unit hydrograph parameters for multi-event data.

    Parameters
    ----------
    all_event_data : list
        List of event data dictionaries.
    common_n_uh : int
        Length of the unit hydrograph.
    smoothing_factor : float
        Smoothing factor for regularization.
    peak_violation_weight : float
        Weight for peak violation penalty.
    apply_peak_penalty : bool
        Whether to apply peak penalty.
    max_iterations : int, optional
        Maximum number of optimization iterations (default is 500).
    verbose : bool, optional
        Whether to display optimization progress (default is True).

    Returns
    -------
    np.ndarray or None
        Optimized unit hydrograph parameters array, or None if optimization fails.
    """
    U_initial_guess = init_unit_hydrograph(common_n_uh)
    bounds = [(0, 1) for _ in range(common_n_uh)]
    constraints = {"type": "eq", "fun": lambda U: np.sum(U) - 1}
    result = minimize(
        objective_function_multi_event,
        U_initial_guess,
        args=(
            all_event_data,
            smoothing_factor,
            peak_violation_weight,
            apply_peak_penalty,
            common_n_uh,
        ),
        # method="L-BFGS-B",
        method="SLSQP",
        bounds=bounds,
        constraints=constraints,
        options={"disp": verbose, "maxiter": max_iterations},
    )
    if result.success or result.status in [0, 2]:
        return result.x
    else:
        return None


def optimize_uh_for_group(events_in_group, group_name, weights, n_uh_group):
    """
    Optimize unit hydrograph for a specific event group.

    Parameters
    ----------
    events_in_group : list
        List of events in the group.
    group_name : str
        Name of the group.
    weights : dict
        Weight dictionary containing 'smoothing_factor' and 'peak_violation_weight'.
    n_uh_group : int
        Length of the unit hydrograph.

    Returns
    -------
    np.ndarray or None
        Optimized unit hydrograph parameters, or None if optimization fails.
    """
    print(
        f"\n--- Ê≠£Âú®‰∏∫ÁªÑ '{group_name}' ‰ºòÂåñÁâπÂæÅÂçï‰ΩçÁ∫ø ({len(events_in_group)} Âú∫) ---"
    )
    if len(events_in_group) < 3:
        print("‰∫ã‰ª∂Êï∞ÈáèËøáÂ∞ëÔºåË∑≥Ëøá‰ºòÂåñ„ÄÇ")
        return None

    smoothing, peak_penalty = (
        weights["smoothing_factor"],
        weights["peak_violation_weight"],
    )
    apply_penalty = n_uh_group > 2  # ÊòØÂê¶Â∫îÁî®ÂçïÂ≥∞ÊÉ©ÁΩö
    print(
        f"  ‰ΩøÁî®ÊùÉÈáç: Âπ≥Êªë={smoothing}, ÂçïÂ≥∞ÁΩö={peak_penalty if apply_penalty else 'N/A'}"
    )
    # Direct optimization logic (previously in _internal_optimize_unit_hydrograph)
    U_initial_guess = init_unit_hydrograph(n_uh_group)
    bounds = [(0, 1) for _ in range(n_uh_group)]
    constraints = {"type": "eq", "fun": lambda U: np.sum(U) - 1}
    result = minimize(
        objective_function_multi_event,
        U_initial_guess,
        args=(
            events_in_group,
            smoothing,
            peak_penalty,
            apply_penalty,
            n_uh_group,
        ),
        method="SLSQP",
        bounds=bounds,
        constraints=constraints,
        options={"disp": False, "maxiter": 500},
    )
    U_optimized = (
        result.x if (result.success or result.status in [0, 2]) else None
    )
    status_message = "ÊàêÂäü" if U_optimized is not None else "ÂèØËÉΩÊú™Êî∂Êïõ"
    print(f"  ‰ºòÂåñ{status_message}")
    return U_optimized


def init_unit_hydrograph(length, method="gamma", **kwargs):
    """
    ÂàùÂßãÂåñ‰∏Ä‰∏™ÂçïÂ≥∞‰∏îÂΩí‰∏ÄÂåñÁöÑÂçï‰ΩçÁ∫ø„ÄÇ

    Parameters
    ----------
    length : int
        Âçï‰ΩçÁ∫øÈïøÂ∫¶„ÄÇ
    method : str, optional
        ÂàùÂßãÂåñÊñπÊ≥ïÔºå'gamma'ÔºàÈªòËÆ§ÔºåÂÅèÊÄÅÔºâÊàñ'gaussian'ÔºàÂØπÁß∞Ôºâ„ÄÇ
    **kwargs :
        gammaÂàÜÂ∏ÉÂèÇÊï∞Ôºöshape, scale
        gaussianÂàÜÂ∏ÉÂèÇÊï∞Ôºöpeak_pos, std_ratio

    Returns
    -------
    uh : np.ndarray
        ÂΩí‰∏ÄÂåñÁöÑÂçï‰ΩçÁ∫øÊï∞ÁªÑ„ÄÇ
    """
    if method == "gaussian":
        peak_pos = kwargs.get("peak_pos", length // 2)
        std_ratio = kwargs.get("std_ratio", 0.15)
        std = length * std_ratio
        x = np.arange(length)
        uh = np.exp(-0.5 * ((x - peak_pos) / std) ** 2)
    else:  # ÈªòËÆ§gamma
        shape = kwargs.get("shape", 2.0)
        scale = kwargs.get("scale", 2.0)
        x = np.arange(length)
        uh = gamma.pdf(x, a=shape, scale=scale)
    uh = np.maximum(uh, 0)
    uh /= uh.sum()
    return uh


def evaluate_single_event_from_uh(
    event_data,
    U_optimized,
    category_name=None,
    net_rain_name="P_eff",
    obs_flow_name="Q_obs_eff",
):
    """
    ËØÑ‰º∞Âçï‰∏™Ê¥™Ê∞¥‰∫ã‰ª∂ÁöÑÊÄßËÉΩÊåáÊ†á

    Parameters:
    ----------
        event_data: ‰∫ã‰ª∂Êï∞ÊçÆÂ≠óÂÖ∏
        U_optimized: ‰ºòÂåñÁöÑÂçï‰ΩçÁ∫øÂèÇÊï∞
        category_name: Á±ªÂà´ÂêçÁß∞ÔºàÂèØÈÄâÔºâ

    Returns
    -------
        dict: ÂåÖÂê´ËØÑ‰º∞ÁªìÊûúÁöÑÂ≠óÂÖ∏
    """
    P_event = event_data[net_rain_name]
    Q_obs_event_full = event_data[obs_flow_name]
    event_filename = os.path.basename(event_data["filepath"])

    # ÂàùÂßãÂåñÊåáÊ†á
    result = {
        "Êñá‰ª∂Âêç": event_filename,
        "NSE": np.nan,
        "Ê¥™ÈáèÁõ∏ËØØ(%)": np.nan,
        "Ê¥™Â≥∞Áõ∏ËØØ(%)": np.nan,
    }

    # Â¶ÇÊûúÊåáÂÆö‰∫ÜÁ±ªÂà´ÔºåÊ∑ªÂä†Âà∞ÁªìÊûú‰∏≠
    if category_name is not None:
        result["ÊâÄÂ±ûÁ±ªÂà´"] = category_name

    if U_optimized is not None:
        Q_sim_event_full = uh_conv(P_event, U_optimized, truncate=False)
        Q_sim_event_compare = Q_sim_event_full[: len(Q_obs_event_full)]

        if len(Q_obs_event_full) > 0 and len(Q_sim_event_compare) == len(
            Q_obs_event_full
        ):
            result["NSE"] = nse(Q_obs_event_full, Q_sim_event_compare)
            result["Ê¥™ÈáèÁõ∏ËØØ(%)"] = flood_volume_error(
                Q_obs_event_full, Q_sim_event_compare
            )
            result["Ê¥™Â≥∞Áõ∏ËØØ(%)"] = flood_peak_error(
                Q_obs_event_full, Q_sim_event_compare
            )

    return result


def categorize_floods_by_peak(all_events_data):
    """
    Ê†πÊçÆÊ¥™Â≥∞Â∞ÜÊ¥™Ê∞¥‰∫ã‰ª∂ÂàÜ‰∏∫‰∏âÁ±ª

    Args:
        all_events_data: ÂåÖÂê´peak_obsÁöÑ‰∫ã‰ª∂Êï∞ÊçÆÂàóË°®

    Returns:
        dict: ÂàÜÁ±ªÂêéÁöÑ‰∫ã‰ª∂Â≠óÂÖ∏
        tuple: (threshold_low, threshold_high) ÂàÜÁ±ªÈòàÂÄº
    """
    event_peaks = [
        data["peak_obs"] for data in all_events_data if data["peak_obs"] > 0
    ]

    if not event_peaks:
        print("‚ùå Ê≤°ÊúâÊúâÊïàÁöÑÊ¥™Â≥∞Êï∞ÊçÆ")
        return None, (None, None)

    threshold_low = np.percentile(event_peaks, 33.3)  # 33.3%ÂàÜ‰ΩçÊï∞
    threshold_high = np.percentile(event_peaks, 66.6)  # 66.6%ÂàÜ‰ΩçÊï∞

    categorized_events = {"small": [], "medium": [], "large": []}

    for event_data in all_events_data:
        peak = event_data["peak_obs"]
        if peak <= threshold_low:
            categorized_events["small"].append(event_data)
        elif peak <= threshold_high:
            categorized_events["medium"].append(event_data)
        else:
            categorized_events["large"].append(event_data)

    return categorized_events, (threshold_low, threshold_high)


# --- ÁªìÊûú‰øùÂ≠òÂíåËæìÂá∫ÂäüËÉΩ ---
def save_results_to_csv(report_data, output_filename, sort_columns=None):
    """
    ‰øùÂ≠òÁªìÊûúÂà∞CSVÊñá‰ª∂

    Args:
        report_data: Êä•ÂëäÊï∞ÊçÆÂàóË°®
        output_filename: ËæìÂá∫Êñá‰ª∂Âêç
        sort_columns: ÊéíÂ∫èÂàóÂêçÂàóË°®

    Returns:
        pd.DataFrame: ÊéíÂ∫èÂêéÁöÑDataFrame
    """
    if not report_data:
        print("‚ùå Ê≤°ÊúâÊï∞ÊçÆÂèØ‰ª•‰øùÂ≠ò")
        return None

    report_df = pd.DataFrame(report_data)

    # ÊéíÂ∫è
    if sort_columns:
        ascending = [True] * len(sort_columns)  # ÈªòËÆ§ÂçáÂ∫è
        if "NSE" in sort_columns:
            # NSEÂàóÊåâÈôçÂ∫èÊéíÂàó
            nse_idx = sort_columns.index("NSE")
            ascending[nse_idx] = False
        report_df_sorted = report_df.sort_values(
            by=sort_columns, ascending=ascending
        ).reset_index(drop=True)
    else:
        report_df_sorted = report_df.copy()

    # ‰øùÂ≠òÊñá‰ª∂
    try:
        save_dataframe_to_csv(
            report_df_sorted,
            output_filename,
            encoding="utf-8-sig",
            float_format="%.4f",
        )
        print(f"\n‚úÖ ËØÑ‰º∞Êä•ÂëäÂ∑≤ÊàêÂäü‰øùÂ≠òÂà∞Êñá‰ª∂: {output_filename}")
    except Exception as e:
        print(f"\n‚ùå ‰øùÂ≠òÊä•ÂëäÂà∞Êñá‰ª∂Â§±Ë¥•: {e}")

    return report_df_sorted


def print_report_preview(report_df_sorted, title="ËØÑ‰º∞Êä•ÂëäÈ¢ÑËßà", top_n=None):
    """
    ÊâìÂç∞Êä•ÂëäÈ¢ÑËßà

    Args:
        report_df_sorted: ÊéíÂ∫èÂêéÁöÑDataFrame
        title: È¢ÑËßàÊ†áÈ¢ò
        top_n: ÊòæÁ§∫Ââçn‰∏™ÊúÄ‰Ω≥‰∫ã‰ª∂ÔºåÂ¶ÇÊûú‰∏∫NoneÂàôÊòæÁ§∫ÊâÄÊúâ‰∫ã‰ª∂
    """
    print(f"\nüìä --- {title} ---")
    
    # Ê†πÊçÆtop_nÂèÇÊï∞ÂÜ≥ÂÆöÊòæÁ§∫ÁöÑÊï∞ÊçÆ
    if top_n is not None and top_n > 0:
        display_df = report_df_sorted.head(top_n)
        print(f"ÊòæÁ§∫Ââç {min(top_n, len(report_df_sorted))} ‰∏™ÊúÄ‰Ω≥‰∫ã‰ª∂Ôºö")
    else:
        display_df = report_df_sorted
        print(f"ÊòæÁ§∫ÂÖ®ÈÉ® {len(report_df_sorted)} ‰∏™‰∫ã‰ª∂Ôºö")
    
    pd.set_option("display.max_rows", 50)
    pd.set_option("display.width", 120)
    print(display_df)
    pd.reset_option("display.max_rows")
    pd.reset_option("display.width")


def print_category_statistics(report_df_sorted):
    """
    ÊâìÂç∞ÂêÑÁ±ªÂà´ÊÄßËÉΩÁªüËÆ°‰ø°ÊÅØ

    Args:
        report_df_sorted: ÂåÖÂê´Á±ªÂà´‰ø°ÊÅØÁöÑDataFrame
    """
    if "ÊâÄÂ±ûÁ±ªÂà´" not in report_df_sorted.columns:
        return

    print("\nüìà --- ÂêÑÁ±ªÂà´ÊÄßËÉΩÁªüËÆ° ---")
    for category in ["small", "medium", "large"]:
        cat_data = report_df_sorted[report_df_sorted["ÊâÄÂ±ûÁ±ªÂà´"] == category]
        if len(cat_data) > 0:
            mean_nse = cat_data["NSE"].mean()
            mean_vol_err = cat_data["Ê¥™ÈáèÁõ∏ËØØ(%)"].mean()
            mean_peak_err = cat_data["Ê¥™Â≥∞Áõ∏ËØØ(%)"].mean()
            print(f"üè∑Ô∏è {category.capitalize()} Á±ªÂà´ ({len(cat_data)} Âú∫):")
            print(f"   Âπ≥ÂùáNSE: {mean_nse:.4f}")
            print(f"   Âπ≥ÂùáÊ¥™ÈáèËØØÂ∑Æ: {mean_vol_err:.2f}%")
            print(f"   Âπ≥ÂùáÊ¥™Â≥∞ËØØÂ∑Æ: {mean_peak_err:.2f}%")


def save_dataframe_to_csv(
    df: pd.DataFrame,
    filepath: str,
    metadata_lines: Optional[List[str]] = None,
    encoding: str = "utf-8",
    float_format: str = "%.6f",
    **kwargs,
) -> None:
    """
    Save DataFrame to CSV file with optional metadata header.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame to save.
    filepath : str
        Output file path.
    metadata_lines : list of str, optional
        Optional metadata lines to write before CSV data.
    encoding : str, optional
        File encoding (default is "utf-8").
    float_format : str, optional
        Float formatting string (default is "%.6f").
    **kwargs
        Additional arguments passed to DataFrame.to_csv().
    """
    # Ensure output directory exists
    os.makedirs(os.path.dirname(filepath), exist_ok=True)

    # Default CSV parameters
    csv_kwargs = {
        "index": False,
        "encoding": encoding,
        "float_format": float_format,
        "header": True,
    }
    csv_kwargs.update(kwargs)

    if metadata_lines:
        with open(filepath, "w", encoding=encoding, newline="") as f:
            f.write("\n".join(metadata_lines) + "\n")
            df.to_csv(f, **csv_kwargs)
    else:
        df.to_csv(filepath, **csv_kwargs)


# =============================================================================
# UNIFIED MODEL INTERFACES
# =============================================================================


def unit_hydrograph(
    inputs: np.ndarray,
    params: np.ndarray,
    warmup_length: int = 0,
    return_state: bool = False,
    **kwargs
) -> Union[np.ndarray, tuple]:
    """
    Unit hydrograph model with unified interface.
    
    This function provides a unified interface consistent with other hydrological 
    models (XAJ, GR series), where the core computation is a convolution between
    net rainfall and unit hydrograph parameters.
    
    Parameters
    ----------
    inputs : np.ndarray
        Input data array with shape [time, basin, features]:
        - time: sequence length
        - basin: number of basins (usually 1 for single basin)
        - features: input variables (net_rain, observed_flow, etc.)
        For single basin, inputs can be [time, features] or [time] for net_rain only
    params : np.ndarray
        Unit hydrograph parameters with shape [basin, n_uh]:
        - basin: number of basins
        - n_uh: length of unit hydrograph
        For single basin, params can be 1D array [n_uh]
    warmup_length : int, optional
        Length of warmup period to exclude from output (default: 0)
    return_state : bool, optional
        If True, return state information (default: False)
    **kwargs
        Additional parameters:
        - net_rain_idx: index of net rainfall in input features (default: 0)
        - truncate: whether to truncate convolution result (default: True)
        
    Returns
    -------
    np.ndarray or tuple
        If return_state=False: simulated flow with shape same as inputs
        If return_state=True: (simulated_flow, state_dict)
        
    Examples
    --------
    >>> # Single basin example
    >>> net_rain = np.array([0, 5, 10, 8, 3, 1, 0])  # [time]
    >>> uh_params = np.array([0.3, 0.5, 0.2])  # [n_uh=3]
    >>> flow = unit_hydrograph(net_rain, uh_params)
    
    >>> # Multi-basin example  
    >>> inputs = np.random.rand(100, 2, 1)  # [time=100, basin=2, feature=1]
    >>> params = np.random.rand(2, 24)  # [basin=2, n_uh=24]  
    >>> flows = unit_hydrograph(inputs, params, warmup_length=10)
    """
    # Ensure inputs and params are numpy arrays
    inputs = np.asarray(inputs)
    params = np.asarray(params)
    
    # Handle different input dimensions
    if inputs.ndim == 1:
        # Single time series: [time] -> [time, 1, 1]
        inputs = inputs.reshape(-1, 1, 1)
        single_series = True
    elif inputs.ndim == 2:
        # Two cases: [time, basin] or [time, features]
        if params.ndim == 1:
            # Assume [time, features], single basin
            inputs = inputs.reshape(inputs.shape[0], 1, -1)
            params = params.reshape(1, -1)
        else:
            # Assume [time, basin], add feature dim
            inputs = inputs.reshape(inputs.shape[0], inputs.shape[1], 1)
        single_series = False
    elif inputs.ndim == 3:
        # Standard format: [time, basin, features]
        single_series = False
    else:
        raise ValueError(f"Unsupported input dimension: {inputs.ndim}")
    
    # Handle parameter dimensions
    if params.ndim == 1:
        # Single UH: [n_uh] -> [1, n_uh]
        params = params.reshape(1, -1)
    elif params.ndim != 2:
        raise ValueError(f"Parameters must be 1D or 2D, got {params.ndim}D")
    
    time_steps, n_basins, n_features = inputs.shape
    n_basins_params, n_uh = params.shape
    
    # Check dimension consistency
    if n_basins != n_basins_params:
        raise ValueError(f"Basin dimension mismatch: inputs has {n_basins}, params has {n_basins_params}")
    
    # Extract net rainfall (default: first feature)
    net_rain_idx = kwargs.get('net_rain_idx', 0)
    if net_rain_idx >= n_features:
        raise ValueError(f"net_rain_idx {net_rain_idx} >= n_features {n_features}")
    
    net_rain = inputs[:, :, net_rain_idx]  # [time, basin]
    
    # Normalize unit hydrograph parameters (ensure sum to 1.0)
    params_normalized = params / params.sum(axis=1, keepdims=True)
    
    # Perform convolution for each basin
    truncate = kwargs.get('truncate', True)
    simulated_flows = np.zeros((time_steps, n_basins))
    
    for basin_idx in range(n_basins):
        basin_net_rain = net_rain[:, basin_idx]  # [time]
        basin_uh = params_normalized[basin_idx, :]  # [n_uh]
        
        # Convolution
        flow_conv = uh_conv(basin_net_rain, basin_uh, truncate=truncate)
        simulated_flows[:, basin_idx] = flow_conv
    
    # Apply warmup period
    if warmup_length > 0:
        simulated_flows = simulated_flows[warmup_length:]
    
    # Prepare output
    if single_series and n_basins == 1:
        # Return to original format for single series
        simulated_flows = simulated_flows.flatten()
    
    # Prepare state information if requested
    if return_state:
        state_dict = {
            'uh_params_normalized': params_normalized,
            'warmup_length': warmup_length,
            'n_uh': n_uh,
            'model_type': 'unit_hydrograph'
        }
        return simulated_flows, state_dict
    
    return simulated_flows


def categorized_unit_hydrograph(
    inputs: np.ndarray,
    params: Dict[str, np.ndarray],
    warmup_length: int = 0,
    return_state: bool = False,
    **kwargs
) -> Union[np.ndarray, tuple]:
    """
    Categorized unit hydrograph model with unified interface.
    
    This model uses different unit hydrographs for different flood magnitude categories
    (e.g., small, medium, large floods). Events are categorized based on peak flow
    and appropriate UH is applied.
    
    Parameters
    ----------
    inputs : np.ndarray
        Input data array with shape [time, basin, features]:
        - Must include net_rain and observed_flow for categorization
        - For flood events, typically [event_length, 1, features]
    params : dict
        Dictionary of unit hydrograph parameters by category:
        {
            'small': np.ndarray [basin, n_uh_small],
            'medium': np.ndarray [basin, n_uh_medium], 
            'large': np.ndarray [basin, n_uh_large],
            'thresholds': dict with categorization thresholds,
            'category_weights': dict with optimization weights (optional)
        }
    warmup_length : int, optional
        Length of warmup period to exclude from output (default: 0)
    return_state : bool, optional
        If True, return state information (default: False)
    **kwargs
        Additional parameters:
        - net_rain_idx: index of net rainfall (default: 0)
        - obs_flow_idx: index of observed flow for categorization (default: 1)
        - categorization_method: 'peak_magnitude' (default)
        
    Returns
    -------
    np.ndarray or tuple
        If return_state=False: simulated flow
        If return_state=True: (simulated_flow, state_dict)
        
    Examples
    --------
    >>> # Setup categorized parameters
    >>> params = {
    ...     'small': np.array([[0.4, 0.4, 0.2]]),  # [1 basin, 3 params]
    ...     'medium': np.array([[0.2, 0.3, 0.3, 0.2]]),  # [1 basin, 4 params]
    ...     'large': np.array([[0.1, 0.2, 0.3, 0.2, 0.2]]),  # [1 basin, 5 params]
    ...     'thresholds': {'small_medium': 10.0, 'medium_large': 25.0}
    ... }
    >>> inputs = np.random.rand(50, 1, 2)  # [time=50, basin=1, features=2]
    >>> flow = categorized_unit_hydrograph(inputs, params)
    """
    # Ensure inputs is numpy array
    inputs = np.asarray(inputs)
    
    if inputs.ndim != 3:
        raise ValueError(f"Categorized UH requires 3D inputs [time, basin, features], got {inputs.ndim}D")
    
    time_steps, n_basins, n_features = inputs.shape
    
    # Validate required parameters
    required_categories = ['small', 'medium', 'large']
    for cat in required_categories:
        if cat not in params:
            raise ValueError(f"Missing UH parameters for category: {cat}")
    
    # Extract input data
    net_rain_idx = kwargs.get('net_rain_idx', 0) 
    obs_flow_idx = kwargs.get('obs_flow_idx', 1)
    
    if net_rain_idx >= n_features:
        raise ValueError(f"net_rain_idx {net_rain_idx} >= n_features {n_features}")
    if obs_flow_idx >= n_features:
        raise ValueError(f"obs_flow_idx {obs_flow_idx} >= n_features {n_features}")
    
    net_rain = inputs[:, :, net_rain_idx]  # [time, basin]
    obs_flow = inputs[:, :, obs_flow_idx]  # [time, basin]
    
    # Apply warmup period to categorization data (but not to simulation)
    if warmup_length > 0:
        obs_flow_for_categorization = obs_flow[warmup_length:]
    else:
        obs_flow_for_categorization = obs_flow
    
    # Determine flood category based on peak flow
    categorization_method = kwargs.get('categorization_method', 'peak_magnitude')
    simulated_flows = np.zeros((time_steps, n_basins))
    
    for basin_idx in range(n_basins):
        basin_obs_flow = obs_flow_for_categorization[:, basin_idx]
        basin_net_rain = net_rain[:, basin_idx]
        
        # Categorize based on peak flow
        peak_flow = np.max(basin_obs_flow)
        
        # Default thresholds if not provided
        thresholds = params.get('thresholds', {'small_medium': 10.0, 'medium_large': 25.0})
        
        if peak_flow < thresholds.get('small_medium', 10.0):
            category = 'small'
        elif peak_flow < thresholds.get('medium_large', 25.0):
            category = 'medium'
        else:
            category = 'large'
        
        # Get UH parameters for this category
        category_uh_params = params[category]
        if category_uh_params.ndim == 1:
            category_uh_params = category_uh_params.reshape(1, -1)
        
        # Normalize UH parameters
        category_uh_normalized = category_uh_params[basin_idx] / category_uh_params[basin_idx].sum()
        
        # Apply unit hydrograph convolution
        basin_flow = uh_conv(basin_net_rain, category_uh_normalized, truncate=True)
        simulated_flows[:, basin_idx] = basin_flow
    
    # Apply warmup period to output
    if warmup_length > 0:
        simulated_flows = simulated_flows[warmup_length:]
    
    # Prepare state information if requested
    if return_state:
        state_dict = {
            'categories_used': {basin_idx: category for basin_idx in range(n_basins)},
            'thresholds': thresholds,
            'warmup_length': warmup_length,
            'model_type': 'categorized_unit_hydrograph',
            'uh_params_by_category': {cat: params[cat] for cat in required_categories}
        }
        return simulated_flows, state_dict
    
    return simulated_flows
