"""
Author: Wenyu Ouyang
Date: 2025-08-06
LastEditTime: 2025-08-07 11:01:36
LastEditors: Wenyu Ouyang
Description: ä½¿ç”¨ç»Ÿä¸€calibrate()æ¥å£çš„åˆ†ç±»å•ä½çº¿ä¼˜åŒ–è„šæœ¬ - å±•ç¤ºç»Ÿä¸€æ¥å£çš„çµæ´»æ€§
FilePath: \hydromodel\scripts\run_categorized_uh_optimization_unified.py

This script demonstrates the unified calibration interface flexibility by
using the same calibrate() function for categorized unit hydrograph models.
The unified interface provides consistent behavior across all model types.
Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.
"""

import os
import argparse
import json
from hydroutils.hydro_plot import (
    plot_unit_hydrograph,
    setup_matplotlib_chinese,
)
from hydrodatasource.configs.config import SETTING
from hydrodatasource.reader.floodevent import (
    FloodEventDatasource,
)
from hydromodel.models.unit_hydrograph import (
    evaluate_single_event_from_uh,
    print_report_preview,
    save_results_to_csv,
    print_category_statistics,
    categorize_floods_by_peak,
)
from hydromodel.trainers.unified_calibrate import calibrate


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ç»Ÿä¸€æ¥å£åˆ†ç±»å•ä½çº¿ä¼˜åŒ–å·¥å…· - æ¾è¾½æ²³æµåŸŸæ•°æ®ä¸“ç”¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨scipyä¼˜åŒ–å¤„ç†ç¢§æµæ²³ç«™ç‚¹æ•°æ®
  python run_categorized_uh_optimization_unified.py --station-id songliao_21401550 --algorithm scipy_minimize
  
  # ä½¿ç”¨é—ä¼ ç®—æ³•
  python run_categorized_uh_optimization_unified.py --station-id songliao_21401550 --algorithm genetic_algorithm --pop-size 100
        """,
    )

    parser.add_argument(
        "--data-path",
        "-d",
        type=str,
        default=os.path.join(
            SETTING["local_data_path"]["datasets-interim"], "songliaorrevent"
        ),
        help="åœºæ¬¡æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„",
    )

    parser.add_argument(
        "--station-id",
        type=str,
        default="songliao_21401550",
        help="ç«™ç‚¹ID (å¦‚: songliao_21401550)",
    )

    parser.add_argument(
        "--output-dir", "-o", type=str, default="results/", help="è¾“å‡ºç»“æœç›®å½•"
    )

    # åˆ†ç±»ç›¸å…³å‚æ•°
    parser.add_argument(
        "--category-weights",
        type=str,
        default="default",
        choices=["default", "balanced", "aggressive"],
        help="åˆ†ç±»æƒé‡æ–¹æ¡ˆ (é»˜è®¤: default)",
    )

    parser.add_argument(
        "--uh-lengths",
        type=str,
        default='{"small":8,"medium":16,"large":24}',
        help='å„ç±»åˆ«å•ä½çº¿é•¿åº¦ï¼ŒJSONæ ¼å¼ (é»˜è®¤: {"small":8,"medium":16,"large":24})',
    )

    parser.add_argument(
        "--warmup-length",
        type=int,
        default=8
        * 60,  # 8 hours * 60 minutes / 3 hours = 160 steps for 3h data
        help="é¢„çƒ­æœŸé•¿åº¦ï¼ˆæ­¥æ•°ï¼‰(é»˜è®¤: 160æ­¥ï¼Œå¯¹åº”8å°æ—¶)",
    )

    # ç®—æ³•é€‰æ‹©å‚æ•°
    parser.add_argument(
        "--algorithm",
        type=str,
        default="scipy_minimize",
        choices=["scipy_minimize", "SCE_UA", "genetic_algorithm"],
        help="ä¼˜åŒ–ç®—æ³•é€‰æ‹© (é»˜è®¤: scipy_minimize)",
    )

    # scipyä¼˜åŒ–å‚æ•°
    parser.add_argument(
        "--max-iterations",
        type=int,
        default=500,
        help="scipyä¼˜åŒ–æœ€å¤§è¿­ä»£æ¬¡æ•° (é»˜è®¤: 500)",
    )

    parser.add_argument(
        "--method",
        type=str,
        default="SLSQP",
        help="scipyä¼˜åŒ–æ–¹æ³• (é»˜è®¤: SLSQP)",
    )

    # SCE-UAå‚æ•°
    parser.add_argument(
        "--rep",
        type=int,
        default=1000,
        help="SCE-UAç®—æ³•repetitions (é»˜è®¤: 1000)",
    )

    parser.add_argument(
        "--random-seed",
        type=int,
        default=1234,
        help="éšæœºç§å­ (é»˜è®¤: 1234)",
    )

    # é—ä¼ ç®—æ³•å‚æ•°
    parser.add_argument(
        "--pop-size",
        type=int,
        default=80,
        help="é—ä¼ ç®—æ³•ç§ç¾¤å¤§å° (é»˜è®¤: 80)",
    )

    parser.add_argument(
        "--n-generations",
        type=int,
        default=50,
        help="é—ä¼ ç®—æ³•è¿›åŒ–ä»£æ•° (é»˜è®¤: 50)",
    )

    parser.add_argument(
        "--cx-prob",
        type=float,
        default=0.7,
        help="é—ä¼ ç®—æ³•äº¤å‰æ¦‚ç‡ (é»˜è®¤: 0.7)",
    )

    parser.add_argument(
        "--mut-prob",
        type=float,
        default=0.2,
        help="é—ä¼ ç®—æ³•å˜å¼‚æ¦‚ç‡ (é»˜è®¤: 0.2)",
    )

    parser.add_argument(
        "--save-freq",
        type=int,
        default=5,
        help="é—ä¼ ç®—æ³•ä¿å­˜é¢‘ç‡ï¼ˆæ¯å‡ ä»£ä¿å­˜ä¸€æ¬¡ï¼‰ (é»˜è®¤: 5)",
    )

    parser.add_argument(
        "--no-peak-obs", action="store_true", help="ä¸åŒ…å«æ´ªå³°è§‚æµ‹å€¼"
    )

    parser.add_argument(
        "--quiet", "-q", action="store_true", help="é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡ºä¿¡æ¯"
    )

    return parser.parse_args()


def get_category_weights(scheme_name):
    """è·å–åˆ†ç±»æƒé‡æ–¹æ¡ˆ"""
    schemes = {
        "default": {
            "small": {"smoothing_factor": 0.1, "peak_violation_weight": 100.0},
            "medium": {
                "smoothing_factor": 0.5,
                "peak_violation_weight": 500.0,
            },
            "large": {
                "smoothing_factor": 1.0,
                "peak_violation_weight": 1000.0,
            },
        },
        "balanced": {
            "small": {"smoothing_factor": 0.2, "peak_violation_weight": 200.0},
            "medium": {
                "smoothing_factor": 0.2,
                "peak_violation_weight": 200.0,
            },
            "large": {"smoothing_factor": 0.2, "peak_violation_weight": 200.0},
        },
        "aggressive": {
            "small": {"smoothing_factor": 0.05, "peak_violation_weight": 50.0},
            "medium": {
                "smoothing_factor": 0.1,
                "peak_violation_weight": 100.0,
            },
            "large": {
                "smoothing_factor": 0.5,
                "peak_violation_weight": 2000.0,
            },
        },
    }
    return schemes.get(scheme_name, schemes["default"])


def create_model_config(args):
    """åˆ›å»ºæ¨¡å‹é…ç½®"""
    try:
        uh_lengths = json.loads(args.uh_lengths)
    except Exception as e:
        print(f"âŒ å•ä½çº¿é•¿åº¦å‚æ•°è§£æå¤±è´¥: {e}")
        raise

    return {
        "name": "categorized_unit_hydrograph",
        "category_weights": get_category_weights(args.category_weights),
        "uh_lengths": uh_lengths,
        "net_rain_name": "P_eff",
        "obs_flow_name": "Q_obs_eff",
    }


def create_algorithm_config(args):
    """åˆ›å»ºç®—æ³•é…ç½®"""
    if args.algorithm == "scipy_minimize":
        return {
            "name": "scipy_minimize",
            "method": args.method,
            "max_iterations": args.max_iterations,
        }
    elif args.algorithm == "SCE_UA":
        return {
            "name": "SCE_UA",
            "rep": args.rep,
            "random_seed": args.random_seed,
        }
    elif args.algorithm == "genetic_algorithm":
        return {
            "name": "genetic_algorithm",
            "random_seed": args.random_seed,
            "pop_size": args.pop_size,
            "n_generations": args.n_generations,
            "cx_prob": args.cx_prob,
            "mut_prob": args.mut_prob,
            "save_freq": args.save_freq,
        }
    else:
        raise ValueError(f"Unsupported algorithm: {args.algorithm}")


def main():
    """ç»Ÿä¸€æ¥å£åˆ†ç±»å•ä½çº¿ä¼˜åŒ–ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # åˆå§‹åŒ–å›¾è¡¨è®¾ç½®
    setup_matplotlib_chinese()

    # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    verbose = not args.quiet
    include_peak_obs = not args.no_peak_obs

    if verbose:
        print("=" * 60)
        print("ğŸš€ ç»Ÿä¸€æ¥å£åˆ†ç±»å•ä½çº¿ä¼˜åŒ–å·¥å…·å¯åŠ¨")
        print("=" * 60)
        print(f"ğŸ“ æ•°æ®è·¯å¾„: {args.data_path}")
        if args.station_id:
            print(f"ğŸ­ æŒ‡å®šç«™ç‚¹: {args.station_id}")
        print(f"ğŸ“¤ è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"ğŸ”§ ä¼˜åŒ–ç®—æ³•: {args.algorithm}")
        print(f"â±ï¸ é¢„çƒ­æœŸé•¿åº¦: {args.warmup_length} æ­¥")
        print(f"âš™ï¸ åˆ†ç±»æƒé‡æ–¹æ¡ˆ: {args.category_weights}")
        print(f"ğŸ“ å•ä½çº¿é•¿åº¦é…ç½®: {args.uh_lengths}")
        print(f"ğŸ“ˆ åŒ…å«æ´ªå³°è§‚æµ‹å€¼: {include_peak_obs}")
        print("-" * 60)

    # åˆ›å»ºæ•°æ®æºï¼ŒåŠ è½½å¸¦é¢„çƒ­æœŸçš„æ•°æ®
    dataset = FloodEventDatasource(
        args.data_path,
        time_unit=["3h"],
        trange4cache=["1960-01-01 02", "2024-12-31 23"],
        warmup_length=args.warmup_length,  # æ•°æ®æºæä¾›å¸¦é¢„çƒ­æœŸçš„æ•°æ®
    )

    all_event_data = dataset.load_1basin_flood_events(
        station_id=args.station_id,
        flow_unit="mm/3h",
        include_peak_obs=include_peak_obs,  # åˆ†ç±»éœ€è¦æ´ªå³°è§‚æµ‹å€¼
        verbose=verbose,
    )

    dataset.check_event_data_nan(all_event_data)

    if verbose:
        print(f"âœ… æˆåŠŸåŠ è½½ {len(all_event_data)} åœºæ´ªæ°´æ•°æ®ï¼ˆå«é¢„çƒ­æœŸï¼‰")

    # 2. åˆ›å»ºé…ç½®
    model_config = create_model_config(args)
    algorithm_config = create_algorithm_config(args)

    if verbose:
        print(f"\nğŸš€ å¼€å§‹ä½¿ç”¨ç»Ÿä¸€æ¥å£ä¼˜åŒ–åˆ†ç±»å•ä½çº¿...")
        print(f"ğŸ“Š æ¨¡å‹é…ç½®: {model_config['name']}")
        print(f"ğŸ”§ ç®—æ³•é…ç½®: {algorithm_config}")
        print(f"ğŸ“ å•ä½çº¿é•¿åº¦: {model_config['uh_lengths']}")

    # 3. æ‰§è¡Œä¼˜åŒ–ï¼ˆä½¿ç”¨ç»Ÿä¸€æ¥å£ï¼‰
    results = calibrate(
        data=all_event_data,
        model_config=model_config,
        algorithm_config=algorithm_config,
        loss_config={"type": "time_series", "obj_func": "RMSE"},
        output_dir=args.output_dir,
        warmup_length=args.warmup_length,  # ç»Ÿä¸€æ¥å£ä¼šå¤„ç†é¢„çƒ­æœŸ
    )

    # 4. æ£€æŸ¥ä¼˜åŒ–ç»“æœ
    if results["convergence"] != "success" or results["best_params"] is None:
        print("âŒ åˆ†ç±»å•ä½çº¿ä¼˜åŒ–å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        print(f"ä¼˜åŒ–ç»“æœ: {results}")
        return

    if verbose:
        print("\nâœ… åˆ†ç±»å•ä½çº¿ä¼˜åŒ–å®Œæˆï¼")
        print(f"ğŸ¯ æœ€ä¼˜ç›®æ ‡å‡½æ•°å€¼: {results['objective_value']:.6f}")

        # æ˜¾ç¤ºåˆ†ç±»ä¿¡æ¯
        cat_info = results.get("categorization_info", {})
        print(f"ğŸ“Š åˆ†ç±»ä¿¡æ¯:")
        print(f"   åˆ†ç±»é˜ˆå€¼: {cat_info.get('thresholds', 'N/A')}")
        for category, count in cat_info.get("events_per_category", {}).items():
            uh_length = cat_info.get("uh_lengths", {}).get(category, 0)
            print(
                f"   {category.capitalize()}: {count} åœºäº‹ä»¶, UHé•¿åº¦: {uh_length}"
            )

    # 5. æå–å„ç±»åˆ«ä¼˜åŒ–çš„å•ä½çº¿å‚æ•°
    best_uh_by_category = {}
    if results["best_params"]:
        categorized_params = results["best_params"][
            "categorized_unit_hydrograph"
        ]
        for category, category_params in categorized_params.items():
            # è½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼
            param_values = []
            for i in range(len(category_params)):
                param_name = f"uh_{category}_{i+1}"
                if param_name in category_params:
                    param_values.append(category_params[param_name])
            best_uh_by_category[category] = param_values

            if verbose:
                print(
                    f"âœ… {category.capitalize()}ç±»å•ä½çº¿: {len(param_values)} ä¸ªå‚æ•°"
                )

    # 6. ç»˜åˆ¶å„ç±»åˆ«å•ä½çº¿å›¾
    if verbose and best_uh_by_category:
        print("\nğŸ“Š ç»˜åˆ¶å„ç±»åˆ«å•ä½çº¿...")
        for category, uh_params in best_uh_by_category.items():
            if uh_params:
                plot_unit_hydrograph(
                    uh_params,
                    f"ç»Ÿä¸€æ¥å£ä¼˜åŒ– - {category.capitalize()}ç±»å•ä½çº¿",
                )

    # 7. è¯„ä¼°å„ç±»åˆ«å•ä½çº¿æ€§èƒ½
    # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è¿›è¡Œåˆ†ç±»å’Œè¯„ä¼°ï¼Œå› ä¸ºè¯„ä¼°å‡½æ•°éœ€è¦åˆ†ç±»ä¿¡æ¯
    if verbose:
        print("\nğŸ“ˆ æ­£åœ¨è¯„ä¼°å„ç±»åˆ«å•ä½çº¿æ€§èƒ½...")

    # è·å–åˆ†ç±»ä¿¡æ¯

    # å¤„ç†äº‹ä»¶æ•°æ®ï¼ˆç§»é™¤é¢„çƒ­æœŸç”¨äºè¯„ä¼°ï¼‰
    processed_events_for_eval = []
    for event_data in all_event_data:
        eval_event_data = {}
        for key, value in event_data.items():
            if key in [
                "P_eff",
                "net_rain",
                "Q_obs_eff",
                "obs_discharge",
            ] and hasattr(value, "__len__"):
                eval_event_data[key] = (
                    value[args.warmup_length :]
                    if args.warmup_length > 0
                    else value
                )
            else:
                eval_event_data[key] = value
        processed_events_for_eval.append(eval_event_data)

    # åˆ†ç±»äº‹ä»¶
    categorized_events, _ = categorize_floods_by_peak(
        processed_events_for_eval
    )

    final_report_data = []
    for category_name, events_in_category in categorized_events.items():
        uh_params = best_uh_by_category.get(category_name)

        if uh_params:
            # ä½¿ç”¨è¯¥ç±»åˆ«çš„ç‰¹å¾å•ä½çº¿è¯„ä¼°å…¶å†…éƒ¨æ‰€æœ‰äº‹ä»¶
            for event_data in events_in_category:
                result = evaluate_single_event_from_uh(
                    event_data, uh_params, category_name
                )
                final_report_data.append(result)

    # 8. ä¿å­˜å’Œæ˜¾ç¤ºç»“æœ
    if final_report_data:
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        station_suffix = f"_{args.station_id}" if args.station_id else ""
        algorithm_suffix = f"_{args.algorithm}"
        output_filename = os.path.join(
            args.output_dir,
            f"UH_categorized_unified_eva_output_songliao{station_suffix}{algorithm_suffix}.csv",
        )

        report_df_sorted = save_results_to_csv(
            final_report_data,
            output_filename,
            sort_columns=["æ‰€å±ç±»åˆ«", "NSE"],
        )

        if verbose:
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_filename}")
            print(
                f"ğŸ“Š JSONç»“æœå·²ä¿å­˜è‡³: {os.path.join(args.output_dir, 'categorized_unit_hydrograph_calibration_results.json')}"
            )

            # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
            print_report_preview(
                report_df_sorted, "åˆ†ç±»è¯„ä¼°æŠ¥å‘Šé¢„è§ˆ (æŒ‰ç±»åˆ«å’ŒNSEæ’åº)"
            )
            print_category_statistics(report_df_sorted)

            # æ˜¾ç¤ºå„ç±»åˆ«æœ€ä¼˜NSE
            best_nse_by_category = report_df_sorted.groupby("æ‰€å±ç±»åˆ«")[
                "NSE"
            ].max()
            print("\nğŸ¯ å„ç±»åˆ«ä¼˜åŒ–å®Œæˆï¼")
            print(f"   ç®—æ³•: {args.algorithm}")
            print(f"   æƒé‡æ–¹æ¡ˆ: {args.category_weights}")
            print("   å„ç±»åˆ«æœ€ä¼˜NSE:")
            for category, nse in best_nse_by_category.items():
                print(f"     {category}: {nse:.4f}")

    print("\nğŸ‰ ç»Ÿä¸€æ¥å£åˆ†ç±»å•ä½çº¿ä¼˜åŒ–å®Œæˆï¼")


if __name__ == "__main__":
    main()
