"""
Author: Zheng Zhang, supervised by Heng Lv
Date: 2025-07-08 18:05:00
LastEditTime: 2025-07-16 16:36:39
LastEditors: Wenyu Ouyang
Description: ä½¿ç”¨æ´ªæ°´äº‹ä»¶æ•°æ®ç”Ÿæˆå”¯ä¸€çš„å…±äº«å•ä½çº¿çš„æ‰§è¡Œè„šæœ¬ï¼ˆæ”¯æŒCSVå’ŒExcelæ•°æ®æºï¼‰
FilePath: \hydromodel_dev\scripts\run_shared_uh_optimization.py
Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.
"""

import sys
import os
import argparse
from hydrodatasource.configs.config import SETTING
from hydromodel_dev.floodevent import check_event_data_nan
from hydromodel_dev.unit_hydrograph import optimize_shared_unit_hydrograph
from plot_rrevents import plot_unit_hydrograph

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from hydromodel_dev import (
    setup_matplotlib,
    load_and_preprocess_events_unified,
    evaluate_single_event,
    save_results_to_csv,
    print_report_preview,
)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="å…±äº«å•ä½çº¿ä¼˜åŒ–å·¥å…· - æ¾è¾½æ²³æµåŸŸæ•°æ®ä¸“ç”¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¤„ç†ç¢§æµæ²³ç«™ç‚¹æ•°æ®
  python run_shared_uh_optimization.py --station-id songliao_21401550
        """,
    )

    parser.add_argument(
        "--data-path",
        "-d",
        type=str,
        default=os.path.join(
            SETTING["local_data_path"]["datasets-interim"], "songliaorrevent"
        ),
        help="åœºæ¬¡æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„",
    )

    parser.add_argument(
        "--station-id",
        type=str,
        default="songliao_21401550",
        help="ç«™ç‚¹ID (å¦‚: songliao_21401550)",
    )

    parser.add_argument(
        "--output-dir", "-o", type=str, default="results/", help="è¾“å‡ºç»“æœç›®å½•"
    )

    parser.add_argument(
        "--common-n-uh",
        type=int,
        default=24,
        help="å…±äº«å•ä½çº¿é•¿åº¦ (é»˜è®¤: 24)",
    )

    parser.add_argument(
        "--smoothing-factor",
        type=float,
        default=0.1,
        help="å¹³æ»‘æ€§æƒ©ç½šæƒé‡å› å­ (é»˜è®¤: 0.1)",
    )

    parser.add_argument(
        "--peak-violation-weight",
        type=float,
        default=10000.0,
        help="å•å³°è¿åæƒ©ç½šæƒé‡å› å­ (é»˜è®¤: 10000.0)",
    )

    parser.add_argument(
        "--max-iterations",
        type=int,
        default=500,
        help="ä¼˜åŒ–æœ€å¤§è¿­ä»£æ¬¡æ•° (é»˜è®¤: 500)",
    )

    parser.add_argument(
        "--no-peak-obs", action="store_true", help="ä¸åŒ…å«æ´ªå³°è§‚æµ‹å€¼"
    )

    parser.add_argument(
        "--quiet", "-q", action="store_true", help="é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡ºä¿¡æ¯"
    )

    return parser.parse_args()


def main():
    """å…±äº«å•ä½çº¿ä¼˜åŒ–ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # åˆå§‹åŒ–å›¾è¡¨è®¾ç½®
    setup_matplotlib()

    # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    verbose = not args.quiet
    include_peak_obs = not args.no_peak_obs
    if verbose:
        print("=" * 60)
        print("ğŸš€ æ¾è¾½æµåŸŸå•ä½çº¿ä¼˜åŒ–å·¥å…·å¯åŠ¨")
        print("=" * 60)
        print(f"ğŸ“ æ•°æ®è·¯å¾„: {args.data_path}")
        if args.station_id:
            print(f"ğŸ­ æŒ‡å®šç«™ç‚¹: {args.station_id}")
        print(f"ğŸ“¤ è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"âš™ï¸ å¹³æ»‘å› å­: {args.smoothing_factor}")
        print(f"âš™ï¸ å•å³°æƒ©ç½šå› å­: {args.peak_violation_weight}")
        print(f"ğŸ”„ æœ€å¤§è¿­ä»£æ¬¡æ•°: {args.max_iterations}")
        print(f"ğŸ“ˆ åŒ…å«æ´ªå³°è§‚æµ‹å€¼: {include_peak_obs}")
        print("-" * 60)
    all_event_data = load_and_preprocess_events_unified(
        data_dir=args.data_path,
        station_id=args.station_id,
        include_peak_obs=include_peak_obs,
        verbose=verbose,
    )
    check_event_data_nan(all_event_data)

    # 2. ä¼˜åŒ–å‚æ•°
    common_n_uh = args.common_n_uh
    smoothing_factor = args.smoothing_factor
    peak_violation_weight = args.peak_violation_weight
    apply_peak_penalty = common_n_uh > 2  # æ˜¯å¦åº”ç”¨å•å³°æƒ©ç½šï¼ˆé•¿åº¦>2æ—¶ï¼‰

    if verbose:
        print(
            f"\nğŸš€ å¼€å§‹ä½¿ç”¨ {len(all_event_data)} åœºæ´ªæ°´æ•°æ®ä¼˜åŒ–å…±äº«å•ä½çº¿..."
        )
        print(
            f"âš™ï¸ å¹³æ»‘å› å­: {smoothing_factor}, å•å³°æƒ©ç½šå› å­: {peak_violation_weight if apply_peak_penalty else 'N/A'}"
        )

    # æ‰§è¡Œä¼˜åŒ–ï¼ˆè°ƒç”¨å…¬ç”¨å‡½æ•°ï¼‰
    U_optimized_shared = optimize_shared_unit_hydrograph(
        all_event_data,
        common_n_uh,
        smoothing_factor,
        peak_violation_weight,
        apply_peak_penalty,
        max_iterations=args.max_iterations,
        verbose=verbose,
    )

    if U_optimized_shared is None:
        print("âŒ å…±äº«å•ä½çº¿ä¼˜åŒ–å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return
    if verbose:
        print("\nâœ… å…±äº«å•ä½çº¿ä¼˜åŒ–å®Œæˆï¼")

    # 3. ç»˜åˆ¶å…±äº«å•ä½çº¿å›¾
    if verbose:
        plot_unit_hydrograph(
            U_optimized_shared,
            "å…±äº«ä¼˜åŒ–å•ä½çº¿",
            smoothing_factor,
            peak_violation_weight if apply_peak_penalty else None,
        )

    # 4. è¯„ä¼°å…±äº«å•ä½çº¿åœ¨æ‰€æœ‰äº‹ä»¶ä¸Šçš„è¡¨ç°
    if verbose:
        print("\nğŸ“ˆ æ­£åœ¨è¯„ä¼°å…±äº«å•ä½çº¿æ€§èƒ½...")
    final_report_data = []

    for event_data in all_event_data:
        result = evaluate_single_event(event_data, U_optimized_shared)
        final_report_data.append(result)

    # 5. ä¿å­˜å’Œæ˜¾ç¤ºç»“æœ
    # ç”Ÿæˆæ¾è¾½æ²³æ•°æ®è¾“å‡ºæ–‡ä»¶å
    station_suffix = f"_{args.station_id}" if args.station_id else ""
    output_filename = os.path.join(
        args.output_dir,
        f"UH_shared_eva_output_songliao{station_suffix}.csv",
    )
    data_source_label = "æ¾è¾½æ²³æ•°æ®æº"

    report_df_sorted = save_results_to_csv(
        final_report_data, output_filename, sort_columns=["NSE"]
    )

    if report_df_sorted is not None and verbose:
        # æ‰“å°æŠ¥å‘Šé¢„è§ˆ
        print_report_preview(
            report_df_sorted,
            f"å…±äº«å•ä½çº¿è¯„ä¼°æŠ¥å‘Šé¢„è§ˆ ({data_source_label}, æŒ‰NSEæ’åº)",
        )

        # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        best_nse = report_df_sorted["NSE"].max()
        print(f"\nğŸ¯ ä¼˜åŒ–å®Œæˆï¼")
        print(f"   æ•°æ®æº: {data_source_label}")
        if args.station_id:
            print(f"   å¤„ç†ç«™ç‚¹: {args.station_id}")
        print(f"   å…±äº«å•ä½çº¿é•¿åº¦: {common_n_uh}")
        print(f"   æœ€ä¼˜NSE: {best_nse:.4f}")
        print(f"   ç»“æœä¿å­˜è‡³: {output_filename}")

    if not verbose:
        # é™é»˜æ¨¡å¼ä¸‹ä¹Ÿè¾“å‡ºå…³é”®ä¿¡æ¯
        best_nse = (
            report_df_sorted["NSE"].max()
            if report_df_sorted is not None
            else 0
        )
        print(
            f"ä¼˜åŒ–å®Œæˆ - å•ä½çº¿é•¿åº¦: {common_n_uh}, æœ€ä¼˜NSE: {best_nse:.4f}, è¾“å‡º: {output_filename}"
        )


if __name__ == "__main__":
    main()
