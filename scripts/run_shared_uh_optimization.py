"""
Author: Wenyu Ouyang
Date: 2025-08-06
LastEditTime: 2025-08-07 08:42:43
LastEditors: Wenyu Ouyang
Description: This script demonstrates the power of the unified calibration interface by using the general calibrate() function instead of model-specific functions. The same interface works for: Unit hydrograph models, Categorized unit hydrograph models, Traditional hydrological models (XAJ, GR series, etc.), All optimization algorithms (scipy, SCE-UA, genetic algorithms)
FilePath: \hydromodel\scripts\run_shared_uh_optimization_unified.py
Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.
"""

import os
import argparse
from hydroutils.hydro_plot import (
    plot_unit_hydrograph,
    setup_matplotlib_chinese,
)
from hydrodatasource.configs.config import SETTING
from hydrodatasource.reader.floodevent import (
    FloodEventDatasource,
)
from hydromodel.models.unit_hydrograph import (
    evaluate_single_event_from_uh,
    print_report_preview,
    save_results_to_csv,
)
from hydromodel.trainers.unified_calibrate import calibrate


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ç»Ÿä¸€æ¥å£å•ä½çº¿ä¼˜åŒ–å·¥å…· - æ¾è¾½æ²³æµåŸŸæ•°æ®ä¸“ç”¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨scipyä¼˜åŒ–å¤„ç†ç¢§æµæ²³ç«™ç‚¹æ•°æ®
  python run_shared_uh_optimization_unified.py --station-id songliao_21401550 --algorithm scipy_minimize
  
  # ä½¿ç”¨SCE-UAç®—æ³•
  python run_shared_uh_optimization_unified.py --station-id songliao_21401550 --algorithm SCE_UA --rep 2000
        """,
    )

    parser.add_argument(
        "--data-path",
        "-d",
        type=str,
        default=os.path.join(
            SETTING["local_data_path"]["datasets-interim"], "songliaorrevent"
        ),
        help="åœºæ¬¡æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„",
    )

    parser.add_argument(
        "--station-id",
        type=str,
        default="songliao_21401550",
        help="ç«™ç‚¹ID (å¦‚: songliao_21401550)",
    )

    parser.add_argument(
        "--output-dir", "-o", type=str, default="results/", help="è¾“å‡ºç»“æœç›®å½•"
    )

    parser.add_argument(
        "--common-n-uh",
        type=int,
        default=24,
        help="å…±äº«å•ä½çº¿é•¿åº¦ (é»˜è®¤: 24)",
    )

    parser.add_argument(
        "--smoothing-factor",
        type=float,
        default=0.1,
        help="å¹³æ»‘æ€§æƒ©ç½šæƒé‡å› å­ (é»˜è®¤: 0.1)",
    )

    parser.add_argument(
        "--peak-violation-weight",
        type=float,
        default=10000.0,
        help="å•å³°è¿åæƒ©ç½šæƒé‡å› å­ (é»˜è®¤: 10000.0)",
    )

    parser.add_argument(
        "--warmup-length",
        type=int,
        default=8
        * 60,  # 8 hours * 60 minutes / 3 hours = 160 steps for 3h data
        help="é¢„çƒ­æœŸé•¿åº¦ï¼ˆæ­¥æ•°ï¼‰(é»˜è®¤: 160æ­¥ï¼Œå¯¹åº”8å°æ—¶)",
    )

    # ç®—æ³•é€‰æ‹©å‚æ•°
    parser.add_argument(
        "--algorithm",
        type=str,
        default="scipy_minimize",
        choices=["scipy_minimize", "SCE_UA", "genetic_algorithm"],
        help="ä¼˜åŒ–ç®—æ³•é€‰æ‹© (é»˜è®¤: scipy_minimize)",
    )

    # scipyä¼˜åŒ–å‚æ•°
    parser.add_argument(
        "--max-iterations",
        type=int,
        default=500,
        help="scipyä¼˜åŒ–æœ€å¤§è¿­ä»£æ¬¡æ•° (é»˜è®¤: 500)",
    )

    parser.add_argument(
        "--method",
        type=str,
        default="SLSQP",
        help="scipyä¼˜åŒ–æ–¹æ³• (é»˜è®¤: SLSQP)",
    )

    # SCE-UAå‚æ•°
    parser.add_argument(
        "--rep",
        type=int,
        default=1000,
        help="SCE-UAç®—æ³•repetitions (é»˜è®¤: 1000)",
    )

    parser.add_argument(
        "--random-seed",
        type=int,
        default=1234,
        help="éšæœºç§å­ (é»˜è®¤: 1234)",
    )

    # é—ä¼ ç®—æ³•å‚æ•°
    parser.add_argument(
        "--pop-size",
        type=int,
        default=50,
        help="é—ä¼ ç®—æ³•ç§ç¾¤å¤§å° (é»˜è®¤: 50)",
    )

    parser.add_argument(
        "--n-generations",
        type=int,
        default=40,
        help="é—ä¼ ç®—æ³•è¿›åŒ–ä»£æ•° (é»˜è®¤: 40)",
    )

    parser.add_argument(
        "--cx-prob",
        type=float,
        default=0.5,
        help="é—ä¼ ç®—æ³•äº¤å‰æ¦‚ç‡ (é»˜è®¤: 0.5)",
    )

    parser.add_argument(
        "--mut-prob",
        type=float,
        default=0.2,
        help="é—ä¼ ç®—æ³•å˜å¼‚æ¦‚ç‡ (é»˜è®¤: 0.2)",
    )

    parser.add_argument(
        "--save-freq",
        type=int,
        default=5,
        help="é—ä¼ ç®—æ³•ä¿å­˜é¢‘ç‡ï¼ˆæ¯å‡ ä»£ä¿å­˜ä¸€æ¬¡ï¼‰ (é»˜è®¤: 5)",
    )

    parser.add_argument(
        "--no-peak-obs", action="store_true", help="ä¸åŒ…å«æ´ªå³°è§‚æµ‹å€¼"
    )

    parser.add_argument(
        "--quiet", "-q", action="store_true", help="é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡ºä¿¡æ¯"
    )

    return parser.parse_args()


def create_model_config(args):
    """åˆ›å»ºæ¨¡å‹é…ç½®"""
    return {
        "name": "unit_hydrograph",
        "n_uh": args.common_n_uh,
        "smoothing_factor": args.smoothing_factor,
        "peak_violation_weight": args.peak_violation_weight,
        "apply_peak_penalty": args.common_n_uh > 2,
        "net_rain_name": "P_eff",
        "obs_flow_name": "Q_obs_eff",
    }


def create_algorithm_config(args):
    """åˆ›å»ºç®—æ³•é…ç½®"""
    if args.algorithm == "scipy_minimize":
        return {
            "name": "scipy_minimize",
            "method": args.method,
            "max_iterations": args.max_iterations,
        }
    elif args.algorithm == "SCE_UA":
        return {
            "name": "SCE_UA",
            "rep": args.rep,
            "random_seed": args.random_seed,
        }
    elif args.algorithm == "genetic_algorithm":
        return {
            "name": "genetic_algorithm",
            "random_seed": args.random_seed,
            "pop_size": args.pop_size,
            "n_generations": args.n_generations,
            "cx_prob": args.cx_prob,
            "mut_prob": args.mut_prob,
            "save_freq": args.save_freq,
        }
    else:
        raise ValueError(f"Unsupported algorithm: {args.algorithm}")


def main():
    """ç»Ÿä¸€æ¥å£å•ä½çº¿ä¼˜åŒ–ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # åˆå§‹åŒ–å›¾è¡¨è®¾ç½®
    setup_matplotlib_chinese()

    # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    verbose = not args.quiet
    include_peak_obs = not args.no_peak_obs

    if verbose:
        print("=" * 60)
        print("ğŸš€ ç»Ÿä¸€æ¥å£å•ä½çº¿ä¼˜åŒ–å·¥å…·å¯åŠ¨")
        print("=" * 60)
        print(f"ğŸ“ æ•°æ®è·¯å¾„: {args.data_path}")
        if args.station_id:
            print(f"ğŸ­ æŒ‡å®šç«™ç‚¹: {args.station_id}")
        print(f"ğŸ“¤ è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"ğŸ”§ ä¼˜åŒ–ç®—æ³•: {args.algorithm}")
        print(f"â±ï¸ é¢„çƒ­æœŸé•¿åº¦: {args.warmup_length} æ­¥")
        print(f"âš™ï¸ å¹³æ»‘å› å­: {args.smoothing_factor}")
        print(f"âš™ï¸ å•å³°æƒ©ç½šå› å­: {args.peak_violation_weight}")
        print(f"ğŸ“ˆ åŒ…å«æ´ªå³°è§‚æµ‹å€¼: {include_peak_obs}")
        print("-" * 60)

    # åˆ›å»ºæ•°æ®æºï¼ŒåŠ è½½å¸¦é¢„çƒ­æœŸçš„æ•°æ®
    dataset = FloodEventDatasource(
        args.data_path,
        time_unit=["3h"],
        trange4cache=["1960-01-01 02", "2024-12-31 23"],
        warmup_length=args.warmup_length,  # æ•°æ®æºæä¾›å¸¦é¢„çƒ­æœŸçš„æ•°æ®
    )

    all_event_data = dataset.load_1basin_flood_events(
        station_id=args.station_id,
        flow_unit="mm/3h",
        include_peak_obs=include_peak_obs,
        verbose=verbose,
    )

    dataset.check_event_data_nan(all_event_data)

    if verbose:
        print(f"âœ… æˆåŠŸåŠ è½½ {len(all_event_data)} åœºæ´ªæ°´æ•°æ®ï¼ˆå«é¢„çƒ­æœŸï¼‰")

    # 2. åˆ›å»ºé…ç½®
    model_config = create_model_config(args)
    algorithm_config = create_algorithm_config(args)

    if verbose:
        print(f"\nğŸš€ å¼€å§‹ä½¿ç”¨ç»Ÿä¸€æ¥å£ä¼˜åŒ–å•ä½çº¿...")
        print(f"âœ¨ ä½¿ç”¨ç»Ÿä¸€çš„ calibrate() å‡½æ•° - ä¸€ä¸ªæ¥å£æ”¯æŒæ‰€æœ‰æ¨¡å‹å’Œç®—æ³•!")
        print(f"ğŸ“Š æ¨¡å‹ç±»å‹: {model_config['name']}")
        print(f"ğŸ”§ ç®—æ³•ç±»å‹: {algorithm_config['name']}")
        print(f"ğŸ“ˆ ç›®æ ‡å‡½æ•°: RMSE")
        print(f"ğŸ¯ ç»Ÿä¸€æ¥å£çš„ä¼˜åŠ¿: ç›¸åŒçš„è°ƒç”¨æ–¹å¼ï¼Œä¸€è‡´çš„è¿”å›æ ¼å¼")

    # 3. æ‰§è¡Œä¼˜åŒ–ï¼ˆä½¿ç”¨ç»Ÿä¸€æ¥å£ï¼‰
    results = calibrate(
        data=all_event_data,
        model_config=model_config,
        algorithm_config=algorithm_config,
        loss_config={"type": "time_series", "obj_func": "RMSE"},
        output_dir=args.output_dir,
        warmup_length=args.warmup_length,  # ç»Ÿä¸€æ¥å£ä¼šå¤„ç†é¢„çƒ­æœŸ
    )

    # 4. æ£€æŸ¥ä¼˜åŒ–ç»“æœ
    if results["convergence"] != "success" or results["best_params"] is None:
        print("âŒ å•ä½çº¿ä¼˜åŒ–å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        print(f"ä¼˜åŒ–ç»“æœ: {results}")
        return

    # æå–ä¼˜åŒ–çš„å•ä½çº¿å‚æ•°
    uh_params_dict = results["best_params"]["unit_hydrograph"]
    U_optimized_shared = [
        uh_params_dict[f"uh_{i+1}"] for i in range(args.common_n_uh)
    ]

    if verbose:
        print("\nâœ… å•ä½çº¿ä¼˜åŒ–å®Œæˆï¼")
        print(f"ğŸ¯ æœ€ä¼˜ç›®æ ‡å‡½æ•°å€¼: {results['objective_value']:.6f}")
        print(f"ğŸ“‹ ä¼˜åŒ–å‚æ•°æ•°é‡: {len(U_optimized_shared)}")

    # 5. ç»˜åˆ¶å…±äº«å•ä½çº¿å›¾
    if verbose:
        apply_peak_penalty = args.common_n_uh > 2
        plot_unit_hydrograph(
            U_optimized_shared,
            "ç»Ÿä¸€æ¥å£ä¼˜åŒ–å•ä½çº¿",
            args.smoothing_factor,
            args.peak_violation_weight if apply_peak_penalty else None,
        )

    # 6. è¯„ä¼°å•ä½çº¿æ€§èƒ½
    # æ³¨æ„ï¼ševaluate_single_event_from_uh éœ€è¦ä½¿ç”¨æ²¡æœ‰é¢„çƒ­æœŸçš„æ•°æ®è¿›è¡Œè¯„ä¼°
    if verbose:
        print("\nğŸ“ˆ æ­£åœ¨è¯„ä¼°å•ä½çº¿æ€§èƒ½...")

    final_report_data = []
    for event_data in all_event_data:
        # å¯¹äºè¯„ä¼°ï¼Œæˆ‘ä»¬éœ€è¦ä»åŸå§‹äº‹ä»¶æ•°æ®ä¸­ç§»é™¤é¢„çƒ­æœŸ
        # å› ä¸ºå•ä½çº¿æ¨¡å‹æœ¬èº«ä¸éœ€è¦é¢„çƒ­æœŸ
        eval_event_data = {}
        for key, value in event_data.items():
            if key in [
                "P_eff",
                "net_rain",
                "Q_obs_eff",
                "obs_discharge",
            ] and hasattr(value, "__len__"):
                # ç§»é™¤é¢„çƒ­æœŸç”¨äºè¯„ä¼°
                eval_event_data[key] = (
                    value[args.warmup_length :]
                    if args.warmup_length > 0
                    else value
                )
            else:
                eval_event_data[key] = value

        result = evaluate_single_event_from_uh(
            eval_event_data, U_optimized_shared
        )
        final_report_data.append(result)

    # 7. ä¿å­˜å’Œæ˜¾ç¤ºç»“æœ
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    station_suffix = f"_{args.station_id}" if args.station_id else ""
    algorithm_suffix = f"_{args.algorithm}"
    output_filename = os.path.join(
        args.output_dir,
        f"UH_unified_eva_output_songliao{station_suffix}{algorithm_suffix}.csv",
    )

    report_df_sorted = save_results_to_csv(
        final_report_data, output_filename, sort_columns=["NSE"]
    )

    if verbose:
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_filename}")
        print(
            f"ğŸ“Š JSONç»“æœå·²ä¿å­˜è‡³: {os.path.join(args.output_dir, 'unit_hydrograph_calibration_results.json')}"
        )

        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        print("\nğŸ“Š å•ä½çº¿æ€§èƒ½ç»Ÿè®¡:")
        print(f"   å¹³å‡NSE: {report_df_sorted['NSE'].mean():.4f}")
        print(
            f"   å¹³å‡æ´ªé‡ç›¸è¯¯: {report_df_sorted['æ´ªé‡ç›¸è¯¯(%)'].mean():.2f}%"
        )
        print(
            f"   å¹³å‡æ´ªå³°ç›¸è¯¯: {report_df_sorted['æ´ªå³°ç›¸è¯¯(%)'].mean():.2f}%"
        )

        # æ˜¾ç¤ºå‰å‡ ä¸ªæœ€ä½³äº‹ä»¶
        print_report_preview(report_df_sorted, "ç»Ÿä¸€æ¥å£ä¼˜åŒ–å•ä½çº¿", top_n=5)

    print("\nğŸ‰ ç»Ÿä¸€æ¥å£å•ä½çº¿ä¼˜åŒ–å®Œæˆï¼")
    print("âœ… æˆåŠŸä½¿ç”¨ç»Ÿä¸€çš„ calibrate() å‡½æ•°å®Œæˆä¼˜åŒ–")
    print("ğŸŒŸ ç»Ÿä¸€æ¥å£çš„ä¼˜åŠ¿:")
    print("   - ä¸€ä¸ªå‡½æ•°æ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹")
    print("   - ä¸€è‡´çš„å‚æ•°ç»“æ„å’Œè¿”å›æ ¼å¼")
    print("   - æ–¹ä¾¿çš„ç®—æ³•åˆ‡æ¢å’Œæ¯”è¾ƒ")
    print("   - æ˜“äºæ‰©å±•æ–°æ¨¡å‹å’Œç®—æ³•")


if __name__ == "__main__":
    main()
