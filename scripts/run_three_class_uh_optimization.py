"""
Author: Zheng Zhang, supervised by Heng Lv
Date: 2025-07-08 17:56:32
LastEditTime: 2025-07-16 16:31:53
LastEditors: Wenyu Ouyang
Description: ä¸‰ç±»åˆ«å•ä½çº¿ä¼˜åŒ–è„šæœ¬ï¼ˆæ”¯æŒCSVå’ŒExcelæ•°æ®æºï¼‰-- å°†æ´ªæ°´æ•°æ®æ ¹æ®å…¶æ´ªå³°å¤§å°åˆ†ä¸ºä¸‰ç±»ï¼ˆå°ã€ä¸­ã€å¤§ï¼‰ï¼Œåˆ†åˆ«æ¨æ±‚ç‰¹å¾å•ä½çº¿
FilePath: \hydromodel_dev\scripts\run_three_class_uh_optimization.py
Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.
"""

import sys
import os
import argparse
import json
from hydrodatasource.configs.config import SETTING
from plot_rrevents import plot_unit_hydrograph

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from hydromodel_dev import (
    optimize_uh_for_group,
    setup_matplotlib,
    load_and_preprocess_events_unified,
    categorize_floods_by_peak,
    evaluate_single_event,
    save_results_to_csv,
    print_report_preview,
    print_category_statistics,
)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸‰ç±»åˆ«å•ä½çº¿ä¼˜åŒ–å·¥å…· - æ”¯æŒCSVå’ŒExcelæ•°æ®æº",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""songliaorreventæ•°æ®é›†åŠ è½½æ•°æ®""",
    )

    parser.add_argument(
        "--data-path",
        "-d",
        type=str,
        default=os.path.join(
            SETTING["local_data_path"]["datasets-interim"], "songliaorrevent"
        ),
        help="æ¾è¾½åœºæ¬¡æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„",
    )

    parser.add_argument(
        "--station-id",
        type=str,
        default="songliao_21401550",
        help="æ¾è¾½åœºæ¬¡æ•°æ®é›†ç«™ç‚¹ID (å¦‚: songliao_21401550)ï¼Œä»…å¯¹æ¾è¾½åœºæ¬¡æ•°æ®é›†æœ‰æ•ˆ",
    )

    parser.add_argument(
        "--output-dir", "-o", type=str, default="results/", help="è¾“å‡ºç»“æœç›®å½•"
    )

    parser.add_argument(
        "--category-weights",
        type=str,
        default="default",
        help="åˆ†ç±»æƒé‡æ–¹æ¡ˆ: default, balanced, aggressive",
    )

    parser.add_argument(
        "--uh-lengths",
        type=str,
        default='{"small":8,"medium":16,"large":24}',
        help='å„ç±»åˆ«å•ä½çº¿é•¿åº¦ï¼ŒJSONæ ¼å¼ï¼Œå¦‚: \'{"small":8,"medium":16,"large":24}\'',
    )

    # parser.add_argument(
    #     "--common-n-uh",
    #     type=int,
    #     default=24,
    #     help="å…±äº«å•ä½çº¿é•¿åº¦ (é»˜è®¤: 24)",
    # )

    parser.add_argument(
        "--quiet", "-q", action="store_true", help="é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡ºä¿¡æ¯"
    )

    return parser.parse_args()


def validate_data_path(data_path):
    """éªŒè¯æ¾è¾½æ²³æ•°æ®è·¯å¾„çš„æœ‰æ•ˆæ€§"""
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        return False

    if not os.path.isdir(data_path):
        print(f"âŒ æ¾è¾½æ²³æ•°æ®æºéœ€è¦æ–‡ä»¶å¤¹è·¯å¾„: {data_path}")
        return False

    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¾è¾½æ²³æ•°æ®æ–‡ä»¶
    try:
        csv_files = [
            f
            for f in os.listdir(data_path)
            if f.startswith("songliao_") and f.endswith(".csv")
        ]
        if not csv_files:
            print(f"âŒ æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°æ¾è¾½æ²³æ•°æ®æ–‡ä»¶: {data_path}")
            return False
    except Exception as e:
        print(f"âŒ æ— æ³•è®¿é—®æ•°æ®æ–‡ä»¶å¤¹: {str(e)}")
        return False

    return True


def get_category_weights(scheme_name):
    """è·å–åˆ†ç±»æƒé‡æ–¹æ¡ˆ"""
    schemes = {
        "default": {
            "small": {"smoothing_factor": 0.1, "peak_violation_weight": 100.0},
            "medium": {
                "smoothing_factor": 0.5,
                "peak_violation_weight": 500.0,
            },
            "large": {
                "smoothing_factor": 1.0,
                "peak_violation_weight": 1000.0,
            },
        },
        "balanced": {
            "small": {"smoothing_factor": 0.2, "peak_violation_weight": 200.0},
            "medium": {
                "smoothing_factor": 0.2,
                "peak_violation_weight": 200.0,
            },
            "large": {"smoothing_factor": 0.2, "peak_violation_weight": 200.0},
        },
        "aggressive": {
            "small": {"smoothing_factor": 0.05, "peak_violation_weight": 50.0},
            "medium": {
                "smoothing_factor": 0.1,
                "peak_violation_weight": 100.0,
            },
            "large": {
                "smoothing_factor": 0.5,
                "peak_violation_weight": 2000.0,
            },
        },
    }
    return schemes.get(scheme_name, schemes["default"])


def main():
    """ä¸‰ç±»åˆ«å•ä½çº¿ä¼˜åŒ–ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # åˆå§‹åŒ–å›¾è¡¨è®¾ç½®
    setup_matplotlib()
    # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    verbose = not args.quiet
    if verbose:
        print("=" * 60)
        print("ğŸš€ ä¸‰ç±»åˆ«å•ä½çº¿ä¼˜åŒ–å·¥å…·å¯åŠ¨")
        print("=" * 60)
        print(f"ğŸ“ æ•°æ®è·¯å¾„: {args.data_path}")
        print(f"ğŸ“¤ è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"âš™ï¸ æƒé‡æ–¹æ¡ˆ: {args.category_weights}")
        print("-" * 60)
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)
    all_events_data = load_and_preprocess_events_unified(
        data_dir=args.data_path,
        station_id=args.station_id,
        include_peak_obs=True,  # ä¸‰ç±»åˆ«åˆ†æéœ€è¦æ´ªå³°è§‚æµ‹å€¼
        verbose=verbose,
    )
    if all_events_data is None:
        return

    # 2. æ´ªæ°´äº‹ä»¶åˆ†ç±»ï¼ˆåŸºäºæ´ªå³°ï¼‰
    categorized_events, (threshold_low, threshold_high) = (
        categorize_floods_by_peak(all_events_data)
    )
    if categorized_events is None:
        return

    if verbose:
        print(
            f"\nğŸ“Š æ´ªå³°åˆ†ç±»é˜ˆå€¼ï¼šå°æ´ªæ°´ â‰¤ {threshold_low:.2f} mm/3h < "
            f"ä¸­æ´ªæ°´ â‰¤ {threshold_high:.2f} mm/3h < å¤§æ´ªæ°´"
        )
        print("ğŸ“ˆ å„ç±»åˆ«æ´ªæ°´äº‹ä»¶æ•°é‡:")
        for category, events in categorized_events.items():
            print(f"  ğŸ·ï¸ {category.capitalize()} æ´ªæ°´: {len(events)} åœº")

    # 3. ä¸ºæ¯ä¸ªç±»åˆ«æ¨æ±‚ç‰¹å¾å•ä½çº¿
    category_weights = get_category_weights(args.category_weights)

    # è§£æå•ä½çº¿é•¿åº¦å‚æ•°
    try:
        uh_length_by_category = json.loads(args.uh_lengths)
    except Exception as e:
        print("âŒ å•ä½çº¿é•¿åº¦å‚æ•°è§£æå¤±è´¥: {}".format(e))
        return

    optimized_uhs = {}
    if verbose:
        print("\nğŸš€ å¼€å§‹ä¸ºå„ç±»åˆ«æ¨æ±‚ç‰¹å¾å•ä½çº¿...")

    for category_name, events in categorized_events.items():
        weights = category_weights.get(category_name, {})
        n_uh = uh_length_by_category.get(category_name, 24)  # é»˜è®¤24
        optimized_uhs[category_name] = optimize_uh_for_group(
            events, category_name, weights, n_uh
        )

    # 4. ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾å•ä½çº¿
    if verbose:
        print("\nğŸ“Š ç»˜åˆ¶å„ç±»åˆ«ç‰¹å¾å•ä½çº¿...")
        for category_name, U_optimized_cat in optimized_uhs.items():
            if U_optimized_cat is not None:
                plot_unit_hydrograph(
                    U_optimized_cat,
                    f"ç‰¹å¾å•ä½çº¿ - ç±»åˆ«: {category_name.capitalize()}",
                )
            else:
                print(f"âš ï¸ ç±»åˆ« '{category_name}' çš„å•ä½çº¿ä¼˜åŒ–å¤±è´¥ï¼Œè·³è¿‡ç»˜å›¾")

    # 5. è¯„ä¼°å¹¶æ•´åˆæ‰€æœ‰ç»“æœ
    if verbose:
        print("\nğŸ“ˆ å¼€å§‹è¯„ä¼°å„ç±»åˆ«å•ä½çº¿æ€§èƒ½...")
    final_report_data = []

    for category_name, events_in_category in categorized_events.items():
        U_optimized_cat = optimized_uhs.get(category_name)

        # ä½¿ç”¨è¯¥ç±»åˆ«çš„ç‰¹å¾å•ä½çº¿è¯„ä¼°å…¶å†…éƒ¨æ‰€æœ‰äº‹ä»¶
        for event_data in events_in_category:
            result = evaluate_single_event(
                event_data, U_optimized_cat, category_name
            )
            final_report_data.append(result)

    # 6. ä¿å­˜å’Œæ˜¾ç¤ºç»“æœ
    if final_report_data:
        # ç”Ÿæˆæ¾è¾½æ²³æ•°æ®è¾“å‡ºæ–‡ä»¶å
        station_suffix = f"_{args.station_id}" if args.station_id else ""
        data_source_suffix = f"songliao{station_suffix}"
        output_filename = os.path.join(
            args.output_dir,
            f"UH_categorized_eva_output_{data_source_suffix}.csv",
        )

        report_df_sorted = save_results_to_csv(
            final_report_data,
            output_filename,
            sort_columns=["æ‰€å±ç±»åˆ«", "NSE"],
        )

        if report_df_sorted is not None and verbose:
            # æ‰“å°æŠ¥å‘Šé¢„è§ˆ
            print_report_preview(
                report_df_sorted, "åˆ†ç±»è¯„ä¼°æŠ¥å‘Šé¢„è§ˆ (æŒ‰ç±»åˆ«å’ŒNSEæ’åº)"
            )

            # æ‰“å°å„ç±»åˆ«ç»Ÿè®¡ä¿¡æ¯
            print_category_statistics(report_df_sorted)

            # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
            best_nse_by_category = report_df_sorted.groupby("æ‰€å±ç±»åˆ«")[
                "NSE"
            ].max()
            print("\nğŸ¯ ä¼˜åŒ–å®Œæˆï¼")
            print(f"   æ•°æ®æº: æ¾è¾½æ²³æµåŸŸæ•°æ®")
            if args.station_id:
                print(f"   å¤„ç†ç«™ç‚¹: {args.station_id}")
            print(f"   æƒé‡æ–¹æ¡ˆ: {args.category_weights}")
            print("   å„ç±»åˆ«æœ€ä¼˜NSE:")
            for category, nse in best_nse_by_category.items():
                print(f"     {category}: {nse:.4f}")
            print(f"   ç»“æœä¿å­˜è‡³: {output_filename}")
    else:
        print("\nâŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•è¯„ä¼°ç»“æœã€‚")

    if not verbose:
        # é™é»˜æ¨¡å¼ä¸‹ä¹Ÿè¾“å‡ºå…³é”®ä¿¡æ¯
        best_nse = (
            report_df_sorted["NSE"].max()
            if report_df_sorted is not None
            else 0
        )
        print(f"ä¼˜åŒ–å®Œæˆ - æœ€ä¼˜NSE: {best_nse:.4f}, è¾“å‡º: {output_filename}")


if __name__ == "__main__":
    main()
