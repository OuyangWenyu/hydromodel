{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-hydromodel","title":"Welcome to hydromodel","text":"<p>A lightweight Python package for hydrological model calibration and evaluation, featuring the XinAnJiang (XAJ) model.</p> <p><code>hydromodel</code> is a Python implementation of conceptual hydrological models, with a focus on the XinAnJiang (XAJ) model - one of the most widely-used rainfall-runoff models, especially in China and Asian regions. The package provides comprehensive tools for model calibration, evaluation, and simulation with a unified API design.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#hydrological-models","title":"\ud83c\udfde\ufe0f Hydrological Models","text":"<ul> <li>XAJ Model Variants: Standard XAJ and optimized versions (xaj_mz with Muskingum routing, xaj_slw)</li> <li>GR Models: GR1A, GR2M, GR3J, GR4J, GR5J, GR6J</li> <li>Other Models: HYMOD, DHF (Dongjiang Hydrology Formula)</li> <li>Extensible Framework: Easy to add custom models</li> </ul>"},{"location":"#calibration-algorithms","title":"\ud83d\udd27 Calibration Algorithms","text":"<ul> <li>SCE-UA: Shuffled Complex Evolution (robust, recommended for global optimization)</li> <li>GA: Genetic Algorithm with DEAP (flexible, handles complex parameter landscapes)</li> <li>scipy: L-BFGS-B, SLSQP, and other gradient-based methods (fast for smooth objectives)</li> </ul>"},{"location":"#evaluation-analysis","title":"\ud83d\udcca Evaluation &amp; Analysis","text":"<ul> <li>Comprehensive Metrics: NSE, KGE, RMSE, PBIAS, FHV, FLV, FMS</li> <li>Multi-Basin Support: Efficient calibration and evaluation for multiple basins simultaneously</li> <li>Time Series Analysis: Flood event extraction and characterization</li> <li>Visualization: Automated plotting of simulation results and metrics</li> </ul>"},{"location":"#data-integration","title":"\ud83d\uddc4\ufe0f Data Integration","text":"<ul> <li>CAMELS Datasets: Seamless support for 11 CAMELS variants via hydrodataset</li> <li>Custom Data: Flexible support for user data via hydrodatasource</li> <li>Flood Event Data: Specialized support for discrete flood event datasets</li> <li>Standardized Format: Unified data interface across all data sources</li> </ul>"},{"location":"#developer-friendly","title":"\ud83d\ude80 Developer-Friendly","text":"<ul> <li>Unified API: Consistent interfaces for calibration, evaluation, and simulation</li> <li>Configuration-Based: YAML configuration for reproducibility</li> <li>Progress Tracking: Real-time progress display and intermediate results saving</li> <li>Standardized Results: All algorithms save results in unified JSON + CSV format</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from hydromodel.trainers.unified_calibrate import calibrate\nfrom hydromodel.trainers.unified_evaluate import evaluate\n\n# Configuration for calibration\nconfig = {\n    \"data_cfgs\": {\n        \"data_source_type\": \"camels_us\",\n        \"basin_ids\": [\"01013500\"],\n        \"train_period\": [\"1985-10-01\", \"1995-09-30\"],\n        \"test_period\": [\"2005-10-01\", \"2014-09-30\"],\n        \"warmup_length\": 365,\n        \"variables\": [\"precipitation\", \"potential_evapotranspiration\", \"streamflow\"]\n    },\n    \"model_cfgs\": {\n        \"model_name\": \"xaj_mz\",\n    },\n    \"training_cfgs\": {\n        \"algorithm_name\": \"SCE_UA\",\n        \"algorithm_params\": {\"rep\": 5000, \"ngs\": 1000},\n        \"loss_config\": {\"type\": \"time_series\", \"obj_func\": \"RMSE\"},\n        \"output_dir\": \"results\",\n        \"experiment_name\": \"my_experiment\",\n    },\n    \"evaluation_cfgs\": {\n        \"metrics\": [\"NSE\", \"KGE\", \"RMSE\"],\n    },\n}\n\n# Run calibration\nresults = calibrate(config)\n\n# Evaluate on test period\nevaluate(config, param_dir=\"results/my_experiment\", eval_period=\"test\")\n</code></pre> <p>Or use command-line scripts:</p> <pre><code># 1. Calibration\npython scripts/run_xaj_calibration.py --config configs/example_config.yaml\n\n# 2. Evaluation\npython scripts/run_xaj_evaluate.py --calibration-dir results/xaj_mz_SCE_UA --eval-period test\n\n# 3. Simulation (no calibration required!)\npython scripts/run_xaj_simulate.py \\\n    --config configs/example_simulate_config.yaml \\\n    --param-file configs/example_xaj_params.yaml \\\n    --plot\n\n# 4. Visualization\npython scripts/visualize.py --eval-dir results/xaj_mz_SCE_UA/evaluation_test\n</code></pre>"},{"location":"#installation","title":"Installation","text":""},{"location":"#quick-installation","title":"Quick Installation","text":"<pre><code>pip install hydromodel hydrodataset\n</code></pre> <p>Or using <code>uv</code> (faster):</p> <pre><code>uv pip install hydromodel hydrodataset\n</code></pre>"},{"location":"#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/OuyangWenyu/hydromodel.git\ncd hydromodel\nuv sync --all-extras\n</code></pre> <p>For detailed installation instructions, see the Installation Guide.</p>"},{"location":"#why-hydromodel","title":"Why hydromodel?","text":"<p>For Researchers: - Battle-tested XAJ implementations used in published research - Configuration-based workflow ensures reproducibility - Easy to extend with new models or calibration algorithms - Lightweight and fast - perfect for parameter sensitivity studies</p> <p>For Practitioners: - Simple YAML configuration, minimal coding required - Handles multi-basin calibration efficiently - Integration with global CAMELS datasets (11 variants) - Clear documentation and examples</p> <p>Compared to other packages: - vs. SWAT/VIC: Lighter weight, Python-native, faster iteration - vs. pySTREPS: Focus on conceptual rainfall-runoff models - vs. custom scripts: Well-tested with unified interfaces</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Installation Guide - Detailed installation instructions for all platforms</li> <li>Quick Start - Get started in 5 minutes</li> <li>Usage Guide - Comprehensive tutorials and examples</li> <li>Data Guide - How to prepare and use different data sources</li> <li>API Reference - Complete API documentation</li> <li>Model Documentation - Detailed model descriptions</li> <li>Contributing - How to contribute to the project</li> <li>FAQ - Frequently asked questions</li> <li>Changelog - Version history and updates</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#1-model-calibration","title":"1. Model Calibration","text":"<p>Calibrate hydrological models on CAMELS datasets or custom data with various algorithms:</p> <pre><code># Use SCE-UA for global optimization\nconfig[\"training_cfgs\"][\"algorithm_name\"] = \"SCE_UA\"\nresults = calibrate(config)\n\n# Or use GA for flexible optimization\nconfig[\"training_cfgs\"][\"algorithm_name\"] = \"GA\"\nresults = calibrate(config)\n</code></pre>"},{"location":"#2-multi-basin-evaluation","title":"2. Multi-Basin Evaluation","text":"<p>Efficiently calibrate and evaluate multiple basins:</p> <pre><code>config[\"data_cfgs\"][\"basin_ids\"] = [\"01013500\", \"01022500\", \"01030500\"]\nresults = calibrate(config)\nevaluate(config, param_dir=\"results/my_experiment\", eval_period=\"test\")\n</code></pre>"},{"location":"#3-parameter-sensitivity-analysis","title":"3. Parameter Sensitivity Analysis","text":"<p>Run simulations with custom parameter sets:</p> <pre><code>from hydromodel.trainers.unified_simulate import UnifiedSimulator\n\n# Test different parameter values\nfor k_value in [0.5, 0.75, 1.0]:\n    parameters = {..., \"K\": k_value, ...}\n    simulator = UnifiedSimulator(model_config, basin_config)\n    results = simulator.simulate(inputs, qobs, warmup_length=365)\n    # Analyze results\n</code></pre>"},{"location":"#4-flood-event-analysis","title":"4. Flood Event Analysis","text":"<p>Extract and calibrate on flood events:</p> <pre><code>config = {\n    \"data_cfgs\": {\n        \"data_source_type\": \"floodevent\",\n        \"time_unit\": [\"3h\"],\n        \"variables\": [\"prcp\", \"PET\", \"streamflow\"],\n        ...\n    },\n    ...\n}\nresults = calibrate(config)\n</code></pre>"},{"location":"#supported-models","title":"Supported Models","text":"Model Description Parameters Routing xaj Standard XinAnJiang model 15 Linear reservoir xaj_mz XAJ with Muskingum routing 15 Muskingum xaj_slw XAJ with SLW routing 26 Storage-lag-weighted gr4j GR4J rainfall-runoff model 4 Unit hydrograph hymod HYMOD model 5 Nash cascade dhf Dongjiang Hydrology Formula 18 Custom <p>For detailed model documentation, see Model Reference.</p>"},{"location":"#calibration-algorithms_1","title":"Calibration Algorithms","text":"Algorithm Type Strengths Best For SCE-UA Global Robust, reliable convergence General purpose, recommended GA Global Flexible, handles discontinuities Complex parameter landscapes scipy Local Fast, gradient-based Smooth objectives, refinement"},{"location":"#data-sources","title":"Data Sources","text":""},{"location":"#camels-datasets","title":"CAMELS Datasets","text":"<p>Support for 11 CAMELS variants worldwide:</p> <ul> <li>CAMELS-US (671 basins)</li> <li>CAMELS-AUS (222 basins)</li> <li>CAMELS-BR (897 basins)</li> <li>CAMELS-CL (516 basins)</li> <li>CAMELS-GB (671 basins)</li> <li>And more...</li> </ul>"},{"location":"#custom-data","title":"Custom Data","text":"<p>Use your own data with <code>selfmadehydrodataset</code> format:</p> <pre><code>my_basin_data/\n\u251c\u2500\u2500 attributes/\n\u2502   \u2514\u2500\u2500 attributes.csv\n\u251c\u2500\u2500 shapes/\n\u2502   \u2514\u2500\u2500 basins.shp\n\u251c\u2500\u2500 timeseries/\n\u2502   \u251c\u2500\u2500 1D/\n\u2502   \u2502   \u251c\u2500\u2500 basin_001.csv\n\u2502   \u2502   \u2514\u2500\u2500 basin_002.csv\n\u2502   \u2514\u2500\u2500 1D_units_info.json\n</code></pre> <p>See Data Guide for complete specifications.</p>"},{"location":"#performance","title":"Performance","text":"<ul> <li>Fast calibration: Optimized algorithms with numba JIT compilation</li> <li>Memory efficient: Handles large datasets with chunked processing</li> <li>Parallel support: Multi-basin calibration runs independently</li> <li>Progress tracking: Real-time monitoring of long-running calibrations</li> </ul>"},{"location":"#references","title":"References","text":"<p>XAJ Model: - Zhao, R.J., 1992. The Xinanjiang model applied in China. Journal of Hydrology, 135(1-4), pp.371-381.</p> <p>Calibration Algorithms: - Duan, Q., et al., 1992. Effective and efficient global optimization for conceptual rainfall-runoff models. Water Resources Research, 28(4), pp.1015-1031. (SCE-UA)</p> <p>Related Projects: - hydrodataset - CAMELS and other datasets - hydrodatasource - Data preparation utilities - torchhydro - PyTorch-based hydrological models</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use hydromodel in your research, please cite:</p> <pre><code>@software{hydromodel,\n  author = {Ouyang, Wenyu},\n  title = {hydromodel: A Python Package for Hydrological Model Calibration},\n  year = {2025},\n  url = {https://github.com/OuyangWenyu/hydromodel}\n}\n</code></pre>"},{"location":"#license-credits","title":"License &amp; Credits","text":"<ul> <li>License: GNU General Public License v3.0</li> <li>Author: Wenyu Ouyang</li> <li>Documentation: https://OuyangWenyu.github.io/hydromodel</li> <li>Source Code: https://github.com/OuyangWenyu/hydromodel</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Browse the complete documentation</li> <li>Issues: Report bugs or request features at GitHub Issues</li> <li>Discussions: Ask questions at GitHub Discussions</li> <li>Email: wenyuouyang@outlook.com</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! See the Contributing Guide for details on:</p> <ul> <li>Reporting bugs</li> <li>Suggesting features</li> <li>Submitting pull requests</li> <li>Code style and testing guidelines</li> </ul>"},{"location":"#community","title":"Community","text":"<p>Join our growing community:</p> <ul> <li>\u2b50 Star the project on GitHub</li> <li>\ud83d\udc1b Report issues and bugs</li> <li>\ud83d\udca1 Suggest new features</li> <li>\ud83d\udcd6 Improve documentation</li> <li>\ud83d\udd27 Contribute code</li> </ul> <p>Ready to get started? Head to the Quick Start Guide or Installation Guide!</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v030-2025-11-07","title":"v0.3.0 - 2025-11-07","text":"<p>Critical Bug Fixes for Multi-Basin Flood Event Data:</p> <ul> <li>Fixed Time Array Misalignment in Evaluation:</li> <li>Fixed critical issue where multi-basin flood events used incorrect time arrays in NetCDF output</li> <li>Previous behavior: Used longest basin's time array for all basins, causing time misalignment for shorter basins</li> <li>Example issue: Basin 21100150's Event 26 showed \"2020-08-31 to 2022-08-02\" (spanning 2 years) when actual data ended at 2020-09-05</li> <li>New behavior: Creates unified time array from all basins' unique timestamps, then remaps each basin's data to correct time positions</li> <li>Modified <code>_save_all_results()</code> in <code>unified_evaluate.py</code> (lines 403-470)</li> <li> <p>Each basin now has accurate event timestamps in evaluation results</p> </li> <li> <p>Fixed Data Access Issues in Calibration:</p> </li> <li>Fixed <code>AttributeError</code> when calibrating multi-basin flood event data</li> <li>Previous behavior: <code>UnifiedCalibrator</code> tried to access <code>p_and_e.shape[1]</code> when <code>p_and_e=None</code> for separate basin data</li> <li>Added proper detection of <code>basin_data_separate</code> mode in <code>UnifiedCalibrator.__init__()</code></li> <li>Modified <code>set_basin_index()</code> to extract basin-specific data from <code>basin_data_separate</code> when available</li> <li>Each basin now uses its own time series without padding during calibration</li> <li> <p>Modified <code>unified_calibrate.py</code> (lines 116-253, 377-386)</p> </li> <li> <p>Code Quality Improvements:</p> </li> <li>Removed debug print statements from data loading pipeline</li> <li>Cleaned up verbose event statistics output in <code>unified_data_loader.py</code></li> <li>Removed redundant basin information prints in <code>floodevent.py</code></li> <li> <p>Fixed <code>SyntaxWarning</code> in <code>run_xaj_calibration.py</code> by using raw string for file path</p> </li> <li> <p>Visualization Enhancements:</p> </li> <li>Fixed metrics text overlap with legend in flood event plots</li> <li>Moved metrics display position from (0.98, 0.97) to (0.98, 0.70) to avoid overlap</li> <li>Metrics (NSE/RMSE/PBIAS) now display cleanly below legend on right side</li> <li>Modified <code>data_visualize.py</code> (line 255)</li> </ul> <p>Impact:</p> <ul> <li>Evaluation: Multi-basin flood event evaluations now produce correct NetCDF files with accurate timestamps for each basin</li> <li>Calibration: Multi-basin flood event calibrations now work correctly without data access errors</li> <li>Visualization: Event plots are cleaner with better layout and no overlapping text</li> </ul> <p>Files Changed:</p> <ul> <li><code>hydromodel/trainers/unified_evaluate.py</code>: Time array unification and remapping logic</li> <li><code>hydromodel/trainers/unified_calibrate.py</code>: Basin data separation support and basin_ids detection</li> <li><code>hydromodel/datasets/unified_data_loader.py</code>: Removed debug print statements</li> <li><code>hydromodel/datasets/data_visualize.py</code>: Adjusted metrics text position</li> <li><code>scripts/run_xaj_calibration.py</code>: Fixed path string syntax warning</li> </ul> <p>Testing:</p> <ul> <li>Verified multi-basin flood event evaluation produces correct timestamps</li> <li>Verified multi-basin flood event calibration accesses correct basin data</li> <li>Confirmed clean console output without excessive debug information</li> </ul>"},{"location":"changelog/#v030-2025-11-06","title":"v0.3.0 - 2025-11-06","text":"<p>Flood Event Data Support:</p> <ul> <li>Event ID System:</li> <li>Added <code>event_id</code> tracking to group multi-peak flood events together</li> <li>Modified <code>floodevent.py</code> to extract and pass <code>event_id</code> and <code>event_name</code> from original data</li> <li>Modified <code>_convert_events_to_arrays()</code> to include <code>event_id</code> as 4th feature in p_and_e array</li> <li>p_and_e shape changed from <code>[time, features=3]</code> to <code>[time, features=4]</code> where features = <code>[prcp, pet, marker, event_id]</code></li> <li> <p>Prevents original flood events with multiple peaks from being split into separate visualizations</p> </li> <li> <p>Simulation Code Fixes:</p> </li> <li>Fixed <code>_simulate_event_data()</code> in <code>unified_simulate.py</code> to handle 4-feature input</li> <li>Changed flood_event_marker extraction from index <code>-1</code> to index <code>2</code> (to account for event_id as 4th feature)</li> <li>Only passes first 3 features <code>[:3]</code> to model function, excluding event_id</li> <li> <p>Updated validation error message to reflect 3+ features format</p> </li> <li> <p>Visualization Improvements:</p> </li> <li>Modified <code>_identify_flood_events()</code> to support grouping by <code>event_id</code> when available</li> <li>Only plots flood period data (<code>marker==1</code>), excluding warmup (<code>marker==NaN</code>) and gaps (<code>marker==0</code>)</li> <li>Filters time and data arrays to only include flood period timesteps</li> <li>Fixes issue where plots would span entire year or show overlapping x-axis ticks</li> <li> <p>Event count now correctly reflects original flood events (e.g., 26 events instead of 39 peaks)</p> </li> <li> <p>Time Handling Fixes:</p> </li> <li>Fixed time array padding for multi-basin flood event data</li> <li>Extends time arrays with regular intervals using <code>pd.date_range()</code> when padding data</li> <li>Fixed pandas FutureWarning by replacing <code>'H'</code> with <code>'h'</code> and <code>'D'</code> with <code>'d'</code> in frequency strings</li> <li> <p>Extracts real event timestamps from original flood event data instead of generating dummy times</p> </li> <li> <p>Evaluation Results:</p> </li> <li>Modified <code>_save_evaluation_results()</code> to save <code>event_id</code> as additional variable in NetCDF output</li> <li>Enables visualization to group multi-peak events by original event_id</li> </ul> <p>Files Changed:</p> <ul> <li><code>hydrodatasource/reader/floodevent.py</code>: Event ID and time extraction</li> <li><code>hydromodel/datasets/unified_data_loader.py</code>: 4-feature p_and_e array, time padding</li> <li><code>hydromodel/trainers/unified_simulate.py</code>: 4-feature input handling</li> <li><code>hydromodel/trainers/unified_evaluate.py</code>: Event ID saving to NetCDF</li> <li><code>hydromodel/datasets/data_visualize.py</code>: Event grouping and flood period filtering</li> </ul> <p>Known Issues:</p> <ul> <li>Visualization still shows some x-axis overlap issues for certain events (pending further investigation)</li> </ul>"},{"location":"changelog/#v030-2025-11-05","title":"v0.3.0 - 2025-11-05","text":"<p>Multi-Basin Support:</p> <ul> <li>Fixed Multi-Basin Unit Conversion:</li> <li>Fixed broadcasting error in <code>streamflow_unit_conv</code> for multi-basin data</li> <li>Modified <code>UnifiedDataLoader._check_and_convert_units()</code> to process basins individually</li> <li>Modified <code>_save_evaluation_results()</code> to convert units basin-by-basin</li> <li>Now properly handles dimension order (<code>[\"basin\", \"time\"]</code> or <code>[\"time\", \"basin\"]</code>)</li> <li>Successfully tested calibration and evaluation on CAMELS-US multi-basin datasets</li> </ul> <p>Calibration Improvements:</p> <ul> <li>Parameter Display Enhancement:</li> <li>All three algorithms (SCE-UA, GA, scipy) now display denormalized parameters (actual physical ranges)</li> <li>Changed console output from normalized values (0-1) to actual parameter ranges</li> <li>Uses the same denormalization formula as <code>process_parameters()</code> in <code>param_utils.py</code></li> <li>Format: \"Best parameters (actual ranges)\" with physical values</li> </ul> <p>Files Changed:</p> <ul> <li><code>hydromodel/trainers/unified_calibrate.py</code>: Parameter display for all three algorithms</li> <li><code>hydromodel/datasets/unified_data_loader.py</code>: Multi-basin unit conversion</li> <li><code>hydromodel/trainers/unified_evaluate.py</code>: Multi-basin evaluation results saving</li> </ul> <p>Testing:</p> <ul> <li>Verified multi-basin calibration on CAMELS-US dataset</li> <li>Verified multi-basin evaluation on CAMELS-US dataset</li> <li>All three calibration algorithms tested with multi-basin data</li> </ul>"},{"location":"changelog/#v030-2025-11-04","title":"v0.3.0 - 2025-11-04","text":"<p>Major Improvements:</p> <ul> <li>Configurable File Saving:</li> <li>Added <code>save_config</code> option in <code>training_cfgs</code> (default: <code>True</code>)</li> <li>Controls saving of <code>calibration_config.yaml</code> and <code>param_range.yaml</code></li> <li><code>param_range.yaml</code> now only contains the current model's parameters (not all models)</li> <li><code>calibration_config.yaml</code> records actual <code>param_range_file</code> path instead of <code>null</code></li> <li>Command-line flag <code>--no-save-config</code> to disable saving</li> </ul> <p>Bug Fixes:</p> <ul> <li>Fixed <code>param_range.yaml</code> containing all models instead of only the current model</li> <li>Fixed <code>calibration_config.yaml</code> not recording the actual <code>param_range_file</code> path</li> </ul> <p>Documentation:</p> <ul> <li>Updated README.md and README_zh.md with <code>save_config</code> usage</li> <li>Documented <code>param_range.yaml</code> behavior (only saves current model)</li> <li>Updated example_config.yaml with save_config option</li> </ul> <p>Testing:</p> <ul> <li>Verified all three calibration algorithms (SCE-UA, GA, scipy) work correctly</li> <li>Tested configuration file saving functionality</li> </ul>"},{"location":"changelog/#v030-2025-11-04_1","title":"v0.3.0 - 2025-11-04","text":"<p>Major Improvements:</p> <ul> <li>Unified Calibration Interface: All three algorithms (SCE-UA, GA, scipy) now use a consistent API</li> <li>Standardized Results Format:</li> <li>All algorithms save best parameters to <code>calibration_results.json</code> (unified format)</li> <li>All algorithms save detailed iteration history to CSV with parameter values</li> <li>CSV format unified across GA and scipy (<code>objective_value</code> + <code>param_{name}</code> columns)</li> <li>Enhanced Progress Tracking:</li> <li>Real-time progress display for all algorithms</li> <li>Progress bars for GA using tqdm</li> <li>Iteration-by-iteration output for scipy</li> <li>Generation-by-generation statistics for GA</li> <li>Improved Parameter Loading:</li> <li>Automatic fallback: JSON \u2192 GA CSV \u2192 scipy CSV \u2192 SCE-UA CSV \u2192 legacy TXT</li> <li>Clear error messages showing all attempted file paths</li> <li>Support for all algorithm result formats in evaluation</li> </ul> <p>New Features:</p> <ul> <li>Genetic Algorithm (GA) Enhancements:</li> <li>Saves detailed generation history with all parameter values</li> <li>Custom bounded mutation operator</li> <li>Generation statistics (min, mean, max, best fitness)</li> <li>Compatible CSV format with scipy results</li> <li>scipy Optimizer Enhancements:</li> <li>Iteration history tracking with parameter values</li> <li>Progress display every N iterations</li> <li>Detailed convergence information</li> <li>Support for multiple scipy methods (SLSQP, L-BFGS-B, TNC, etc.)</li> <li>Unified Configuration:</li> <li>All algorithm parameters configurable via YAML</li> <li>Example config includes all three algorithms with defaults</li> <li>Algorithm aliases: <code>SCE_UA</code>/<code>sceua</code>, <code>GA</code>/<code>genetic_algorithm</code>, <code>scipy</code>/<code>scipy_minimize</code></li> </ul> <p>Bug Fixes:</p> <ul> <li>Fixed GA evaluation function returning tuple of list instead of tuple of float</li> <li>Fixed scipy/GA algorithm name matching using <code>in</code> operator instead of <code>or</code></li> <li>Fixed parameter loading failure when <code>calibration_results.json</code> not found</li> <li>Added error handling for all parameter loading methods</li> </ul> <p>Documentation:</p> <ul> <li>Updated README.md with complete algorithm parameter documentation</li> <li>Updated README_zh.md with Chinese documentation</li> <li>Updated quickstart.md with algorithm comparison guide</li> <li>Added convergence analysis examples</li> <li>Added results format explanation</li> <li>Updated configuration examples with all algorithm parameters</li> </ul>"},{"location":"changelog/#v0211-2025-11-02","title":"v0.2.11 - 2025-11-02","text":"<p>Improvement:</p> <ul> <li>Refactored visualization and cleanup codebase</li> <li>Updated GitHub workflows</li> </ul> <p>New Features:</p> <ul> <li>Added unified simulate interface</li> <li>Added parameter file support for simulation</li> </ul>"},{"location":"changelog/#v001-2024-xx-xx","title":"v0.0.1 - 2024-XX-XX","text":"<p>Initial Release:</p> <ul> <li>Basic XAJ model implementation</li> <li>SCE-UA calibration support</li> <li>CAMELS dataset integration</li> </ul>"},{"location":"common/","title":"Common Utilities and Configurations","text":"<p>This section covers common utilities, configurations, and shared components used across all hydrological models in hydromodel.</p>"},{"location":"common/#model-configuration","title":"Model Configuration","text":"<p>Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"common/#parameter-utilities","title":"Parameter Utilities","text":"<p>Copyright (c) 2025 Wenyu Ouyang. All rights reserved.</p>"},{"location":"common/#hydromodel.models.param_utils.detect_parameter_format","title":"<code>detect_parameter_format(parameters, param_ranges)</code>","text":"<p>Detect whether parameters are normalized (0-1 range) or in original scale.</p>"},{"location":"common/#hydromodel.models.param_utils.detect_parameter_format--parameters","title":"Parameters","text":"<p>parameters : np.ndarray     Model parameters array [basin, parameter] param_ranges : Dict[str, List[float]]     Parameter ranges dictionary with min/max values for each parameter</p>"},{"location":"common/#hydromodel.models.param_utils.detect_parameter_format--returns","title":"Returns","text":"<p>bool     True if parameters appear to be normalized (0-1 range), False otherwise</p> Source code in <code>hydromodel/models/param_utils.py</code> <pre><code>def detect_parameter_format(\n    parameters: np.ndarray, param_ranges: Dict[str, List[float]]\n) -&gt; bool:\n    \"\"\"\n    Detect whether parameters are normalized (0-1 range) or in original scale.\n\n    Parameters\n    ----------\n    parameters : np.ndarray\n        Model parameters array [basin, parameter]\n    param_ranges : Dict[str, List[float]]\n        Parameter ranges dictionary with min/max values for each parameter\n\n    Returns\n    -------\n    bool\n        True if parameters appear to be normalized (0-1 range), False otherwise\n    \"\"\"\n    # Check if all parameters are within [0, 1] range\n    # Allow small tolerance for numerical precision\n    tolerance = 1e-6\n\n    # If any parameter is outside [0, 1] with tolerance, assume original scale\n    if np.any(parameters &lt; -tolerance) or np.any(parameters &gt; 1 + tolerance):\n        return False\n\n    # Additional check: if parameters are suspiciously close to range boundaries\n    # when interpreted as original values, they're likely normalized\n    param_values = list(param_ranges.values())\n    for i, param_range in enumerate(param_values):\n        if i &gt;= parameters.shape[1]:\n            break\n        param_col = parameters[:, i]\n        min_val, max_val = param_range[0], param_range[1]\n\n        # If parameters are all very close to 0-1 range but the actual range\n        # is much larger, they're likely normalized\n        if max_val - min_val &gt; 2 and np.all(param_col &lt;= 1.1):\n            return True\n\n    return True  # Default assumption: parameters are normalized\n</code></pre>"},{"location":"common/#hydromodel.models.param_utils.get_parameter_scales","title":"<code>get_parameter_scales(param_ranges)</code>","text":"<p>Extract parameter scales from param_ranges for backward compatibility.</p>"},{"location":"common/#hydromodel.models.param_utils.get_parameter_scales--parameters","title":"Parameters","text":"<p>param_ranges : Dict[str, List[float]]     Parameter ranges dictionary</p>"},{"location":"common/#hydromodel.models.param_utils.get_parameter_scales--returns","title":"Returns","text":"<p>Dict[str, List[float]]     Dictionary mapping parameter names to [min, max] ranges</p> Source code in <code>hydromodel/models/param_utils.py</code> <pre><code>def get_parameter_scales(\n    param_ranges: Dict[str, List[float]],\n) -&gt; Dict[str, List[float]]:\n    \"\"\"\n    Extract parameter scales from param_ranges for backward compatibility.\n\n    Parameters\n    ----------\n    param_ranges : Dict[str, List[float]]\n        Parameter ranges dictionary\n\n    Returns\n    -------\n    Dict[str, List[float]]\n        Dictionary mapping parameter names to [min, max] ranges\n    \"\"\"\n    return param_ranges.copy()\n</code></pre>"},{"location":"common/#hydromodel.models.param_utils.normalize_parameters","title":"<code>normalize_parameters(parameters, param_ranges)</code>","text":"<p>Convert parameters from original scale to normalized (0-1) scale.</p>"},{"location":"common/#hydromodel.models.param_utils.normalize_parameters--parameters","title":"Parameters","text":"<p>parameters : np.ndarray     Model parameters in original scale [basin, parameter] param_ranges : Dict[str, List[float]]     Parameter ranges dictionary</p>"},{"location":"common/#hydromodel.models.param_utils.normalize_parameters--returns","title":"Returns","text":"<p>np.ndarray     Parameters normalized to 0-1 scale</p> Source code in <code>hydromodel/models/param_utils.py</code> <pre><code>def normalize_parameters(\n    parameters: np.ndarray, param_ranges: Dict[str, List[float]]\n) -&gt; np.ndarray:\n    \"\"\"\n    Convert parameters from original scale to normalized (0-1) scale.\n\n    Parameters\n    ----------\n    parameters : np.ndarray\n        Model parameters in original scale [basin, parameter]\n    param_ranges : Dict[str, List[float]]\n        Parameter ranges dictionary\n\n    Returns\n    -------\n    np.ndarray\n        Parameters normalized to 0-1 scale\n    \"\"\"\n    normalized_params = np.zeros_like(parameters)\n    param_list = list(param_ranges.values())\n\n    for i, param_range in enumerate(param_list):\n        min_val, max_val = param_range[0], param_range[1]\n        normalized_params[:, i] = (parameters[:, i] - min_val) / (\n            max_val - min_val\n        )\n\n    return normalized_params\n</code></pre>"},{"location":"common/#hydromodel.models.param_utils.process_parameters","title":"<code>process_parameters(parameters, param_ranges, normalized='auto')</code>","text":"<p>Process model parameters to convert from normalized to original scale if needed.</p> <p>This function provides a unified interface for parameter handling across all models, supporting both normalized (0-1 range) and original scale parameters.</p>"},{"location":"common/#hydromodel.models.param_utils.process_parameters--parameters","title":"Parameters","text":"<p>parameters : np.ndarray     Model parameters array [basin, parameter] param_ranges : Dict[str, List[float]]     Parameter ranges dictionary with min/max values for each parameter normalized : Union[bool, str], optional     Parameter format specification:     - \"auto\": Automatically detect parameter format (default)     - True: Parameters are normalized (0-1 range), convert to original scale     - False: Parameters are already in original scale, use as-is</p>"},{"location":"common/#hydromodel.models.param_utils.process_parameters--returns","title":"Returns","text":"<p>np.ndarray     Parameters in original scale, ready for model computation</p>"},{"location":"common/#hydromodel.models.param_utils.process_parameters--examples","title":"Examples","text":"<p>param_ranges = {\"K\": [0.1, 1.0], \"B\": [0.1, 0.4]}</p> Source code in <code>hydromodel/models/param_utils.py</code> <pre><code>def process_parameters(\n    parameters: np.ndarray,\n    param_ranges: Dict[str, List[float]],\n    normalized: Union[bool, str] = \"auto\",\n) -&gt; np.ndarray:\n    \"\"\"\n    Process model parameters to convert from normalized to original scale if needed.\n\n    This function provides a unified interface for parameter handling across all models,\n    supporting both normalized (0-1 range) and original scale parameters.\n\n    Parameters\n    ----------\n    parameters : np.ndarray\n        Model parameters array [basin, parameter]\n    param_ranges : Dict[str, List[float]]\n        Parameter ranges dictionary with min/max values for each parameter\n    normalized : Union[bool, str], optional\n        Parameter format specification:\n        - \"auto\": Automatically detect parameter format (default)\n        - True: Parameters are normalized (0-1 range), convert to original scale\n        - False: Parameters are already in original scale, use as-is\n\n    Returns\n    -------\n    np.ndarray\n        Parameters in original scale, ready for model computation\n\n    Examples\n    --------\n    &gt;&gt;&gt; param_ranges = {\"K\": [0.1, 1.0], \"B\": [0.1, 0.4]}\n    &gt;&gt;&gt; # Normalized parameters\n    &gt;&gt;&gt; norm_params = np.array([[0.5, 0.8]])  # Will be converted to [0.55, 0.34]\n    &gt;&gt;&gt; orig_params = process_parameters(norm_params, param_ranges, normalized=True)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Original scale parameters\n    &gt;&gt;&gt; orig_params = np.array([[0.55, 0.34]])  # Will be used as-is\n    &gt;&gt;&gt; final_params = process_parameters(orig_params, param_ranges, normalized=False)\n    \"\"\"\n    if parameters.shape[1] != len(param_ranges):\n        raise ValueError(\n            f\"Parameter array has {parameters.shape[1]} columns but \"\n            f\"param_ranges has {len(param_ranges)} parameters\"\n        )\n\n    # Auto-detect parameter format if requested\n    if normalized == \"auto\":\n        normalized = detect_parameter_format(parameters, param_ranges)\n\n    # If parameters are already in original scale, return as-is\n    if not normalized:\n        return parameters.copy()\n\n    # Convert normalized parameters to original scale\n    converted_params = np.zeros_like(parameters)\n    param_list = list(param_ranges.values())\n\n    for i, param_range in enumerate(param_list):\n        min_val, max_val = param_range[0], param_range[1]\n        converted_params[:, i] = min_val + parameters[:, i] * (\n            max_val - min_val\n        )\n\n    return converted_params\n</code></pre>"},{"location":"common/#hydromodel.models.param_utils.process_parameters--normalized-parameters","title":"Normalized parameters","text":"<p>norm_params = np.array([[0.5, 0.8]])  # Will be converted to [0.55, 0.34] orig_params = process_parameters(norm_params, param_ranges, normalized=True)</p>"},{"location":"common/#hydromodel.models.param_utils.process_parameters--original-scale-parameters","title":"Original scale parameters","text":"<p>orig_params = np.array([[0.55, 0.34]])  # Will be used as-is final_params = process_parameters(orig_params, param_ranges, normalized=False)</p>"},{"location":"common/#hydromodel.models.param_utils.validate_parameters","title":"<code>validate_parameters(parameters, param_ranges, normalized=False)</code>","text":"<p>Validate that parameters are within acceptable ranges.</p>"},{"location":"common/#hydromodel.models.param_utils.validate_parameters--parameters","title":"Parameters","text":"<p>parameters : np.ndarray     Model parameters array [basin, parameter] param_ranges : Dict[str, List[float]]     Parameter ranges dictionary normalized : bool, optional     Whether parameters are normalized (default: False)</p>"},{"location":"common/#hydromodel.models.param_utils.validate_parameters--returns","title":"Returns","text":"<p>bool     True if all parameters are within valid ranges</p> Source code in <code>hydromodel/models/param_utils.py</code> <pre><code>def validate_parameters(\n    parameters: np.ndarray,\n    param_ranges: Dict[str, List[float]],\n    normalized: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Validate that parameters are within acceptable ranges.\n\n    Parameters\n    ----------\n    parameters : np.ndarray\n        Model parameters array [basin, parameter]\n    param_ranges : Dict[str, List[float]]\n        Parameter ranges dictionary\n    normalized : bool, optional\n        Whether parameters are normalized (default: False)\n\n    Returns\n    -------\n    bool\n        True if all parameters are within valid ranges\n    \"\"\"\n    if normalized:\n        # For normalized parameters, check 0-1 range\n        return np.all(parameters &gt;= 0) and np.all(parameters &lt;= 1)\n    else:\n        # For original scale parameters, check against param_ranges\n        param_list = list(param_ranges.values())\n        for i, param_range in enumerate(param_list):\n            if i &gt;= parameters.shape[1]:\n                break\n            min_val, max_val = param_range[0], param_range[1]\n            param_col = parameters[:, i]\n            if np.any(param_col &lt; min_val) or np.any(param_col &gt; max_val):\n                return False\n        return True\n</code></pre>"},{"location":"common/#model-dictionary","title":"Model Dictionary","text":""},{"location":"common/#hydromodel.models.model_dict.rmse43darr","title":"<code>rmse43darr(obs, sim)</code>","text":"<p>RMSE for 3D array</p>"},{"location":"common/#hydromodel.models.model_dict.rmse43darr--parameters","title":"Parameters","text":"<p>obs : np.ndarray     observation data sim : np.ndarray     simulation data</p>"},{"location":"common/#hydromodel.models.model_dict.rmse43darr--returns","title":"Returns","text":"<p>type description</p>"},{"location":"common/#hydromodel.models.model_dict.rmse43darr--raises","title":"Raises","text":"<p>ValueError     description</p> Source code in <code>hydromodel/models/model_dict.py</code> <pre><code>def rmse43darr(obs, sim):\n    \"\"\"RMSE for 3D array\n\n    Parameters\n    ----------\n    obs : np.ndarray\n        observation data\n    sim : np.ndarray\n        simulation data\n\n    Returns\n    -------\n    _type_\n        _description_\n\n    Raises\n    ------\n    ValueError\n        _description_\n    \"\"\"\n    rmses = np.sqrt(np.nanmean((sim - obs) ** 2, axis=0))\n    rmse = rmses.mean(axis=0)\n    if np.isnan(rmse) or any(np.isnan(sim)):\n        raise ValueError(\n            \"RMSE is nan or there are nan values in the simulation data, \"\n            \"please check the input data.\"\n        )\n    # tolist is necessary for spotpy to get the value\n    # otherwise the print will incur to an issue\n    # https://github.com/thouska/spotpy/issues/319\n    return rmse.tolist()\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/OuyangWenyu/hydromodel/issues.</p> <p>When reporting a bug, please include:</p> <ul> <li>Operating system name and version</li> <li>Python version you're using</li> <li>hydromodel version (run `python -c \"import hydromodel; print(hydromodel.version)\")</li> <li>Detailed steps to reproduce the bug</li> <li>Error messages (full traceback if available)</li> <li>Expected behavior vs. actual behavior</li> <li>Code snippet to reproduce (if applicable)</li> </ul> <p>Example bug report: <pre><code>**Environment:**\n- OS: Windows 11\n- Python: 3.11.5\n- hydromodel: 0.2.11\n\n**Steps to reproduce:**\n1. Load CAMELS-US dataset\n2. Run calibration with SCE-UA\n3. Error occurs during parameter saving\n\n**Error message:**\n</code></pre> FileNotFoundError: calibration_results.json not found <pre><code>**Expected:** Results should be saved to output directory\n</code></pre></p>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Issues tagged with: - <code>bug</code> - Confirmed bugs - <code>help wanted</code> - Good for contributors - <code>good first issue</code> - Great for first-time contributors</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Issues tagged with: - <code>enhancement</code> - New features or improvements - <code>help wanted</code> - Open for contributions - <code>documentation</code> - Documentation improvements</p>"},{"location":"contributing/#add-new-models","title":"Add New Models","text":"<p>hydromodel welcomes new hydrological model implementations! When adding a model:</p> <ol> <li>Create a new file in <code>hydromodel/models/</code> (e.g., <code>your_model.py</code>)</li> <li>Inherit from base classes or follow existing model patterns</li> <li>Implement required methods:</li> <li><code>__init__</code>: Model initialization</li> <li><code>run</code>: Main simulation function</li> <li>Define <code>param_limits</code> (parameter ranges)</li> <li>Add model tests in <code>test/</code></li> <li>Document the model in <code>docs/models/your_model.md</code></li> <li>Update <code>MODEL_PARAM_DICT</code> in <code>model_config.py</code></li> </ol> <p>Example: <pre><code>class YourModel:\n    def __init__(self, params, **kwargs):\n        self.params = params\n\n    def run(self, inputs, **kwargs):\n        # Your model logic\n        return outputs\n</code></pre></p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>hydromodel can always use more documentation! You can contribute by:</p> <ul> <li>Improving existing documentation</li> <li>Writing tutorials and examples</li> <li>Creating Jupyter notebook demonstrations</li> <li>Translating documentation to other languages</li> <li>Writing blog posts or articles about hydromodel</li> </ul> <p>Documentation is built using MkDocs with the Material theme.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>To propose a feature:</p> <ol> <li>Open an issue at https://github.com/OuyangWenyu/hydromodel/issues</li> <li>Tag it with <code>enhancement</code></li> <li>Explain in detail:</li> <li>What the feature would do</li> <li>Why it's needed</li> <li>How it should work (if you have ideas)</li> <li>Keep the scope narrow to make implementation easier</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Git for version control</li> <li>uv (recommended) or pip for package management</li> </ul>"},{"location":"contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":""},{"location":"contributing/#method-1-using-uv-recommended","title":"Method 1: Using uv (Recommended)","text":"<pre><code># 1. Fork the repository on GitHub\n# 2. Clone your fork\ngit clone https://github.com/your_username/hydromodel.git\ncd hydromodel\n\n# 3. Add upstream remote\ngit remote add upstream https://github.com/OuyangWenyu/hydromodel.git\n\n# 4. Install uv (if not already installed)\npip install uv\n\n# 5. Create development environment with all dependencies\nuv sync --all-extras\n\n# 6. Activate environment\n# On Windows:\n.venv\\Scripts\\activate\n# On macOS/Linux:\nsource .venv/bin/activate\n\n# 7. Verify installation\npytest test/\n</code></pre>"},{"location":"contributing/#method-2-using-pip-and-venv","title":"Method 2: Using pip and venv","text":"<pre><code># 1. Fork and clone (same as above)\ngit clone https://github.com/your_username/hydromodel.git\ncd hydromodel\n\n# 2. Create virtual environment\npython -m venv venv\n\n# 3. Activate environment\n# On Windows:\nvenv\\Scripts\\activate\n# On macOS/Linux:\nsource venv/bin/activate\n\n# 4. Install in editable mode with dev dependencies\npip install -e \".[dev,docs]\"\n\n# 5. Verify installation\npytest test/\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Create a branch for your changes:    <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b bugfix/issue-number-description\n</code></pre></p> </li> <li> <p>Make your changes following our coding standards (see below)</p> </li> <li> <p>Write tests for your changes:    <pre><code># Add tests to test/ directory\n# Run tests to ensure they pass\npytest test/\n</code></pre></p> </li> <li> <p>Update documentation if needed:    <pre><code># Edit docs in docs/ directory\n# Build docs locally to preview\nmkdocs serve\n# View at http://127.0.0.1:8000/\n</code></pre></p> </li> <li> <p>Format and lint your code:    <pre><code># Format with black (if available)\nblack hydromodel/ test/\n\n# Check code style (if flake8 available)\nflake8 hydromodel/ test/\n</code></pre></p> </li> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"Add feature: brief description\n\nDetailed description of what changed and why.\n\nFixes #123\"  # Reference issue number if applicable\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request on GitHub</p> </li> </ol>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"contributing/#python-style-guide","title":"Python Style Guide","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Use 4 spaces for indentation (no tabs)</li> <li>Maximum line length: 88 characters (Black default)</li> <li>Use type hints where appropriate</li> <li>Write docstrings for all public functions, classes, and modules</li> </ul>"},{"location":"contributing/#docstring-format","title":"Docstring Format","text":"<p>Use NumPy-style docstrings:</p> <pre><code>def calibrate(config, param_range_file=None):\n    \"\"\"\n    Calibrate hydrological model with specified configuration.\n\n    Parameters\n    ----------\n    config : dict\n        Configuration dictionary containing data_cfgs, model_cfgs,\n        training_cfgs, and evaluation_cfgs.\n    param_range_file : str, optional\n        Path to parameter range YAML file. If None, uses default ranges.\n\n    Returns\n    -------\n    dict\n        Dictionary mapping basin IDs to calibration results containing\n        best parameters and objective values.\n\n    Raises\n    ------\n    ValueError\n        If configuration is invalid or missing required fields.\n\n    Examples\n    --------\n    &gt;&gt;&gt; config = {...}\n    &gt;&gt;&gt; results = calibrate(config)\n    &gt;&gt;&gt; print(results['01013500']['best_params'])\n\n    See Also\n    --------\n    evaluate : Evaluate calibrated model\n    UnifiedSimulator : Run model simulation\n\n    Notes\n    -----\n    Results are saved to {output_dir}/{experiment_name}/ directory.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Functions/methods: <code>lowercase_with_underscores</code></li> <li>Classes: <code>CapitalizedWords</code></li> <li>Constants: <code>UPPERCASE_WITH_UNDERSCORES</code></li> <li>Private methods: <code>_leading_underscore</code></li> <li>Protected methods: <code>_single_leading_underscore</code></li> </ul>"},{"location":"contributing/#code-organization","title":"Code Organization","text":"<ul> <li>Keep functions focused and small (ideally &lt; 50 lines)</li> <li>Use meaningful variable names</li> <li>Add comments for complex logic</li> <li>Avoid deep nesting (max 3-4 levels)</li> <li>Extract repeated code into functions</li> </ul>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Use <code>pytest</code> for all tests</li> <li>Place tests in <code>test/</code> directory</li> <li>Name test files as <code>test_*.py</code></li> <li>Name test functions as <code>test_*</code></li> </ul> <p>Example test: <pre><code>import pytest\nimport numpy as np\nfrom hydromodel.models.xaj import xaj\n\ndef test_xaj_basic_run():\n    \"\"\"Test basic XAJ model execution.\"\"\"\n    # Arrange\n    params = np.array([0.5, 0.3, 0.05, 15.0, 75.0, 90.0,\n                       0.1, 50.0, 1.2, 0.3, 0.3, 0.5,\n                       5.0, 0.7, 0.99])\n    p_and_e = np.random.rand(100, 1, 2) * 10  # 100 days, 1 basin, 2 vars\n\n    # Act\n    results = xaj(p_and_e, params)\n\n    # Assert\n    assert results.shape == (100, 1)\n    assert not np.any(np.isnan(results))\n    assert np.all(results &gt;= 0)\n\ndef test_xaj_parameter_validation():\n    \"\"\"Test XAJ parameter validation.\"\"\"\n    with pytest.raises(ValueError):\n        xaj(np.random.rand(100, 1, 2), params=None)\n</code></pre></p>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest test/test_xaj.py\n\n# Run specific test\npytest test/test_xaj.py::test_xaj_basic_run\n\n# Run with coverage\npytest --cov=hydromodel --cov-report=html\n\n# Run with verbose output\npytest -v\n\n# Run only fast tests (skip slow)\npytest -m \"not slow\"\n</code></pre>"},{"location":"contributing/#test-coverage","title":"Test Coverage","text":"<ul> <li>Aim for &gt;80% code coverage for new code</li> <li>Write tests for:</li> <li>Normal cases</li> <li>Edge cases</li> <li>Error conditions</li> <li>Boundary values</li> </ul>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Tests pass: Run <code>pytest</code> and ensure all tests pass</li> <li>Code is formatted: Use <code>black</code> or similar formatter</li> <li>Documentation updated: Update relevant docs</li> <li>CHANGELOG updated: Add entry to <code>docs/changelog.md</code></li> <li>Commits are clean: Use meaningful commit messages</li> </ol>"},{"location":"contributing/#pr-template","title":"PR Template","text":"<p>When creating a PR, include:</p> <pre><code>## Description\nBrief description of changes.\n\n## Type of Change\n- [ ] Bug fix (non-breaking change fixing an issue)\n- [ ] New feature (non-breaking change adding functionality)\n- [ ] Breaking change (fix or feature causing existing functionality to change)\n- [ ] Documentation update\n\n## Related Issue\nFixes #123\n\n## How Has This Been Tested?\nDescribe the tests you ran.\n\n## Checklist\n- [ ] Code follows project style guidelines\n- [ ] Self-reviewed my own code\n- [ ] Commented complex code sections\n- [ ] Updated documentation\n- [ ] Added tests that prove fix/feature works\n- [ ] New and existing tests pass locally\n- [ ] CHANGELOG.md updated\n</code></pre>"},{"location":"contributing/#review-process","title":"Review Process","text":"<ol> <li>Maintainers will review your PR</li> <li>Address any requested changes</li> <li>Once approved, your PR will be merged</li> <li>Your contribution will be credited in the release notes</li> </ol>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>hydromodel/\n\u251c\u2500\u2500 hydromodel/              # Main package\n\u2502   \u251c\u2500\u2500 models/              # Model implementations\n\u2502   \u251c\u2500\u2500 trainers/            # Calibration, evaluation, simulation\n\u2502   \u251c\u2500\u2500 datasets/            # Data loading and preprocessing\n\u2502   \u2514\u2500\u2500 utils/               # Utility functions\n\u251c\u2500\u2500 test/                    # Tests\n\u251c\u2500\u2500 docs/                    # Documentation\n\u251c\u2500\u2500 scripts/                 # Command-line scripts\n\u251c\u2500\u2500 configs/                 # Example configurations\n\u2514\u2500\u2500 pyproject.toml           # Project configuration\n</code></pre>"},{"location":"contributing/#versioning","title":"Versioning","text":"<p>We use Semantic Versioning:</p> <ul> <li>MAJOR: Breaking changes</li> <li>MINOR: New features (backward compatible)</li> <li>PATCH: Bug fixes (backward compatible)</li> </ul> <p>Example: <code>0.2.11</code> \u2192 <code>0.3.0</code> (new features) \u2192 <code>1.0.0</code> (major release)</p>"},{"location":"contributing/#code-review-process","title":"Code Review Process","text":""},{"location":"contributing/#what-reviewers-look-for","title":"What Reviewers Look For","text":"<ul> <li>Correctness: Does the code work as intended?</li> <li>Tests: Are there adequate tests?</li> <li>Documentation: Is it well-documented?</li> <li>Style: Does it follow coding standards?</li> <li>Performance: Are there any performance concerns?</li> <li>Maintainability: Is the code easy to understand and maintain?</li> </ul>"},{"location":"contributing/#response-time","title":"Response Time","text":"<ul> <li>Initial review: within 1 week</li> <li>Follow-up reviews: within 3-5 days</li> <li>For urgent fixes: within 1-2 days</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<ul> <li>Issues: For bug reports and feature requests</li> <li>Discussions: For questions and ideas</li> <li>Pull Requests: For code contributions</li> <li>Email: wenyuouyang@outlook.com for private matters</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are credited in: - Release notes - <code>CONTRIBUTORS.md</code> file - GitHub contributors page</p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: https://OuyangWenyu.github.io/hydromodel</li> <li>Issues: https://github.com/OuyangWenyu/hydromodel/issues</li> <li>Discussions: https://github.com/OuyangWenyu/hydromodel/discussions</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the GNU General Public License v3.0.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"contributing/#our-pledge","title":"Our Pledge","text":"<p>We pledge to make participation in our project a harassment-free experience for everyone, regardless of: - Age, body size, disability - Ethnicity, gender identity and expression - Level of experience - Nationality, personal appearance - Race, religion, sexual identity and orientation</p>"},{"location":"contributing/#our-standards","title":"Our Standards","text":"<p>Positive behavior: - Using welcoming and inclusive language - Being respectful of differing viewpoints - Gracefully accepting constructive criticism - Focusing on what is best for the community</p> <p>Unacceptable behavior: - Use of sexualized language or imagery - Trolling, insulting/derogatory comments - Public or private harassment - Publishing others' private information</p>"},{"location":"contributing/#enforcement","title":"Enforcement","text":"<p>Project maintainers have the right to remove, edit, or reject contributions that do not align with this Code of Conduct.</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Don't hesitate to ask! We're here to help:</p> <ol> <li>Check existing issues</li> <li>Search discussions</li> <li>Open a new issue with the <code>question</code> label</li> <li>Email: wenyuouyang@outlook.com</li> </ol> <p>Thank you for contributing to hydromodel! \ud83c\udf89</p>"},{"location":"data_guide/","title":"Data Preparation Guide","text":"<p>This guide explains how to prepare and use data with <code>hydromodel</code>, covering both public CAMELS datasets and custom data.</p>"},{"location":"data_guide/#overview","title":"Overview","text":"<p><code>hydromodel</code> supports two main data sources:</p> <ol> <li>Public CAMELS Datasets - Using hydrodataset package</li> <li>11 global CAMELS variants (US, GB, AUS, BR, CL, etc.)</li> <li>Automatic download and caching</li> <li> <p>Standardized format and quality-controlled</p> </li> <li> <p>Custom Data - Using hydrodatasource package</p> </li> <li>Your own basin data</li> <li>Flexible data organization</li> <li>Integration with cloud storage</li> </ol>"},{"location":"data_guide/#option-1-using-camels-datasets-hydrodataset","title":"Option 1: Using CAMELS Datasets (hydrodataset)","text":""},{"location":"data_guide/#step-1-install-hydrodataset","title":"Step 1: Install hydrodataset","text":"<pre><code>pip install hydrodataset\n</code></pre>"},{"location":"data_guide/#step-2-configure-data-path-optional","title":"Step 2: Configure Data Path (Optional)","text":"<p><code>hydromodel</code> automatically uses default paths, but you can customize:</p> <p>Default paths: - Windows: <code>C:\\Users\\YourUsername\\hydromodel_data\\</code> - macOS/Linux: <code>~/hydromodel_data/</code></p> <p>To customize, create <code>~/hydro_setting.yml</code>:</p> <pre><code>local_data_path:\n  root: 'D:/data'\n  datasets-origin: 'D:/data'  # CAMELS datasets location\n</code></pre> <p>Important: Provide only the <code>datasets-origin</code> directory. The system automatically appends the dataset name (e.g., <code>CAMELS_US</code>, <code>CAMELS_GB</code>).</p> <p>Example: If your data is in <code>D:/data/CAMELS_US/</code>, set <code>datasets-origin: 'D:/data'</code>.</p>"},{"location":"data_guide/#step-3-download-data","title":"Step 3: Download Data","text":"<p>The data downloads automatically on first use:</p> <pre><code>from hydrodataset.camels_us import CamelsUs\nfrom hydrodataset import SETTING\n\n# Initialize dataset (auto-downloads if not present)\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsUs(data_path, download=True)\n\n# Get available basins\nbasin_ids = ds.read_object_ids()\nprint(f\"Downloaded {len(basin_ids)} basins\")\n</code></pre> <p>Note: First download may take 30-120 minutes depending on dataset size. CAMELS-US is ~70GB.</p>"},{"location":"data_guide/#step-4-use-with-hydromodel","title":"Step 4: Use with hydromodel","text":"<pre><code>from hydromodel.trainers.unified_calibrate import calibrate\n\nconfig = {\n    \"data_cfgs\": {\n        \"data_source_type\": \"camels_us\",  # Dataset name\n        \"basin_ids\": [\"01013500\", \"01022500\"],\n        \"train_period\": [\"1990-10-01\", \"2000-09-30\"],\n        \"test_period\": [\"2000-10-01\", \"2010-09-30\"],\n        \"warmup_length\": 365,\n        \"variables\": [\"precipitation\", \"potential_evapotranspiration\", \"streamflow\"]\n    },\n    # ... other configs\n}\n\nresults = calibrate(config)\n</code></pre>"},{"location":"data_guide/#available-camels-datasets","title":"Available CAMELS Datasets","text":"Dataset Region Basins Package Name CAMELS-US United States 671 <code>camels_us</code> CAMELS-GB Great Britain 671 <code>camels_gb</code> CAMELS-AUS Australia 222 <code>camels_aus</code> CAMELS-BR Brazil 897 <code>camels_br</code> CAMELS-CL Chile 516 <code>camels_cl</code> CAMELS-CH Switzerland 331 <code>camels_ch</code> CAMELS-DE Germany 1555 <code>camels_de</code> CAMELS-DK Denmark 304 <code>camels_dk</code> CAMELS-FR France 654 <code>camels_fr</code> CAMELS-NZ New Zealand 70 <code>camels_nz</code> CAMELS-SE Sweden 54 <code>camels_se</code> <p>Usage example:</p> <pre><code># Use different datasets by changing data_source_type\nconfig[\"data_cfgs\"][\"data_source_type\"] = \"camels_gb\"\nconfig[\"data_cfgs\"][\"basin_ids\"] = [\"28015\"]  # GB basin ID\n</code></pre>"},{"location":"data_guide/#camels-data-structure","title":"CAMELS Data Structure","text":"<p>CAMELS datasets provide standardized variables:</p> <p>Time Series Variables: - <code>precipitation</code> (mm/day or mm/hour) - <code>potential_evapotranspiration</code> (mm/day) - <code>streamflow</code> (mm/day or m\u00b3/s) - <code>temperature</code> (\u00b0C) - And more depending on dataset</p> <p>Basin Attributes: - <code>area</code> (km\u00b2) - <code>elevation</code> (m) - <code>latitude</code>, <code>longitude</code> - Climate, soil, vegetation attributes</p> <p>For detailed documentation, see: - hydrodataset GitHub - hydrodataset documentation</p>"},{"location":"data_guide/#option-2-using-custom-data-hydrodatasource","title":"Option 2: Using Custom Data (hydrodatasource)","text":""},{"location":"data_guide/#step-1-install-hydrodatasource","title":"Step 1: Install hydrodatasource","text":"<pre><code>pip install hydrodatasource\n</code></pre>"},{"location":"data_guide/#step-2-organize-your-data","title":"Step 2: Organize Your Data","text":"<p>Create a directory with this structure:</p> <pre><code>my_basin_data/\n\u251c\u2500\u2500 attributes/\n\u2502   \u2514\u2500\u2500 attributes.csv              # Basin metadata (required)\n\u251c\u2500\u2500 timeseries/\n\u2502   \u251c\u2500\u2500 1D/                         # Daily time series\n\u2502   \u2502   \u251c\u2500\u2500 basin_001.csv          # One file per basin\n\u2502   \u2502   \u251c\u2500\u2500 basin_002.csv\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 1D_units_info.json          # Variable units (required)\n\u2514\u2500\u2500 shapes/                         # Basin boundaries (optional)\n    \u2514\u2500\u2500 basins.shp\n</code></pre>"},{"location":"data_guide/#step-3-prepare-required-files","title":"Step 3: Prepare Required Files","text":""},{"location":"data_guide/#31-attributescsv","title":"3.1 attributes.csv","text":"<p>Minimum required columns: <code>basin_id</code> and <code>area</code> (km\u00b2)</p> <pre><code>basin_id,area,lat,lon,elevation\nbasin_001,1250.5,30.5,105.2,850\nbasin_002,856.3,31.2,106.1,920\n</code></pre> <p>Important: - <code>basin_id</code>: String identifier (matches filename) - <code>area</code>: Basin area in km\u00b2 - Other columns optional but recommended</p>"},{"location":"data_guide/#32-basin_xxxcsv-time-series","title":"3.2 basin_XXX.csv (Time Series)","text":"<p>Required column: <code>time</code>. Other columns are your variables.</p> <pre><code>time,prcp,PET,streamflow\n1990-01-01,5.2,2.1,45.3\n1990-01-02,0.0,2.3,42.1\n1990-01-03,12.5,1.8,58.7\n</code></pre> <p>Important: - <code>time</code> format: <code>YYYY-MM-DD</code> (for daily data) - Variable names: Lowercase, underscores for multi-word - Missing values: Use empty cells or <code>NaN</code> (not <code>-9999</code> or <code>0</code>) - No duplicate time stamps</p>"},{"location":"data_guide/#33-1d_units_infojson-units-definition","title":"3.3 1D_units_info.json (Units Definition)","text":"<p>Define physical units for all variables:</p> <pre><code>{\n  \"prcp\": \"mm/day\",\n  \"PET\": \"mm/day\",\n  \"streamflow\": \"m^3/s\",\n  \"temp\": \"degC\"\n}\n</code></pre> <p>Common units: - Precipitation/ET: <code>mm/day</code> or <code>mm/hour</code> - Streamflow: <code>m^3/s</code> or <code>mm/day</code> - Temperature: <code>degC</code> or <code>K</code> - Area: <code>km^2</code></p>"},{"location":"data_guide/#step-4-verify-data-structure","title":"Step 4: Verify Data Structure","text":"<pre><code>from hydrodatasource.reader.data_source import SelfMadeHydroDataset\n\n# Initialize dataset\ndataset = SelfMadeHydroDataset(\n    data_path=\"D:/my_basin_data\",\n    time_unit=\"1D\"\n)\n\n# Check basins\nbasin_ids = dataset.read_object_ids()\nprint(f\"Found {len(basin_ids)} basins: {basin_ids}\")\n\n# Check time series\ndata = dataset.read_timeseries(\n    gage_id_lst=[\"basin_001\"],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\"prcp\", \"PET\", \"streamflow\"]\n)\n\nprint(f\"Data shape: {data['1D'].shape}\")  # [n_basins, n_time, n_vars]\n</code></pre>"},{"location":"data_guide/#step-5-use-with-hydromodel","title":"Step 5: Use with hydromodel","text":"<pre><code>from hydromodel.trainers.unified_calibrate import calibrate\n\nconfig = {\n    \"data_cfgs\": {\n        \"data_source_type\": \"selfmadehydrodataset\",  # Use custom data\n        \"data_source_path\": \"D:/my_basin_data\",      # Your data path\n        \"basin_ids\": [\"basin_001\", \"basin_002\"],\n        \"train_period\": [\"1990-01-01\", \"2000-12-31\"],\n        \"test_period\": [\"2001-01-01\", \"2010-12-31\"],\n        \"warmup_length\": 365,\n    },\n    \"model_cfgs\": {\n        \"model_name\": \"xaj_mz\",\n    },\n    \"training_cfgs\": {\n        \"algorithm\": \"SCE_UA\",\n        \"loss_func\": \"RMSE\",\n        \"output_dir\": \"results\",\n        \"experiment_name\": \"my_basins\",\n        \"rep\": 10000,\n        \"ngs\": 100,\n    },\n    \"evaluation_cfgs\": {\n        \"metrics\": [\"NSE\", \"KGE\", \"RMSE\"],\n    },\n}\n\nresults = calibrate(config)\n</code></pre>"},{"location":"data_guide/#option-3-using-flood-event-data-hydrodatasource","title":"Option 3: Using Flood Event Data (hydrodatasource)","text":""},{"location":"data_guide/#overview_1","title":"Overview","text":"<p>Flood event data is designed for event-based hydrological modeling where you focus on specific flood episodes rather than continuous time series. This is particularly useful for:</p> <ul> <li>Flood forecasting and warning systems</li> <li>Peak flow estimation</li> <li>Event-based rainfall-runoff analysis</li> <li>Unit hydrograph calibration</li> </ul>"},{"location":"data_guide/#key-differences-from-continuous-data","title":"Key Differences from Continuous Data","text":"Feature Continuous Data Flood Event Data Data Structure Complete time series Individual flood events with gaps Input Features 2D: [prcp, PET] 4D: [prcp, PET, marker, event_id] Warmup Handling Removed after simulation Included in each event (NaN markers) Time Coverage Full period Only flood periods + warmup Use Case Long-term water balance Flood peak prediction"},{"location":"data_guide/#data-structure-and-components","title":"Data Structure and Components","text":""},{"location":"data_guide/#1-event-format","title":"1. Event Format","text":"<p>Each flood event contains:</p> <pre><code>event = {\n    \"rain\": np.array([...]),           # Precipitation (with NaN in warmup)\n    \"ES\": np.array([...]),             # Evapotranspiration\n    \"inflow\": np.array([...]),         # Streamflow (with NaN in warmup)\n    \"flood_event_markers\": np.array([...]),  # NaN=warmup, 1=flood\n    \"event_id\": 1,                     # Event identifier\n    \"time\": np.array([...])            # Datetime array\n}\n</code></pre>"},{"location":"data_guide/#2-three-key-periods-in-each-event","title":"2. Three Key Periods in Each Event","text":"<pre><code>[Warmup Period] \u2192 [Flood Period] \u2192 [GAP]\n  marker=NaN        marker=1         marker=0\n</code></pre> <p>Warmup Period (e.g., 30 days before flood): - Contains NaN values in observations - Used to initialize model states - Length specified by <code>warmup_length</code> in config - Extracted from real data before the flood event</p> <p>Flood Period (actual event): - Contains valid observations (marker=1) - The period of interest for simulation - Used for model calibration and evaluation</p> <p>GAP Period (between events): - Artificial buffer (10 time steps by default) - Designed for visualization clarity - NOT used in simulation (marker=0, ignored by model) - Created with: precipitation=0, ET=0.27, flow=0</p>"},{"location":"data_guide/#3-how-events-are-concatenated","title":"3. How Events are Concatenated","text":"<p>When loading multiple events, they are combined as:</p> <pre><code>Event1: [warmup-NaN][flood-1][GAP-0]\nEvent2: [warmup-NaN][flood-1][GAP-0]\nEvent3: [warmup-NaN][flood-1]\n</code></pre> <p>Important: The final data structure includes GAP periods, but these are automatically skipped during simulation based on the marker values.</p>"},{"location":"data_guide/#simulation-behavior","title":"Simulation Behavior","text":""},{"location":"data_guide/#event-detection","title":"Event Detection","text":"<p>During simulation (<code>unified_simulate.py</code>), the system:</p> <ol> <li>Reads the <code>flood_event_markers</code> array (3rd feature)</li> <li>Uses <code>find_flood_event_segments_as_tuples()</code> to identify events</li> <li>Only processes segments where <code>marker &gt; 0</code> (i.e., marker=1)</li> <li>Skips GAP periods (marker=0) completely</li> </ol> <pre><code># Simulation automatically identifies event segments\nflood_event_array = inputs[:, basin_idx, 2]  # marker column\nevent_segments = find_flood_event_segments_as_tuples(\n    flood_event_array, warmup_length\n)\n\n# Each event is simulated independently\nfor start, end, orig_start, orig_end in event_segments:\n    event_inputs = inputs[start:end+1, :, :3]  # Extract event data\n    result = model(event_inputs, ...)  # Simulate this event\n</code></pre>"},{"location":"data_guide/#why-gap-doesnt-affect-results","title":"Why GAP Doesn't Affect Results","text":"<p>The GAP period is present in the loaded data but does not participate in simulation:</p> <ul> <li>\u2705 GAP helps separate events visually in plots</li> <li>\u2705 GAP provides clear boundaries between independent floods</li> <li>\u274c GAP data is never fed to the hydrological model</li> <li>\u274c GAP does not contribute to loss calculation</li> </ul> <p>This design ensures that: 1. Each flood event is simulated independently 2. Model states are reset via warmup for each event 3. Events don't interfere with each other</p>"},{"location":"data_guide/#configuration-example","title":"Configuration Example","text":"<pre><code>data:\n  dataset: \"floodevent\"\n  dataset_name: \"my_flood_events\"\n  data_source_path: \"D:/flood_data\"\n  is_event_data: true\n  time_unit: [\"1D\"]\n\n  # Datasource parameters\n  datasource_kwargs:\n    warmup_length: 30      # Days before flood for warmup\n    offset_to_utc: false   # Time zone handling\n    version: null\n\n  basin_ids: [\"basin_001\"]\n  train_period: [\"2000-01-01\", \"2020-12-31\"]\n  test_period: [\"2020-01-01\", \"2023-12-31\"]\n\n  variables: [\"rain\", \"ES\", \"inflow\", \"flood_event\"]\n  warmup_length: 30\n\nmodel:\n  name: \"xaj\"\n  params:\n    source_type: \"sources\"\n    source_book: \"HF\"\n    kernel_size: 15\n</code></pre>"},{"location":"data_guide/#data-file-structure","title":"Data File Structure","text":"<pre><code>my_flood_events/\n\u251c\u2500\u2500 attributes/\n\u2502   \u2514\u2500\u2500 attributes.csv\n\u251c\u2500\u2500 timeseries/\n\u2502   \u251c\u2500\u2500 1D/\n\u2502   \u2502   \u251c\u2500\u2500 basin_001.csv      # Must include 'flood_event' column\n\u2502   \u2502   \u2514\u2500\u2500 basin_002.csv\n\u2502   \u2514\u2500\u2500 1D_units_info.json\n\u2514\u2500\u2500 shapes/\n    \u2514\u2500\u2500 basins.shp\n</code></pre>"},{"location":"data_guide/#basin_001csv-format","title":"basin_001.csv Format","text":"<pre><code>time,rain,ES,inflow,flood_event\n2020-06-01,0.0,3.2,5.1,0\n2020-06-02,2.5,3.5,5.8,0\n2020-07-01,5.0,4.0,10.2,1    # Flood event starts\n2020-07-02,12.5,3.8,25.5,1\n2020-07-03,8.0,3.5,30.1,1\n2020-07-04,0.0,3.2,20.5,1\n2020-07-05,0.0,3.0,12.0,0    # Event ends\n</code></pre> <p>Key Points: - <code>flood_event</code> column: 0=no flood, 1=flood period - Continuous time series with all periods marked - System automatically extracts events with warmup</p>"},{"location":"data_guide/#best-practices","title":"Best Practices","text":"<ol> <li>Warmup Length:</li> <li>Typical: 30 days for daily data</li> <li>Should be long enough to initialize soil moisture states</li> <li>Too short: poor initial conditions</li> <li> <p>Too long: data availability issues</p> </li> <li> <p>Event Selection:</p> </li> <li>Focus on significant floods (peak &gt; threshold)</li> <li>Include complete rising and recession limbs</li> <li> <p>Ensure warmup period has valid data</p> </li> <li> <p>Data Quality:</p> </li> <li>Check for missing data in warmup periods</li> <li>Verify flood markers are correctly assigned</li> <li> <p>Ensure precipitation and flow are synchronized</p> </li> <li> <p>Marker Assignment:    <pre><code># Example: Mark floods based on threshold\nthreshold = flow.quantile(0.95)\nflood_event = (flow &gt; threshold).astype(int)\n</code></pre></p> </li> </ol>"},{"location":"data_guide/#common-issues","title":"Common Issues","text":"<p>1. \"Warmup period contains all NaN\" - Ensure data exists before each flood event - Check <code>warmup_length</code> is not too long - Verify CSV has continuous time series</p> <p>2. \"No flood events found\" - Check <code>flood_event</code> column exists - Verify flood markers are 1 (not True or other values) - Ensure train_period covers some flood events</p> <p>3. \"Simulation results are all zeros\" - Check if events are detected: markers should be 1 for flood periods - Verify warmup_length matches the actual warmup in data - Ensure model parameters are physically reasonable</p>"},{"location":"data_guide/#advanced-manual-event-creation","title":"Advanced: Manual Event Creation","text":"<p>For custom event extraction:</p> <pre><code>from hydroutils import hydro_event\n\n# Extract events from continuous data\nevents = hydro_event.extract_flood_events(\n    df=continuous_data,\n    warmup_length=30,\n    flood_event_col=\"flood_event\",\n    time_col=\"time\"\n)\n\n# Each event includes warmup automatically\nfor event in events:\n    print(f\"Event: {event['event_name']}\")\n    print(f\"  Total length: {len(event['data'])}\")\n    print(f\"  Warmup markers: {event['data']['flood_event'].isna().sum()}\")\n    print(f\"  Flood markers: {(event['data']['flood_event']==1).sum()}\")\n</code></pre>"},{"location":"data_guide/#data-requirements-for-xaj-model","title":"Data Requirements for XAJ Model","text":""},{"location":"data_guide/#required-variables","title":"Required Variables","text":"Variable Description Unit Typical Source <code>prcp</code> Precipitation mm/day Rain gauge, gridded data (CHIRPS, ERA5) <code>PET</code> Potential Evapotranspiration mm/day Penman, Priestley-Taylor, or reanalysis <code>streamflow</code> Observed streamflow m\u00b3/s Stream gauge <code>area</code> Basin area km\u00b2 GIS analysis"},{"location":"data_guide/#optional-variables","title":"Optional Variables","text":"Variable Description Unit Usage <code>temp</code> Temperature \u00b0C Snow module (if enabled) <code>elevation</code> Basin elevation m PET estimation <code>lat</code>, <code>lon</code> Coordinates degrees Spatial analysis"},{"location":"data_guide/#data-quality-guidelines","title":"Data Quality Guidelines","text":"<ol> <li> <p>Time Resolution: Daily (1D) is standard for XAJ model</p> </li> <li> <p>Data Completeness:</p> </li> <li>Training period: \u22655 years continuous data</li> <li>Warmup period: \u22651 year before training</li> <li> <p>Missing data: &lt;5% acceptable, continuous gaps &lt;7 days</p> </li> <li> <p>Physical Consistency:</p> </li> <li>Precipitation \u2265 0</li> <li>Streamflow \u2265 0</li> <li>PET \u2265 0</li> <li> <p>Check water balance: P \u2248 Q + ET (within 20%)</p> </li> <li> <p>Unit Consistency:</p> </li> <li>Ensure all units match <code>units_info.json</code></li> <li>Use consistent time stamps (no daylight saving shifts)</li> </ol>"},{"location":"data_guide/#advanced-features","title":"Advanced Features","text":""},{"location":"data_guide/#netcdf-caching-for-large-datasets","title":"NetCDF Caching (For Large Datasets)","text":"<p>Convert CSV to NetCDF for 10x faster access:</p> <pre><code>from hydrodatasource.reader.data_source import SelfMadeHydroDataset\n\ndataset = SelfMadeHydroDataset(\n    data_path=\"D:/my_basin_data\",\n    time_unit=\"1D\"\n)\n\n# Cache all data as NetCDF (one-time operation)\ndataset.cache_xrdataset(\n    gage_id_lst=basin_ids,\n    t_range=[\"1990-01-01\", \"2010-12-31\"],\n    var_lst=[\"prcp\", \"PET\", \"streamflow\"]\n)\n\n# Now access is much faster\ndata_xr = dataset.read_ts_xrdataset(\n    gage_id_lst=[\"basin_001\"],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\"prcp\", \"PET\", \"streamflow\"]\n)\n</code></pre>"},{"location":"data_guide/#multi-scale-time-series","title":"Multi-Scale Time Series","text":"<p>Support different time scales in one dataset:</p> <pre><code>timeseries/\n\u251c\u2500\u2500 1h/                     # Hourly data\n\u2502   \u251c\u2500\u2500 basin_001.csv\n\u2502   \u2514\u2500\u2500 1h_units_info.json\n\u251c\u2500\u2500 1D/                     # Daily data (most common)\n\u2502   \u251c\u2500\u2500 basin_001.csv\n\u2502   \u2514\u2500\u2500 1D_units_info.json\n\u2514\u2500\u2500 8D/                     # 8-day data (e.g., MODIS)\n    \u251c\u2500\u2500 basin_001.csv\n    \u2514\u2500\u2500 8D_units_info.json\n</code></pre> <p>Specify in config: <pre><code>dataset = SelfMadeHydroDataset(\n    data_path=\"D:/my_basin_data\",\n    time_unit=\"1h\"  # or \"1D\", \"8D\"\n)\n</code></pre></p>"},{"location":"data_guide/#cloud-storage-minios3","title":"Cloud Storage (MinIO/S3)","text":"<p>For large datasets in the cloud:</p> <pre><code>from hydrodatasource.reader.data_source import SelfMadeHydroDataset\n\ndataset = SelfMadeHydroDataset(\n    data_path=\"s3://my-bucket/basin-data\",\n    time_unit=\"1D\",\n    minio_paras={\n        \"endpoint_url\": \"http://minio.example.com:9000\",\n        \"key_id\": \"access_key\",\n        \"secret_key\": \"secret_key\"\n    }\n)\n</code></pre>"},{"location":"data_guide/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Here's a complete example from raw data to calibration:</p> <pre><code>import pandas as pd\nimport json\nfrom hydrodatasource.reader.data_source import SelfMadeHydroDataset\nfrom hydromodel.trainers.unified_calibrate import calibrate\n\n# Step 1: Prepare attributes\nattributes = pd.DataFrame({\n    'basin_id': ['basin_001', 'basin_002'],\n    'area': [1250.5, 856.3],\n    'lat': [30.5, 31.2],\n    'lon': [105.2, 106.1]\n})\nattributes.to_csv(\"my_data/attributes/attributes.csv\", index=False)\n\n# Step 2: Prepare time series (assume you have daily_data_001.csv)\n# Make sure it has columns: time, prcp, PET, streamflow\ndaily_data = pd.read_csv(\"daily_data_001.csv\")\ndaily_data.to_csv(\"my_data/timeseries/1D/basin_001.csv\", index=False)\n\n# Step 3: Create units info\nunits = {\n    \"prcp\": \"mm/day\",\n    \"PET\": \"mm/day\",\n    \"streamflow\": \"m^3/s\"\n}\nwith open(\"my_data/timeseries/1D_units_info.json\", \"w\") as f:\n    json.dump(units, f, indent=2)\n\n# Step 4: Verify data loads correctly\ndataset = SelfMadeHydroDataset(\n    data_path=\"my_data\",\n    time_unit=\"1D\"\n)\nprint(f\"Basins: {dataset.read_object_ids()}\")\n\n# Step 5: Cache for faster access (optional)\ndataset.cache_xrdataset(\n    gage_id_lst=['basin_001'],\n    t_range=[\"1990-01-01\", \"2010-12-31\"],\n    var_lst=[\"prcp\", \"PET\", \"streamflow\"]\n)\n\n# Step 6: Run calibration with hydromodel\nconfig = {\n    \"data_cfgs\": {\n        \"data_source_type\": \"selfmadehydrodataset\",\n        \"data_source_path\": \"my_data\",\n        \"basin_ids\": [\"basin_001\"],\n        \"train_period\": [\"1990-01-01\", \"2000-12-31\"],\n        \"test_period\": [\"2001-01-01\", \"2010-12-31\"],\n        \"warmup_length\": 365,\n    },\n    \"model_cfgs\": {\n        \"model_name\": \"xaj_mz\",\n    },\n    \"training_cfgs\": {\n        \"algorithm\": \"SCE_UA\",\n        \"loss_func\": \"RMSE\",\n        \"output_dir\": \"results\",\n        \"experiment_name\": \"my_basin_001\",\n        \"rep\": 5000,\n        \"ngs\": 50,\n    },\n    \"evaluation_cfgs\": {\n        \"metrics\": [\"NSE\", \"KGE\", \"RMSE\"],\n    },\n}\n\nresults = calibrate(config)\nprint(\"Calibration complete!\")\n</code></pre>"},{"location":"data_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data_guide/#common-issues_1","title":"Common Issues","text":"<p>1. \"Basin ID not found\" - Check <code>basin_id</code> column in <code>attributes.csv</code> matches CSV filenames - Basin IDs must be strings (not numbers) - Filenames: <code>{basin_id}.csv</code> (e.g., <code>basin_001.csv</code>)</p> <p>2. \"Time column not found\" - CSV must have <code>time</code> column (case-sensitive) - Format: <code>YYYY-MM-DD</code> for daily, <code>YYYY-MM-DD HH:MM</code> for hourly</p> <p>3. \"Unit info file not found\" - Create <code>{time_unit}_units_info.json</code> in timeseries folder - Example: <code>1D_units_info.json</code> for daily data</p> <p>4. \"Variable not found in units info\" - Every variable in CSV must be in <code>units_info.json</code> - Check spelling matches exactly (case-sensitive)</p> <p>5. \"Data shape mismatch\" - All basins should have same variables - All basins should cover the requested time range</p> <p>6. CAMELS data download fails - Check internet connection - Check disk space (CAMELS-US needs ~70GB) - Try manual download from official sources - Set <code>download=False</code> if data already exists</p>"},{"location":"data_guide/#data-validation-checklist","title":"Data Validation Checklist","text":"<p>Before using your custom data:</p> <ul> <li>[ ] <code>attributes.csv</code> exists with <code>basin_id</code> and <code>area</code> columns</li> <li>[ ] Time series files named <code>{basin_id}.csv</code></li> <li>[ ] All CSV files have <code>time</code> column</li> <li>[ ] <code>{time_unit}_units_info.json</code> exists</li> <li>[ ] All variables in CSV are in <code>units_info.json</code></li> <li>[ ] No negative precipitation or streamflow values</li> <li>[ ] Time series is continuous (no large gaps)</li> <li>[ ] Data covers warmup + train + test periods</li> <li>[ ] Units are physically reasonable</li> </ul>"},{"location":"data_guide/#data-conversion-tools","title":"Data Conversion Tools","text":""},{"location":"data_guide/#from-gis-shapefile","title":"From GIS Shapefile","text":"<p>Extract basin attributes from shapefile:</p> <pre><code>import geopandas as gpd\n\n# Read shapefile\nbasins = gpd.read_file(\"basins.shp\")\n\n# Calculate area (convert to km\u00b2)\nbasins['area'] = basins.geometry.area / 1e6\n\n# Export to CSV\nbasins[['basin_id', 'area', 'lat', 'lon']].to_csv(\n    \"attributes/attributes.csv\",\n    index=False\n)\n</code></pre>"},{"location":"data_guide/#from-other-formats","title":"From Other Formats","text":"<pre><code># From Excel\nimport pandas as pd\ndf = pd.read_excel(\"basin_data.xlsx\")\ndf.to_csv(\"timeseries/1D/basin_001.csv\", index=False)\n\n# From NetCDF\nimport xarray as xr\nds = xr.open_dataset(\"data.nc\")\ndf = ds.to_dataframe().reset_index()\ndf.to_csv(\"timeseries/1D/basin_001.csv\", index=False)\n</code></pre>"},{"location":"data_guide/#summary","title":"Summary","text":""},{"location":"data_guide/#quick-decision-guide","title":"Quick Decision Guide","text":"<p>Choose CAMELS (hydrodataset) if: - \u2705 You need quality-controlled data - \u2705 Working with well-studied basins - \u2705 Want standardized format - \u2705 Need consistent attributes</p> <p>Choose Custom Data (hydrodatasource) if: - \u2705 Using your own field data - \u2705 Working with ungauged basins - \u2705 Need specific time periods - \u2705 Have proprietary data</p>"},{"location":"data_guide/#key-points","title":"Key Points","text":"<ol> <li>Public Data: Use <code>hydrodataset</code> for CAMELS variants</li> <li>Custom Data: Use <code>hydrodatasource</code> with <code>selfmadehydrodataset</code> format</li> <li>Data Structure: Follow standard directory layout</li> <li>Required Files: <code>attributes.csv</code>, time series CSVs, <code>units_info.json</code></li> <li>Data Quality: Check completeness, consistency, and physical validity</li> <li>Performance: Use NetCDF caching for large datasets</li> </ol>"},{"location":"data_guide/#additional-resources","title":"Additional Resources","text":"<ul> <li>hydrodataset GitHub: https://github.com/OuyangWenyu/hydrodataset</li> <li>hydrodataset docs: https://hydrodataset.readthedocs.io/</li> <li>hydrodatasource GitHub: https://github.com/OuyangWenyu/hydrodatasource</li> <li>hydromodel docs: usage.md, quickstart.md</li> <li>CAMELS official sites:</li> <li>US: https://ral.ucar.edu/solutions/products/camels</li> <li>GB: https://catalogue.ceh.ac.uk/documents/8344e4f3-d2ea-44f5-8afa-86d2987543a9</li> </ul>"},{"location":"faq/","title":"FAQ","text":""},{"location":"hydromodel/","title":"hydromodel module","text":"<p>Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"hydromodel/#hydromodel.Basin","title":"<code>Basin</code>","text":"<p>Comprehensive basin configuration class for hydrological modeling.</p> <p>This class encapsulates all basin-related information and provides interfaces for both lumped and semi-distributed modeling approaches. It serves as the central hub for basin data management and model configuration decisions.</p> <p>Design Philosophy: - Single source of truth for basin information - Support for both lumped and semi-distributed models - Extensible for future modeling approaches - Integration with frontend basin configuration</p> Source code in <code>hydromodel/trainers/basin.py</code> <pre><code>class Basin:\n    \"\"\"\n    Comprehensive basin configuration class for hydrological modeling.\n\n    This class encapsulates all basin-related information and provides interfaces\n    for both lumped and semi-distributed modeling approaches. It serves as the\n    central hub for basin data management and model configuration decisions.\n\n    Design Philosophy:\n    - Single source of truth for basin information\n    - Support for both lumped and semi-distributed models\n    - Extensible for future modeling approaches\n    - Integration with frontend basin configuration\n    \"\"\"\n\n    def __init__(\n        self,\n        basin_info: Union[BasinInfo, Dict[str, Any]],\n        modeling_approach: str = \"lumped\",\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize basin configuration.\n\n        Parameters\n        ----------\n        basin_info : BasinInfo or Dict[str, Any]\n            Basic basin information\n        modeling_approach : str, default \"lumped\"\n            Modeling approach: \"lumped\" or \"semi_distributed\"\n        **kwargs\n            Additional configuration parameters\n        \"\"\"\n        # Handle basin_info input\n        if isinstance(basin_info, dict):\n            self.basin_info = BasinInfo(**basin_info)\n        else:\n            self.basin_info = basin_info\n\n        # Modeling configuration\n        self.modeling_approach = modeling_approach\n\n        # Additional configuration\n        self.config = kwargs\n\n        # Initialize approach-specific configurations\n        self._setup_modeling_approach()\n\n    def _setup_modeling_approach(self):\n        \"\"\"Setup configuration based on modeling approach.\"\"\"\n        if self.modeling_approach == \"lumped\":\n            self._setup_lumped_config()\n        elif self.modeling_approach == \"semi_distributed\":\n            self._setup_semi_distributed_config()\n        else:\n            raise ValueError(\n                f\"Unsupported modeling approach: {self.modeling_approach}\"\n            )\n\n    def _setup_lumped_config(self):\n        \"\"\"Setup configuration for lumped modeling.\"\"\"\n        # For lumped models, we use single basin area\n        self.n_units = 1\n        self.unit_areas = np.array([self.basin_info.basin_area])\n        self.unit_ids = [self.basin_info.basin_id]\n\n    def _setup_semi_distributed_config(self):\n        \"\"\"Setup configuration for semi-distributed modeling.\"\"\"\n        # For semi-distributed models, we would have multiple units\n        # This is a placeholder for future implementation\n        sub_basins = self.config.get(\"sub_basins\", [])\n\n        if sub_basins:\n            self.n_units = len(sub_basins)\n            self.unit_areas = np.array(\n                [sub[\"basin_area\"] for sub in sub_basins]\n            )\n            self.unit_ids = [sub[\"basin_id\"] for sub in sub_basins]\n        else:\n            # Fallback to lumped if no sub-basins defined\n            self._setup_lumped_config()\n\n    @property\n    def basin_area(self) -&gt; float:\n        \"\"\"Get total basin area in km\u00b2.\"\"\"\n        return self.basin_info.basin_area\n\n    @property\n    def main_river_length(self) -&gt; float:\n        \"\"\"Get main river length in km.\"\"\"\n        return self.basin_info.main_river_length\n\n    @property\n    def basin_id(self) -&gt; str:\n        \"\"\"Get basin ID.\"\"\"\n        return self.basin_info.basin_id\n\n    @property\n    def basin_name(self) -&gt; str:\n        \"\"\"Get basin name.\"\"\"\n        return self.basin_info.basin_name\n\n    def is_lumped(self) -&gt; bool:\n        \"\"\"Check if this is a lumped model configuration.\"\"\"\n        return self.modeling_approach == \"lumped\"\n\n    def is_semi_distributed(self) -&gt; bool:\n        \"\"\"Check if this is a semi-distributed model configuration.\"\"\"\n        return self.modeling_approach == \"semi_distributed\"\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert basin configuration to dictionary.\"\"\"\n        return {\n            \"basin_info\": self.basin_info.to_dict(),\n            \"modeling_approach\": self.modeling_approach,\n            \"n_units\": self.n_units,\n            \"unit_areas\": self.unit_areas.tolist(),\n            \"unit_ids\": self.unit_ids,\n            \"config\": self.config,\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Basin\":\n        \"\"\"Create Basin instance from dictionary.\"\"\"\n        basin_info = BasinInfo(**data[\"basin_info\"])\n\n        return cls(\n            basin_info=basin_info,\n            modeling_approach=data.get(\"modeling_approach\", \"lumped\"),\n            **data.get(\"config\", {}),\n        )\n\n    @classmethod\n    def from_config(cls, basin_data: Dict[str, Any]) -&gt; \"Basin\":\n        \"\"\"\n        Create Basin instance from configuration data.\n\n        This is a flexible factory method that can handle various data formats\n        and field name variations commonly used in basin configuration.\n\n        Parameters\n        ----------\n        basin_data : Dict[str, Any]\n            Basin configuration data, supporting various field name formats:\n            - basin_id/basin_code, basin_name/name, basin_area\n            - output_unit, time_step_hours, modeling_approach\n            - geographical and hydrological attributes\n\n        Returns\n        -------\n        Basin\n            Configured Basin instance\n        \"\"\"\n        # Extract basic basin info\n        basin_info = BasinInfo(\n            basin_id=basin_data.get(\n                \"basin_id\", basin_data.get(\"basin_code\", \"unknown\")\n            ),\n            basin_name=basin_data.get(\n                \"basin_name\", basin_data.get(\"name\", \"Unknown Basin\")\n            ),\n            basin_area=basin_data.get(\"basin_area\", 0.0),\n            main_river_length=basin_data.get(\"main_river_length\"),\n            location=basin_data.get(\"location\"),\n            description=basin_data.get(\"description\"),\n        )\n\n        # Extract modeling configuration\n        modeling_approach = basin_data.get(\"modeling_approach\", \"lumped\")\n\n        # Handle additional configuration (excluding basin info and modeling approach)\n        config = {\n            k: v\n            for k, v in basin_data.items()\n            if k\n            not in [\n                \"basin_id\",\n                \"basin_code\",\n                \"basin_name\",\n                \"name\",\n                \"basin_area\",\n                \"main_river_length\",\n                \"location\",\n                \"description\",\n                \"modeling_approach\",\n            ]\n        }\n\n        return cls(\n            basin_info=basin_info,\n            modeling_approach=modeling_approach,\n            **config,\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of Basin.\"\"\"\n        return (\n            f\"Basin(id='{self.basin_id}', name='{self.basin_name}', \"\n            f\"basin_area={self.basin_area}km\u00b2, \"\n            f\"main_river_length={self.main_river_length}km, \"\n            f\"approach='{self.modeling_approach}')\"\n        )\n</code></pre>"},{"location":"hydromodel/#hydromodel.Basin.basin_area","title":"<code>basin_area</code>  <code>property</code>","text":"<p>Get total basin area in km\u00b2.</p>"},{"location":"hydromodel/#hydromodel.Basin.basin_id","title":"<code>basin_id</code>  <code>property</code>","text":"<p>Get basin ID.</p>"},{"location":"hydromodel/#hydromodel.Basin.basin_name","title":"<code>basin_name</code>  <code>property</code>","text":"<p>Get basin name.</p>"},{"location":"hydromodel/#hydromodel.Basin.main_river_length","title":"<code>main_river_length</code>  <code>property</code>","text":"<p>Get main river length in km.</p>"},{"location":"hydromodel/#hydromodel.Basin.__init__","title":"<code>__init__(basin_info, modeling_approach='lumped', **kwargs)</code>","text":"<p>Initialize basin configuration.</p>"},{"location":"hydromodel/#hydromodel.Basin.__init__--parameters","title":"Parameters","text":"<p>basin_info : BasinInfo or Dict[str, Any]     Basic basin information modeling_approach : str, default \"lumped\"     Modeling approach: \"lumped\" or \"semi_distributed\" **kwargs     Additional configuration parameters</p> Source code in <code>hydromodel/trainers/basin.py</code> <pre><code>def __init__(\n    self,\n    basin_info: Union[BasinInfo, Dict[str, Any]],\n    modeling_approach: str = \"lumped\",\n    **kwargs,\n):\n    \"\"\"\n    Initialize basin configuration.\n\n    Parameters\n    ----------\n    basin_info : BasinInfo or Dict[str, Any]\n        Basic basin information\n    modeling_approach : str, default \"lumped\"\n        Modeling approach: \"lumped\" or \"semi_distributed\"\n    **kwargs\n        Additional configuration parameters\n    \"\"\"\n    # Handle basin_info input\n    if isinstance(basin_info, dict):\n        self.basin_info = BasinInfo(**basin_info)\n    else:\n        self.basin_info = basin_info\n\n    # Modeling configuration\n    self.modeling_approach = modeling_approach\n\n    # Additional configuration\n    self.config = kwargs\n\n    # Initialize approach-specific configurations\n    self._setup_modeling_approach()\n</code></pre>"},{"location":"hydromodel/#hydromodel.Basin.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of Basin.</p> Source code in <code>hydromodel/trainers/basin.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of Basin.\"\"\"\n    return (\n        f\"Basin(id='{self.basin_id}', name='{self.basin_name}', \"\n        f\"basin_area={self.basin_area}km\u00b2, \"\n        f\"main_river_length={self.main_river_length}km, \"\n        f\"approach='{self.modeling_approach}')\"\n    )\n</code></pre>"},{"location":"hydromodel/#hydromodel.Basin.from_config","title":"<code>from_config(basin_data)</code>  <code>classmethod</code>","text":"<p>Create Basin instance from configuration data.</p> <p>This is a flexible factory method that can handle various data formats and field name variations commonly used in basin configuration.</p>"},{"location":"hydromodel/#hydromodel.Basin.from_config--parameters","title":"Parameters","text":"<p>basin_data : Dict[str, Any]     Basin configuration data, supporting various field name formats:     - basin_id/basin_code, basin_name/name, basin_area     - output_unit, time_step_hours, modeling_approach     - geographical and hydrological attributes</p>"},{"location":"hydromodel/#hydromodel.Basin.from_config--returns","title":"Returns","text":"<p>Basin     Configured Basin instance</p> Source code in <code>hydromodel/trainers/basin.py</code> <pre><code>@classmethod\ndef from_config(cls, basin_data: Dict[str, Any]) -&gt; \"Basin\":\n    \"\"\"\n    Create Basin instance from configuration data.\n\n    This is a flexible factory method that can handle various data formats\n    and field name variations commonly used in basin configuration.\n\n    Parameters\n    ----------\n    basin_data : Dict[str, Any]\n        Basin configuration data, supporting various field name formats:\n        - basin_id/basin_code, basin_name/name, basin_area\n        - output_unit, time_step_hours, modeling_approach\n        - geographical and hydrological attributes\n\n    Returns\n    -------\n    Basin\n        Configured Basin instance\n    \"\"\"\n    # Extract basic basin info\n    basin_info = BasinInfo(\n        basin_id=basin_data.get(\n            \"basin_id\", basin_data.get(\"basin_code\", \"unknown\")\n        ),\n        basin_name=basin_data.get(\n            \"basin_name\", basin_data.get(\"name\", \"Unknown Basin\")\n        ),\n        basin_area=basin_data.get(\"basin_area\", 0.0),\n        main_river_length=basin_data.get(\"main_river_length\"),\n        location=basin_data.get(\"location\"),\n        description=basin_data.get(\"description\"),\n    )\n\n    # Extract modeling configuration\n    modeling_approach = basin_data.get(\"modeling_approach\", \"lumped\")\n\n    # Handle additional configuration (excluding basin info and modeling approach)\n    config = {\n        k: v\n        for k, v in basin_data.items()\n        if k\n        not in [\n            \"basin_id\",\n            \"basin_code\",\n            \"basin_name\",\n            \"name\",\n            \"basin_area\",\n            \"main_river_length\",\n            \"location\",\n            \"description\",\n            \"modeling_approach\",\n        ]\n    }\n\n    return cls(\n        basin_info=basin_info,\n        modeling_approach=modeling_approach,\n        **config,\n    )\n</code></pre>"},{"location":"hydromodel/#hydromodel.Basin.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create Basin instance from dictionary.</p> Source code in <code>hydromodel/trainers/basin.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Basin\":\n    \"\"\"Create Basin instance from dictionary.\"\"\"\n    basin_info = BasinInfo(**data[\"basin_info\"])\n\n    return cls(\n        basin_info=basin_info,\n        modeling_approach=data.get(\"modeling_approach\", \"lumped\"),\n        **data.get(\"config\", {}),\n    )\n</code></pre>"},{"location":"hydromodel/#hydromodel.Basin.is_lumped","title":"<code>is_lumped()</code>","text":"<p>Check if this is a lumped model configuration.</p> Source code in <code>hydromodel/trainers/basin.py</code> <pre><code>def is_lumped(self) -&gt; bool:\n    \"\"\"Check if this is a lumped model configuration.\"\"\"\n    return self.modeling_approach == \"lumped\"\n</code></pre>"},{"location":"hydromodel/#hydromodel.Basin.is_semi_distributed","title":"<code>is_semi_distributed()</code>","text":"<p>Check if this is a semi-distributed model configuration.</p> Source code in <code>hydromodel/trainers/basin.py</code> <pre><code>def is_semi_distributed(self) -&gt; bool:\n    \"\"\"Check if this is a semi-distributed model configuration.\"\"\"\n    return self.modeling_approach == \"semi_distributed\"\n</code></pre>"},{"location":"hydromodel/#hydromodel.Basin.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert basin configuration to dictionary.</p> Source code in <code>hydromodel/trainers/basin.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert basin configuration to dictionary.\"\"\"\n    return {\n        \"basin_info\": self.basin_info.to_dict(),\n        \"modeling_approach\": self.modeling_approach,\n        \"n_units\": self.n_units,\n        \"unit_areas\": self.unit_areas.tolist(),\n        \"unit_ids\": self.unit_ids,\n        \"config\": self.config,\n    }\n</code></pre>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator","title":"<code>UnifiedSimulator</code>","text":"<p>Unified simulator for all hydrological models.</p> <p>This class provides a single interface for running simulations with any model type. The key design principle is separation of concerns: - Model configuration is handled during initialization - Input data is provided as arguments to simulate() method - This allows one simulator instance to run multiple different datasets</p> <p>Key features: - Single interface for all models (XAJ, GR series, unit hydrograph, etc.) - Flexible data input: simulate(inputs, **kwargs) - One-time model setup, multiple simulations - Direct parameter specification - Consistent output format across all models - Support for both single and multi-basin simulations</p> Source code in <code>hydromodel/trainers/unified_simulate.py</code> <pre><code>class UnifiedSimulator:\n    \"\"\"\n    Unified simulator for all hydrological models.\n\n    This class provides a single interface for running simulations with any model type.\n    The key design principle is separation of concerns:\n    - Model configuration is handled during initialization\n    - Input data is provided as arguments to simulate() method\n    - This allows one simulator instance to run multiple different datasets\n\n    Key features:\n    - Single interface for all models (XAJ, GR series, unit hydrograph, etc.)\n    - Flexible data input: simulate(inputs, **kwargs)\n    - One-time model setup, multiple simulations\n    - Direct parameter specification\n    - Consistent output format across all models\n    - Support for both single and multi-basin simulations\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: Dict[str, Any],\n        basin_config: Optional[Union[Basin, Dict[str, Any]]] = None,\n    ):\n        \"\"\"\n        Initialize the unified simulator with model configuration and basin information.\n        NOTE: Now we only support single basin simulation.\n\n        Parameters\n        ----------\n        model_config : Dict[str, Any]\n            Model configuration containing:\n            - model_name: Name of the model to use\n            - model_params: Model-specific parameters (structure, etc.)\n            - parameters: Specific parameter values for simulation\n        basin_config : Basin or Dict[str, Any], optional\n            Basin configuration for unit conversion and modeling approach.\n            If dict provided, it will be converted to Basin instance.\n            If None, unit conversion will be disabled.\n        \"\"\"\n        self.model_config = model_config\n\n        # Extract model information\n        self.model_type = self.model_config.get(\"type\", \"lumped\")\n        self.model_name = self.model_config[\"model_name\"]\n        self.model_params = self.model_config.get(\"model_params\", {})\n        self.parameters = OrderedDict(self.model_config.get(\"parameters\", {}))\n\n        # Store basin configuration\n        if basin_config is not None:\n            if isinstance(basin_config, dict):\n                self.basin = Basin.from_config(basin_config)\n            else:\n                self.basin = basin_config\n        else:\n            self.basin = None\n            # Validata model exists\n        if self.model_name not in MODEL_DICT:\n            raise ValueError(\n                f\"Model '{self.model_name}' not found in MODEL_DICT\"\n            )\n\n        # Get model function\n        self.model_function = MODEL_DICT[self.model_name]\n\n        # Setup parameter handling (convert dict to array format)\n        self._setup_parameters()\n\n    def _setup_parameters(self):\n        \"\"\"\n        Setup model parameters for simulation.\n        Convert parameter dictionary to array format expected by models\n        \"\"\"\n        if not self.parameters:\n            raise ValueError(\n                f\"Model '{self.model_name}' requires parameters to be specified\"\n            )\n        # Convert parameter dictionary to list format\n        param_names = list(self.parameters.keys())\n        param_values = list(self.parameters.values())\n\n        # Store parameter info for later use when we know number of basins\n        self.param_names = param_names\n        self.param_values = np.expand_dims(param_values, axis=0)\n\n    def update_parameters(self, new_parameters: Dict[str, Any]):\n        \"\"\"\n        Update model parameters without reinitializing the entire simulator.\n        This is useful for calibration where parameters change frequently.\n\n        Parameters\n        ----------\n        new_parameters : Dict[str, Any]\n            New parameter values as key-value pairs\n        \"\"\"\n        self.parameters = OrderedDict(new_parameters)\n        # Update internal parameter arrays\n        self.param_names = list(self.parameters.keys())\n        self.param_values = np.expand_dims(\n            list(self.parameters.values()), axis=0\n        )\n\n    def simulate(\n        self,\n        inputs: np.ndarray,\n        qobs: Optional[np.ndarray] = None,\n        warmup_length: int = 365,\n        is_event_data: bool = False,\n        return_intermediate: bool = True,\n        return_warmup_states: bool = False,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Run model simulation with provided input data.\n\n        Parameters\n        ----------\n        inputs : np.ndarray\n            Input data array with shape [time, basin, features]\n            Features typically include [precipitation, potential_evapotranspiration]\n        qobs : np.ndarray, optional\n            Observed streamflow data with shape [time, basin, 1]\n        warmup_length : int, default 365\n            Length of warmup period (time steps)\n        is_event_data : bool, default False\n            Whether input data represents event-based data\n        return_intermediate : bool, default True\n            Whether to return intermediate results from model computation\n        return_warmup_states : bool, default False\n            Whether to return initial states after warmup period.\n            Returns warmup states in simulation result dict\n        **kwargs\n            Additional arguments passed to the model function.\n            Can include 'initial_states': Dict[str, Any] - Dictionary of initial\n            state values to override after warmup. For DHF model, keys can\n            include: \"sa0\", \"ua0\", \"ya0\"\n\n        Returns\n        -------\n        Dict[str, Any]\n            Dictionary containing simulation results:\n            - simulation: Model simulation output [time, basin, 1]\n            - observation: Observed data (if provided) [time, basin, 1]\n            - input_data: Input data used for simulation [time, basin, n_features]\n            - intermediate: Intermediate results (if return_intermediate=True)\n            - metadata: Simulation metadata and configuration\n        \"\"\"\n        # Validate inputs\n        if not isinstance(inputs, np.ndarray):\n            inputs = np.array(inputs)\n\n        if inputs.ndim != 3:\n            raise ValueError(\n                f\"Input data must be 3D array [time, basin, features], got shape {inputs.shape}\"\n            )\n\n        # Handle different simulation scenarios\n        if is_event_data:\n            # Event data with traditional models\n            simulation_result = self._simulate_event_data(\n                inputs,\n                warmup_length,\n                return_intermediate,\n                return_warmup_states,\n                **kwargs,\n            )\n        else:\n            # Standard simulation\n            simulation_result = self._simulate_continuous_data(\n                inputs,\n                warmup_length,\n                return_intermediate,\n                return_warmup_states,\n                **kwargs,\n            )\n\n        return simulation_result\n\n    def _process_model_result(\n        self,\n        model_result,\n        output_names,\n        return_warmup_states,\n    ):\n        \"\"\"\n        Process model result into dictionary format.\n\n        Parameters\n        ----------\n        model_result : tuple or np.ndarray\n            Raw model output\n        output_names : list\n            List of output variable names\n        return_warmup_states : bool\n            Whether warmup states should be included\n\n        Returns\n        -------\n        dict\n            Dictionary containing processed model results\n        \"\"\"\n        if isinstance(model_result, tuple):\n            # Check if last element is warmup_states dict\n            if (\n                return_warmup_states\n                and len(model_result) &gt; len(output_names)\n                and isinstance(model_result[-1], dict)\n            ):\n                # Extract warmup states and process remaining results\n                warmup_states = model_result[-1]\n                model_arrays = model_result[:-1]\n                result_dict = {\n                    name: arr for name, arr in zip(output_names, model_arrays)\n                }\n                result_dict[\"warmup_states\"] = warmup_states\n            else:\n                # Traditional case without warmup states\n                result_dict = {\n                    name: arr for name, arr in zip(output_names, model_result)\n                }\n        else:\n            # Handle single array or tuple with warmup_states\n            if return_warmup_states and isinstance(model_result, tuple):\n                # Single array + warmup_states case\n                result_dict = {output_names[0]: model_result[0]}\n                result_dict[\"warmup_states\"] = model_result[1]\n            else:\n                # Unit hydrograph models return single array\n                result_dict = {output_names[0]: model_result}\n\n        return result_dict\n\n    def trans_sim_results_unit(\n        self,\n        simulation,\n        output_unit=\"m^3/s\",\n        time_interval_hours=3.0,\n    ):\n        \"\"\"\n        Apply unit conversion to simulation results using basin configuration.\n\n        Parameters\n        ----------\n        simulation : np.ndarray\n            Simulation results\n        output_unit : str, default \"m^3/s\"\n            Target output unit for conversion\n        time_interval_hours : float, default 3.0\n            Time step in hours for the data\n\n        Returns\n        -------\n        tuple\n            tuple of (converted_simulation, unitconv_metadata)\n        \"\"\"\n        if self.basin is not None and output_unit == \"m^3/s\":\n            from hydroutils.hydro_units import streamflow_unit_conv\n\n            # Get simulation results\n            if simulation is not None:\n                # Detect time interval from time series or use provided time step\n                # Convert time_step_hours to integer format for time_interval\n                if time_interval_hours.is_integer():\n                    time_interval = f\"{int(time_interval_hours)}h\"\n                else:\n                    # Handle fractional hours by converting to minutes if &lt; 1 hour\n                    if time_interval_hours &lt; 1:\n                        time_interval_minutes = int(time_interval_hours * 60)\n                        time_interval = f\"{time_interval_minutes}m\"\n                    else:\n                        # Round to nearest hour for other cases\n                        time_interval = f\"{round(time_interval_hours)}h\"\n\n                # Convert simulation results from mm/time to m\u00b3/s\n                # simulation shape is [time, basin, 1]\n                converted_simulation = np.zeros_like(simulation)\n\n                for basin_idx in range(simulation.shape[1]):\n                    # TODO: Only support 1 basin for now\n                    basin_simulation = simulation[\n                        :, basin_idx, 0\n                    ]  # Extract time series for this basin\n\n                    # Get basin area for this unit (supports semi-distributed)\n                    basin_area_km2 = self.basin.unit_areas\n\n                    converted_discharge = streamflow_unit_conv(\n                        data=basin_simulation,\n                        area=basin_area_km2,\n                        target_unit=output_unit,\n                        source_unit=f\"mm/{time_interval}\",\n                        area_unit=\"km^2\",\n                    )\n\n                    converted_simulation[:, basin_idx, 0] = converted_discharge\n\n                    # Add conversion metadata\n                    unitconv_metadata = {\n                        \"applied\": True,\n                        \"source_unit\": f\"mm/{time_interval}\",\n                        \"target_unit\": output_unit,\n                    }\n\n        else:\n            # No unit conversion applied\n            reason = \"No basin configuration provided\"\n            if self.basin is not None:\n                reason = (\n                    f\"Output unit '{output_unit}' does not require conversion\"\n                )\n            unitconv_metadata = {\n                \"applied\": False,\n                \"reason\": reason,\n                \"source_unit\": None,\n                \"target_unit\": output_unit,\n            }\n        return converted_simulation, unitconv_metadata\n\n    def _simulate_continuous_data(\n        self,\n        inputs: np.ndarray,\n        warmup_length: int,\n        return_intermediate: bool,\n        return_warmup_states: bool,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Standard simulation for continuous data.\"\"\"\n        # Prepare model configuration\n        model_config = dict(self.model_params)\n        model_config.update(kwargs)\n\n        # Add basin_area to model_config if available and not already provided\n        # This is needed for models like xaj_slw and dhf that require basin_area\n        if \"basin_area\" not in model_config and self.basin is not None:\n            if hasattr(self.basin, \"basin_area\"):\n                model_config[\"basin_area\"] = self.basin.basin_area\n\n        # Run model simulation\n        model_result = self.model_function(\n            inputs,\n            self.param_values,\n            warmup_length=warmup_length,\n            return_state=return_intermediate,\n            return_warmup_states=return_warmup_states,\n            **model_config,\n        )\n\n        # Convert model_result to dictionary based on model output names\n        output_names = get_model_output_names(\n            self.model_name, return_intermediate\n        )\n\n        result_dict = self._process_model_result(\n            model_result, output_names, return_warmup_states\n        )\n\n        return result_dict\n\n    def _simulate_event_data(\n        self,\n        inputs: np.ndarray,\n        warmup_length: int,\n        return_intermediate: bool,\n        return_warmup_states: bool,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Special simulation for event data with traditional models.\"\"\"\n        # Validate that flood_event markers are present\n        if inputs.shape[2] &lt; 3:\n            raise ValueError(\n                \"Event data simulation requires flood_event markers. \"\n                f\"Expected input shape [time, basin, 3+] with features [rain, pet, flood_event, (event_id)], \"\n                f\"but got shape {inputs.shape}.\"\n            )\n\n        # Initialize output array and warmup states storage\n        outputs = []\n        event_warmup_states = None\n\n        # Process each basin separately\n        for basin_idx in range(inputs.shape[1]):\n            # Find event segments using flood_event markers (including warmup period)\n            # Use index 2 for marker (3rd feature), not -1, since we may have event_id as 4th feature\n            flood_event_array = inputs[\n                :, basin_idx, 2\n            ]  # feature index 2 is flood_event marker\n            event_segments = find_flood_event_segments_as_tuples(\n                flood_event_array, warmup_length\n            )\n\n            basin_params = self.param_values\n\n            # Process each event segment\n            for j, (\n                extended_start,\n                extended_end,\n                original_start,\n                original_end,\n            ) in enumerate(event_segments):\n                # Extract event data (including warmup period)\n                # Only extract first 3 features [prcp, pet, marker], exclude event_id\n                event_inputs = inputs[\n                    extended_start : extended_end + 1,\n                    basin_idx : basin_idx + 1,\n                    :3,\n                ]\n                # Run model on this event segment\n                model_config = dict(self.model_params)\n                model_config.update(kwargs)\n\n                # Add basin_area to model_config if available and not already provided\n                # This is needed for models like xaj_slw and dhf that require basin_area\n                if \"basin_area\" not in model_config and self.basin is not None:\n                    if hasattr(self.basin, \"basin_area\"):\n                        model_config[\"basin_area\"] = self.basin.basin_area\n\n                event_result = self.model_function(\n                    event_inputs,\n                    basin_params,\n                    warmup_length=warmup_length,\n                    return_state=return_intermediate,\n                    return_warmup_states=return_warmup_states,\n                    **model_config,\n                )\n\n                # Convert event_result tuple to dictionary based on model output names\n                output_names = get_model_output_names(\n                    self.model_name, return_intermediate\n                )\n\n                event_dict = self._process_model_result(\n                    event_result, output_names, return_warmup_states\n                )\n\n                # Store warmup states for later use (only from first event)\n                if j == 0 and \"warmup_states\" in event_dict:\n                    event_warmup_states = event_dict[\"warmup_states\"]\n\n                if j == 0:\n                    # Initialize output dictionaries for each variable (excluding warmup_states)\n                    simulation_output = {}\n                    for name, arr in event_dict.items():\n                        if (\n                            name != \"warmup_states\"\n                        ):  # Skip warmup_states as it's not a time series\n                            simulation_output[name] = np.zeros(\n                                (inputs.shape[0], inputs.shape[1], 1)\n                            )\n\n                # Save the event result to its location in long time series data (excluding warmup_states)\n                for name, arr in event_dict.items():\n                    if name != \"warmup_states\":\n                        simulation_output[name][\n                            original_start : original_end + 1,\n                            basin_idx : basin_idx + 1,\n                            :,\n                        ] = arr\n\n            outputs.append(simulation_output)\n\n        # Combine outputs from all basins into final output dictionary\n        final_output = {}\n        for var_name in outputs[0].keys():\n            basin_arrays = [output[var_name] for output in outputs]\n            final_output[var_name] = np.concatenate(basin_arrays, axis=1)\n\n        # Add warmup states if requested and available\n        if return_warmup_states and event_warmup_states is not None:\n            final_output[\"warmup_states\"] = event_warmup_states\n\n        return final_output\n</code></pre>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.__init__","title":"<code>__init__(model_config, basin_config=None)</code>","text":"<p>Initialize the unified simulator with model configuration and basin information. NOTE: Now we only support single basin simulation.</p>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.__init__--parameters","title":"Parameters","text":"<p>model_config : Dict[str, Any]     Model configuration containing:     - model_name: Name of the model to use     - model_params: Model-specific parameters (structure, etc.)     - parameters: Specific parameter values for simulation basin_config : Basin or Dict[str, Any], optional     Basin configuration for unit conversion and modeling approach.     If dict provided, it will be converted to Basin instance.     If None, unit conversion will be disabled.</p> Source code in <code>hydromodel/trainers/unified_simulate.py</code> <pre><code>def __init__(\n    self,\n    model_config: Dict[str, Any],\n    basin_config: Optional[Union[Basin, Dict[str, Any]]] = None,\n):\n    \"\"\"\n    Initialize the unified simulator with model configuration and basin information.\n    NOTE: Now we only support single basin simulation.\n\n    Parameters\n    ----------\n    model_config : Dict[str, Any]\n        Model configuration containing:\n        - model_name: Name of the model to use\n        - model_params: Model-specific parameters (structure, etc.)\n        - parameters: Specific parameter values for simulation\n    basin_config : Basin or Dict[str, Any], optional\n        Basin configuration for unit conversion and modeling approach.\n        If dict provided, it will be converted to Basin instance.\n        If None, unit conversion will be disabled.\n    \"\"\"\n    self.model_config = model_config\n\n    # Extract model information\n    self.model_type = self.model_config.get(\"type\", \"lumped\")\n    self.model_name = self.model_config[\"model_name\"]\n    self.model_params = self.model_config.get(\"model_params\", {})\n    self.parameters = OrderedDict(self.model_config.get(\"parameters\", {}))\n\n    # Store basin configuration\n    if basin_config is not None:\n        if isinstance(basin_config, dict):\n            self.basin = Basin.from_config(basin_config)\n        else:\n            self.basin = basin_config\n    else:\n        self.basin = None\n        # Validata model exists\n    if self.model_name not in MODEL_DICT:\n        raise ValueError(\n            f\"Model '{self.model_name}' not found in MODEL_DICT\"\n        )\n\n    # Get model function\n    self.model_function = MODEL_DICT[self.model_name]\n\n    # Setup parameter handling (convert dict to array format)\n    self._setup_parameters()\n</code></pre>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.simulate","title":"<code>simulate(inputs, qobs=None, warmup_length=365, is_event_data=False, return_intermediate=True, return_warmup_states=False, **kwargs)</code>","text":"<p>Run model simulation with provided input data.</p>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.simulate--parameters","title":"Parameters","text":"<p>inputs : np.ndarray     Input data array with shape [time, basin, features]     Features typically include [precipitation, potential_evapotranspiration] qobs : np.ndarray, optional     Observed streamflow data with shape [time, basin, 1] warmup_length : int, default 365     Length of warmup period (time steps) is_event_data : bool, default False     Whether input data represents event-based data return_intermediate : bool, default True     Whether to return intermediate results from model computation return_warmup_states : bool, default False     Whether to return initial states after warmup period.     Returns warmup states in simulation result dict **kwargs     Additional arguments passed to the model function.     Can include 'initial_states': Dict[str, Any] - Dictionary of initial     state values to override after warmup. For DHF model, keys can     include: \"sa0\", \"ua0\", \"ya0\"</p>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.simulate--returns","title":"Returns","text":"<p>Dict[str, Any]     Dictionary containing simulation results:     - simulation: Model simulation output [time, basin, 1]     - observation: Observed data (if provided) [time, basin, 1]     - input_data: Input data used for simulation [time, basin, n_features]     - intermediate: Intermediate results (if return_intermediate=True)     - metadata: Simulation metadata and configuration</p> Source code in <code>hydromodel/trainers/unified_simulate.py</code> <pre><code>def simulate(\n    self,\n    inputs: np.ndarray,\n    qobs: Optional[np.ndarray] = None,\n    warmup_length: int = 365,\n    is_event_data: bool = False,\n    return_intermediate: bool = True,\n    return_warmup_states: bool = False,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Run model simulation with provided input data.\n\n    Parameters\n    ----------\n    inputs : np.ndarray\n        Input data array with shape [time, basin, features]\n        Features typically include [precipitation, potential_evapotranspiration]\n    qobs : np.ndarray, optional\n        Observed streamflow data with shape [time, basin, 1]\n    warmup_length : int, default 365\n        Length of warmup period (time steps)\n    is_event_data : bool, default False\n        Whether input data represents event-based data\n    return_intermediate : bool, default True\n        Whether to return intermediate results from model computation\n    return_warmup_states : bool, default False\n        Whether to return initial states after warmup period.\n        Returns warmup states in simulation result dict\n    **kwargs\n        Additional arguments passed to the model function.\n        Can include 'initial_states': Dict[str, Any] - Dictionary of initial\n        state values to override after warmup. For DHF model, keys can\n        include: \"sa0\", \"ua0\", \"ya0\"\n\n    Returns\n    -------\n    Dict[str, Any]\n        Dictionary containing simulation results:\n        - simulation: Model simulation output [time, basin, 1]\n        - observation: Observed data (if provided) [time, basin, 1]\n        - input_data: Input data used for simulation [time, basin, n_features]\n        - intermediate: Intermediate results (if return_intermediate=True)\n        - metadata: Simulation metadata and configuration\n    \"\"\"\n    # Validate inputs\n    if not isinstance(inputs, np.ndarray):\n        inputs = np.array(inputs)\n\n    if inputs.ndim != 3:\n        raise ValueError(\n            f\"Input data must be 3D array [time, basin, features], got shape {inputs.shape}\"\n        )\n\n    # Handle different simulation scenarios\n    if is_event_data:\n        # Event data with traditional models\n        simulation_result = self._simulate_event_data(\n            inputs,\n            warmup_length,\n            return_intermediate,\n            return_warmup_states,\n            **kwargs,\n        )\n    else:\n        # Standard simulation\n        simulation_result = self._simulate_continuous_data(\n            inputs,\n            warmup_length,\n            return_intermediate,\n            return_warmup_states,\n            **kwargs,\n        )\n\n    return simulation_result\n</code></pre>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.trans_sim_results_unit","title":"<code>trans_sim_results_unit(simulation, output_unit='m^3/s', time_interval_hours=3.0)</code>","text":"<p>Apply unit conversion to simulation results using basin configuration.</p>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.trans_sim_results_unit--parameters","title":"Parameters","text":"<p>simulation : np.ndarray     Simulation results output_unit : str, default \"m^3/s\"     Target output unit for conversion time_interval_hours : float, default 3.0     Time step in hours for the data</p>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.trans_sim_results_unit--returns","title":"Returns","text":"<p>tuple     tuple of (converted_simulation, unitconv_metadata)</p> Source code in <code>hydromodel/trainers/unified_simulate.py</code> <pre><code>def trans_sim_results_unit(\n    self,\n    simulation,\n    output_unit=\"m^3/s\",\n    time_interval_hours=3.0,\n):\n    \"\"\"\n    Apply unit conversion to simulation results using basin configuration.\n\n    Parameters\n    ----------\n    simulation : np.ndarray\n        Simulation results\n    output_unit : str, default \"m^3/s\"\n        Target output unit for conversion\n    time_interval_hours : float, default 3.0\n        Time step in hours for the data\n\n    Returns\n    -------\n    tuple\n        tuple of (converted_simulation, unitconv_metadata)\n    \"\"\"\n    if self.basin is not None and output_unit == \"m^3/s\":\n        from hydroutils.hydro_units import streamflow_unit_conv\n\n        # Get simulation results\n        if simulation is not None:\n            # Detect time interval from time series or use provided time step\n            # Convert time_step_hours to integer format for time_interval\n            if time_interval_hours.is_integer():\n                time_interval = f\"{int(time_interval_hours)}h\"\n            else:\n                # Handle fractional hours by converting to minutes if &lt; 1 hour\n                if time_interval_hours &lt; 1:\n                    time_interval_minutes = int(time_interval_hours * 60)\n                    time_interval = f\"{time_interval_minutes}m\"\n                else:\n                    # Round to nearest hour for other cases\n                    time_interval = f\"{round(time_interval_hours)}h\"\n\n            # Convert simulation results from mm/time to m\u00b3/s\n            # simulation shape is [time, basin, 1]\n            converted_simulation = np.zeros_like(simulation)\n\n            for basin_idx in range(simulation.shape[1]):\n                # TODO: Only support 1 basin for now\n                basin_simulation = simulation[\n                    :, basin_idx, 0\n                ]  # Extract time series for this basin\n\n                # Get basin area for this unit (supports semi-distributed)\n                basin_area_km2 = self.basin.unit_areas\n\n                converted_discharge = streamflow_unit_conv(\n                    data=basin_simulation,\n                    area=basin_area_km2,\n                    target_unit=output_unit,\n                    source_unit=f\"mm/{time_interval}\",\n                    area_unit=\"km^2\",\n                )\n\n                converted_simulation[:, basin_idx, 0] = converted_discharge\n\n                # Add conversion metadata\n                unitconv_metadata = {\n                    \"applied\": True,\n                    \"source_unit\": f\"mm/{time_interval}\",\n                    \"target_unit\": output_unit,\n                }\n\n    else:\n        # No unit conversion applied\n        reason = \"No basin configuration provided\"\n        if self.basin is not None:\n            reason = (\n                f\"Output unit '{output_unit}' does not require conversion\"\n            )\n        unitconv_metadata = {\n            \"applied\": False,\n            \"reason\": reason,\n            \"source_unit\": None,\n            \"target_unit\": output_unit,\n        }\n    return converted_simulation, unitconv_metadata\n</code></pre>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.update_parameters","title":"<code>update_parameters(new_parameters)</code>","text":"<p>Update model parameters without reinitializing the entire simulator. This is useful for calibration where parameters change frequently.</p>"},{"location":"hydromodel/#hydromodel.UnifiedSimulator.update_parameters--parameters","title":"Parameters","text":"<p>new_parameters : Dict[str, Any]     New parameter values as key-value pairs</p> Source code in <code>hydromodel/trainers/unified_simulate.py</code> <pre><code>def update_parameters(self, new_parameters: Dict[str, Any]):\n    \"\"\"\n    Update model parameters without reinitializing the entire simulator.\n    This is useful for calibration where parameters change frequently.\n\n    Parameters\n    ----------\n    new_parameters : Dict[str, Any]\n        New parameter values as key-value pairs\n    \"\"\"\n    self.parameters = OrderedDict(new_parameters)\n    # Update internal parameter arrays\n    self.param_names = list(self.parameters.keys())\n    self.param_values = np.expand_dims(\n        list(self.parameters.values()), axis=0\n    )\n</code></pre>"},{"location":"hydromodel/#hydromodel.calibrate","title":"<code>calibrate(config, **kwargs)</code>","text":"<p>Unified calibration interface for all hydrological models.</p>"},{"location":"hydromodel/#hydromodel.calibrate--parameters","title":"Parameters","text":"<p>config : Dict     Configuration dictionary containing all settings.     Must contain 'data_cfgs', 'model_cfgs', 'training_cfgs' keys     Optional in training_cfgs: 'save_config' (bool, default: True) **kwargs     Additional arguments</p>"},{"location":"hydromodel/#hydromodel.calibrate--returns","title":"Returns","text":"<p>Dict[str, Any]     Dictionary containing calibration results</p> Source code in <code>hydromodel/trainers/unified_calibrate.py</code> <pre><code>def calibrate(config, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Unified calibration interface for all hydrological models.\n\n    Parameters\n    ----------\n    config : Dict\n        Configuration dictionary containing all settings.\n        Must contain 'data_cfgs', 'model_cfgs', 'training_cfgs' keys\n        Optional in training_cfgs: 'save_config' (bool, default: True)\n    **kwargs\n        Additional arguments\n\n    Returns\n    -------\n    Dict[str, Any]\n        Dictionary containing calibration results\n    \"\"\"\n\n    # Validate config structure\n    if not isinstance(config, dict):\n        raise ValueError(\"Config must be a dictionary\")\n\n    if (\n        \"data_cfgs\" not in config\n        or \"model_cfgs\" not in config\n        or \"training_cfgs\" not in config\n    ):\n        raise ValueError(\n            \"Config dictionary must contain 'data_cfgs', 'model_cfgs', and 'training_cfgs' keys\"\n        )\n\n    data_config = config[\"data_cfgs\"]\n    # Extract model config\n    model_cfgs = config[\"model_cfgs\"]\n    model_config = {\n        \"name\": model_cfgs.get(\"model_name\"),\n        **model_cfgs.get(\"model_params\", {}),\n    }\n    training_config = config[\"training_cfgs\"]\n\n    # Extract components from training_config\n    algorithm_config = {\n        \"name\": training_config.get(\"algorithm_name\", \"SCE_UA\"),\n        **training_config.get(\"algorithm_params\", {}),\n    }\n    loss_config = training_config.get(\n        \"loss_config\", {\"type\": \"time_series\", \"obj_func\": \"RMSE\"}\n    )\n\n    # Create output directory\n    output_dir = os.path.join(\n        training_config.get(\"output_dir\", \"results\"),\n        training_config.get(\"experiment_name\", \"experiment\"),\n    )\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create unified model setup\n    model_setup = UnifiedCalibrator(\n        data_config=data_config,\n        model_config=model_config,\n        loss_config=loss_config,\n        training_config=training_config,\n        **kwargs,\n    )\n\n    results = {}\n    # Get basin IDs from data loader or config\n    if (\n        hasattr(model_setup.data_loader, \"basin_ids\")\n        and model_setup.data_loader.basin_ids\n    ):\n        basin_ids = model_setup.data_loader.basin_ids\n    elif \"basin_ids\" in data_config:\n        basin_ids = data_config[\"basin_ids\"]\n    elif model_setup.p_and_e is not None:\n        # Fallback to p_and_e shape\n        basin_ids = [f\"basin_{i}\" for i in range(model_setup.p_and_e.shape[1])]\n    else:\n        raise ValueError(\n            \"Cannot determine basin_ids: not found in config, data_loader, or p_and_e\"\n        )\n\n    # For multi-basin calibration, we can either:\n    # 1. Calibrate all basins together (TODO - future implementation)\n    # 2. Calibrate each basin separately (current implementation)\n\n    # Currently using approach 2 - calibrate each basin separately\n    # This is the default approach that hydromodel supports\n\n    total_basins = len(basin_ids)\n    print(f\"\\n\ud83d\ude80 {'='*60}\")\n    print(f\"\ud83d\ude80 Starting calibration for {total_basins} basin(s)\")\n    print(f\"\ud83d\ude80 {'='*60}\\n\")\n\n    for i, basin_id in enumerate(basin_ids):\n        print(f\"\u25b6\ufe0f  {'='*60}\")\n        print(f\"\u25b6\ufe0f  Basin {i+1}/{total_basins}: {basin_id}\")\n        print(f\"\u25b6\ufe0f  {'='*60}\")\n\n        basin_result = _calibrate_model(\n            model_setup,\n            algorithm_config,\n            output_dir,\n            basin_id,\n            basin_index=i,\n            **kwargs,\n        )\n        results[basin_id] = basin_result\n\n        print(f\"\u2705 Basin {i+1}/{total_basins} completed: {basin_id}\")\n\n    # Save calibration results to JSON file for evaluation\n    print(f\"\\n Saving calibration results...\")\n    results_file = os.path.join(output_dir, \"calibration_results.json\")\n    with open(results_file, \"w\") as f:\n        json.dump(results, f, indent=2)\n    print(f\" Results saved to: {results_file}\")\n\n    # Save configuration files if requested (default: True)\n    save_config = training_config.get(\"save_config\", True)\n    if save_config:\n        _save_calibration_config(\n            config, output_dir, model_setup.model_name, model_setup.param_range\n        )\n\n    print(f\"\\n {'='*60}\")\n    print(f\" All calibrations completed successfully!\")\n    print(f\" Total basins calibrated: {total_basins}\")\n    print(f\" {'='*60}\")\n\n    return results\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers all methods to install hydromodel on different platforms.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>Operating System: Windows, macOS, or Linux</li> <li>Disk Space: ~500 MB for package + data storage for datasets</li> </ul>"},{"location":"installation/#quick-installation","title":"Quick Installation","text":"<p>The fastest way to install hydromodel:</p> <pre><code>pip install hydromodel\n</code></pre> <p>This installs the latest stable release from PyPI.</p>"},{"location":"installation/#recommended-installation-methods","title":"Recommended Installation Methods","text":""},{"location":"installation/#method-1-using-pip-standard","title":"Method 1: Using pip (Standard)","text":"<p>For most users, pip is the recommended installation method:</p> <pre><code># Create a virtual environment (recommended)\npython -m venv hydromodel-env\n\n# Activate virtual environment\n# On Windows:\nhydromodel-env\\Scripts\\activate\n# On macOS/Linux:\nsource hydromodel-env/bin/activate\n\n# Install hydromodel\npip install hydromodel\n\n# Install hydrodataset for data access\npip install hydrodataset\n</code></pre>"},{"location":"installation/#method-2-using-uv-faster","title":"Method 2: Using uv (Faster)","text":"<p>uv is a faster package manager:</p> <pre><code># Install uv\npip install uv\n\n# Create virtual environment\nuv venv\n\n# Activate virtual environment\n# On Windows:\n.venv\\Scripts\\activate\n# On macOS/Linux:\nsource .venv/bin/activate\n\n# Install packages\nuv pip install hydromodel hydrodataset\n</code></pre> <p>Installation with uv is typically 10-100x faster than pip.</p>"},{"location":"installation/#method-3-using-conda","title":"Method 3: Using conda","text":"<p>If you use conda/mamba:</p> <pre><code># Create environment\nconda create -n hydromodel python=3.11\n\n# Activate environment\nconda activate hydromodel\n\n# Install from conda-forge (if available)\nconda install -c conda-forge hydromodel\n\n# Or use pip within conda\npip install hydromodel hydrodataset\n</code></pre>"},{"location":"installation/#installation-from-source","title":"Installation from Source","text":"<p>For developers or to get the latest development version:</p>"},{"location":"installation/#option-a-direct-from-github","title":"Option A: Direct from GitHub","text":"<pre><code>pip install git+https://github.com/OuyangWenyu/hydromodel.git\n</code></pre>"},{"location":"installation/#option-b-clone-and-install","title":"Option B: Clone and Install","text":"<pre><code># Clone repository\ngit clone https://github.com/OuyangWenyu/hydromodel.git\ncd hydromodel\n\n# Install in editable mode (for development)\npip install -e .\n\n# Or install with all development dependencies\npip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"installation/#option-c-using-uv-recommended-for-developers","title":"Option C: Using uv (Recommended for Developers)","text":"<pre><code># Clone repository\ngit clone https://github.com/OuyangWenyu/hydromodel.git\ncd hydromodel\n\n# Create environment and install dependencies\nuv sync --all-extras\n\n# Activate environment\n# On Windows:\n.venv\\Scripts\\activate\n# On macOS/Linux:\nsource .venv/bin/activate\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"installation/#for-data-access","title":"For Data Access","text":"<pre><code># CAMELS and other public datasets\npip install hydrodataset\n</code></pre>"},{"location":"installation/#for-visualization","title":"For Visualization","text":"<pre><code># Plotting and analysis\npip install matplotlib seaborn plotly\n</code></pre>"},{"location":"installation/#for-development","title":"For Development","text":"<pre><code># Testing, linting, documentation\npip install pytest black flake8 mkdocs mkdocs-material\n</code></pre>"},{"location":"installation/#all-optional-dependencies","title":"All Optional Dependencies","text":"<pre><code># Install everything\npip install \"hydromodel[all]\"\n</code></pre>"},{"location":"installation/#platform-specific-instructions","title":"Platform-Specific Instructions","text":""},{"location":"installation/#windows","title":"Windows","text":"<ol> <li>Install Python:</li> <li>Download from python.org</li> <li> <p>Check \"Add Python to PATH\" during installation</p> </li> <li> <p>Install hydromodel:    <pre><code>pip install hydromodel hydrodataset\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>python -c \"import hydromodel; print(hydromodel.__version__)\"\n</code></pre></p> </li> </ol>"},{"location":"installation/#macos","title":"macOS","text":"<ol> <li> <p>Install Python (using Homebrew):    <pre><code>brew install python@3.11\n</code></pre></p> </li> <li> <p>Install hydromodel:    <pre><code>pip3 install hydromodel hydrodataset\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>python3 -c \"import hydromodel; print(hydromodel.__version__)\"\n</code></pre></p> </li> </ol>"},{"location":"installation/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<ol> <li> <p>Install Python:    <pre><code>sudo apt update\nsudo apt install python3 python3-pip python3-venv\n</code></pre></p> </li> <li> <p>Install hydromodel:    <pre><code>pip3 install hydromodel hydrodataset\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>python3 -c \"import hydromodel; print(hydromodel.__version__)\"\n</code></pre></p> </li> </ol>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify everything works:</p> <pre><code># Test import\nimport hydromodel\nimport hydrodataset\n\n# Check versions\nprint(f\"hydromodel version: {hydromodel.__version__}\")\n\n# Test basic functionality\nfrom hydromodel.models.model_factory import model_factory\n\n# Create a model\nmodel = model_factory(model_name=\"xaj_mz\")\nprint(f\"\u2713 Successfully created {model.__class__.__name__}\")\nprint(f\"\u2713 Model has {model.param_limits.shape[0]} parameters\")\nprint(\"\u2713 Installation verified!\")\n</code></pre> <p>Expected output: <pre><code>hydromodel version: 0.1.0\n\u2713 Successfully created XajMz\n\u2713 Model has 15 parameters\n\u2713 Installation verified!\n</code></pre></p>"},{"location":"installation/#configuration-setup","title":"Configuration Setup","text":"<p>After installation, configure data paths:</p>"},{"location":"installation/#step-1-create-configuration-file","title":"Step 1: Create Configuration File","text":"<p>Create <code>hydro_setting.yml</code> in your home directory:</p> <p>Windows: <code>C:\\Users\\YourUsername\\hydro_setting.yml</code> macOS/Linux: <code>~/hydro_setting.yml</code></p> <pre><code>local_data_path:\n  root: '/path/to/data'\n  datasets-origin: '/path/to/data/datasets'\n  cache: '/path/to/data/.cache'\n</code></pre>"},{"location":"installation/#step-2-verify-configuration","title":"Step 2: Verify Configuration","text":"<pre><code>from hydromodel import SETTING\n\nprint(\"Configuration loaded:\")\nprint(f\"Root: {SETTING.get('local_data_path', {}).get('root')}\")\nprint(f\"Datasets: {SETTING.get('local_data_path', {}).get('datasets-origin')}\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":""},{"location":"installation/#issue-1-no-module-named-hydromodel","title":"Issue 1: \"No module named 'hydromodel'\"","text":"<p>Solution: Make sure the virtual environment is activated: <pre><code># Check Python path\nwhich python  # macOS/Linux\nwhere python  # Windows\n\n# Should point to virtual environment, not system Python\n</code></pre></p>"},{"location":"installation/#issue-2-permission-denied-during-installation","title":"Issue 2: \"Permission denied\" during installation","text":"<p>Solution: Use <code>--user</code> flag or virtual environment: <pre><code>pip install --user hydromodel\n</code></pre></p>"},{"location":"installation/#issue-3-dependency-conflicts","title":"Issue 3: Dependency conflicts","text":"<p>Solution: Use a fresh virtual environment: <pre><code>python -m venv fresh-env\nsource fresh-env/bin/activate  # or fresh-env\\Scripts\\activate on Windows\npip install hydromodel\n</code></pre></p>"},{"location":"installation/#issue-4-slow-pip-installation","title":"Issue 4: Slow pip installation","text":"<p>Solution: Use uv for faster installation: <pre><code>pip install uv\nuv pip install hydromodel\n</code></pre></p>"},{"location":"installation/#issue-5-microsoft-visual-c-required-windows","title":"Issue 5: \"Microsoft Visual C++ required\" (Windows)","text":"<p>Solution: Install Visual C++ Build Tools: - Download from visualstudio.microsoft.com - Or install via Anaconda which includes pre-compiled packages</p>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check documentation: Browse docs</li> <li>Search issues: GitHub Issues</li> <li>Ask questions: Open a new issue with:</li> <li>Your OS and Python version</li> <li>Full error message</li> <li>Installation command used</li> </ol>"},{"location":"installation/#updating-hydromodel","title":"Updating hydromodel","text":""},{"location":"installation/#update-to-latest-stable-version","title":"Update to Latest Stable Version","text":"<pre><code>pip install --upgrade hydromodel\n</code></pre>"},{"location":"installation/#update-to-development-version","title":"Update to Development Version","text":"<pre><code>pip install --upgrade git+https://github.com/OuyangWenyu/hydromodel.git\n</code></pre>"},{"location":"installation/#check-current-version","title":"Check Current Version","text":"<pre><code>import hydromodel\nprint(hydromodel.__version__)\n</code></pre>"},{"location":"installation/#uninstallation","title":"Uninstallation","text":"<p>To remove hydromodel:</p> <pre><code>pip uninstall hydromodel\n</code></pre> <p>To remove everything including dependencies:</p> <pre><code># List installed packages\npip list | grep hydro\n\n# Uninstall\npip uninstall hydromodel hydrodataset\n</code></pre> <p>To remove the virtual environment:</p> <pre><code># Deactivate first\ndeactivate\n\n# Remove directory\nrm -rf hydromodel-env  # Linux/macOS\nrmdir /s hydromodel-env  # Windows\n</code></pre>"},{"location":"installation/#docker-installation-advanced","title":"Docker Installation (Advanced)","text":"<p>For reproducible environments:</p> <pre><code># Dockerfile\nFROM python:3.11-slim\n\n# Install dependencies\nRUN pip install --no-cache-dir hydromodel hydrodataset\n\n# Set working directory\nWORKDIR /app\n\n# Copy your code\nCOPY . .\n\n# Run your script\nCMD [\"python\", \"your_script.py\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t hydromodel-app .\ndocker run -v $(pwd)/data:/app/data hydromodel-app\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Quick Start: Follow the Quick Start Guide</li> <li>Configuration: Set up data paths and settings</li> <li>Tutorial: Try the usage examples</li> <li>API Documentation: Browse the API reference</li> </ol>"},{"location":"installation/#support","title":"Support","text":"<ul> <li>Documentation: https://OuyangWenyu.github.io/hydromodel</li> <li>Issues: https://github.com/OuyangWenyu/hydromodel/issues</li> <li>Discussions: https://github.com/OuyangWenyu/hydromodel/discussions</li> </ul>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>Document Purpose: This guide is designed for end users who want to quickly start using hydromodel for hydrological modeling. For developers who need detailed understanding of the code architecture, please refer to Usage Guide.</p> <p>Get started with hydromodel in 5 minutes! This guide walks you through a complete workflow using command-line scripts - no complex coding required.</p>"},{"location":"quickstart/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>\u2705 Install hydromodel and prepare data</li> <li>\u2705 Calibrate a hydrological model</li> <li>\u2705 Evaluate model performance</li> <li>\u2705 Run simulations with custom parameters</li> <li>\u2705 Visualize results</li> </ul>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Basic command-line knowledge</li> <li>Understanding of hydrological modeling concepts</li> </ul>"},{"location":"quickstart/#step-1-installation-2-minutes","title":"Step 1: Installation (2 minutes)","text":"<p>Install hydromodel with data support:</p> <pre><code>pip install hydromodel hydrodataset\n</code></pre> <p>Or using <code>uv</code> (faster):</p> <pre><code>uv pip install hydromodel hydrodataset\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import hydromodel; print(hydromodel.__version__)\"\n</code></pre>"},{"location":"quickstart/#step-2-get-the-scripts-1-minute","title":"Step 2: Get the Scripts (1 minute)","text":"<p>Clone the repository to access example scripts and configs:</p> <pre><code>git clone https://github.com/OuyangWenyu/hydromodel.git\ncd hydromodel\n</code></pre> <p>The key files you'll use: <pre><code>hydromodel/\n\u251c\u2500\u2500 scripts/                         # Command-line scripts\n\u2502   \u251c\u2500\u2500 run_xaj_calibration.py      # Calibrate models\n\u2502   \u251c\u2500\u2500 run_xaj_evaluate.py         # Evaluate performance\n\u2502   \u251c\u2500\u2500 run_xaj_simulate.py         # Run simulations\n\u2502   \u2514\u2500\u2500 visualize.py                 # Visualize results\n\u2514\u2500\u2500 configs/                         # Configuration files\n    \u251c\u2500\u2500 example_config.yaml          # Example calibration config\n    \u251c\u2500\u2500 example_simulate_config.yaml # Example simulation config\n    \u2514\u2500\u2500 example_xaj_params.yaml      # Example parameters\n</code></pre></p>"},{"location":"quickstart/#step-3-prepare-data-5-30-minutes","title":"Step 3: Prepare Data (5-30 minutes)","text":""},{"location":"quickstart/#option-a-use-camels-public-data-recommended-for-first-try","title":"Option A: Use CAMELS Public Data (Recommended for First Try)","text":"<p>The data downloads automatically on first use:</p> <pre><code># Check available basins\nfrom hydrodataset.camels_us import CamelsUs\nfrom hydrodataset import SETTING\n\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsUs(data_path, download=True)\nbasin_ids = ds.read_object_ids()\n\nprint(f\"Available basins: {len(basin_ids)}\")\nprint(f\"Example IDs: {basin_ids[:5]}\")\n</code></pre> <p>First download may take 30-120 minutes for the complete CAMELS dataset (~70GB). See Data Guide for details.</p>"},{"location":"quickstart/#option-b-use-your-own-data","title":"Option B: Use Your Own Data","text":"<p>See Data Guide - Custom Data Section for preparing your own basin data.</p>"},{"location":"quickstart/#step-4-configure-your-experiment-2-minutes","title":"Step 4: Configure Your Experiment (2 minutes)","text":"<p>Edit <code>configs/example_config.yaml</code>:</p> <pre><code># Configuration for model calibration and evaluation\ndata_cfgs:\n  data_source_type: \"camels_us\"                 # Dataset type\n  basin_ids: [\"01013500\"]                       # Basin(s) to calibrate\n  train_period: [\"1990-10-01\", \"2000-09-30\"]   # Calibration period\n  test_period: [\"2000-10-01\", \"2010-09-30\"]    # Evaluation period\n  warmup_length: 365                            # Warmup days\n  variables: [\"precipitation\", \"potential_evapotranspiration\", \"streamflow\"]\n\nmodel_cfgs:\n  model_name: \"xaj_mz\"                          # XAJ model with Muskingum routing\n  model_params:\n    source_type: \"sources\"\n    source_book: \"HF\"\n\ntraining_cfgs:\n  algorithm: \"SCE_UA\"                           # Algorithm: SCE_UA, GA, or scipy\n\n  # SCE-UA (Shuffled Complex Evolution) - Recommended for global optimization\n  SCE_UA:\n    rep: 1000                                   # Iterations (5000+ for production)\n    ngs: 1000                                   # Number of complexes\n    kstop: 500                                  # Stop if no improvement\n    peps: 0.1                                   # Parameter convergence\n    pcento: 0.1                                 # Percentage change allowed\n    random_seed: 1234\n\n  # GA (Genetic Algorithm) - Flexible and customizable\n  GA:\n    pop_size: 80                                # Population size\n    n_generations: 50                           # Generations (100+ for production)\n    cx_prob: 0.7                                # Crossover probability\n    mut_prob: 0.2                               # Mutation probability\n    random_seed: 1234\n\n  # scipy - Fast gradient-based optimization\n  scipy:\n    method: \"SLSQP\"                             # L-BFGS-B, SLSQP, TNC, etc.\n    max_iterations: 500                         # Maximum iterations\n\n  loss: \"RMSE\"                                  # Loss function: RMSE, NSE, KGE\n  output_dir: \"results\"\n  experiment_name: \"quickstart_exp\"\n\nevaluation_cfgs:\n  metrics: [\"NSE\", \"KGE\", \"RMSE\", \"PBIAS\"]\n</code></pre> <p>Quick Tips: - Start with one basin for first\u8bd5\u9a8c - Choose an algorithm:   - <code>SCE_UA</code>: Most robust, good for complex problems (slower)   - <code>GA</code>: Flexible, good balance of speed and accuracy   - <code>scipy</code>: Fastest, good for smooth objective functions - For quick testing: reduce iterations   - SCE_UA: <code>rep: 1000, ngs: 100</code>   - GA: <code>pop_size: 50, n_generations: 30</code>   - scipy: <code>max_iterations: 200</code> - For production: use higher values   - SCE_UA: <code>rep: 10000, ngs: 1000</code>   - GA: <code>pop_size: 100, n_generations: 200</code>   - scipy: <code>max_iterations: 1000</code></p>"},{"location":"quickstart/#step-5-run-calibration-5-30-minutes","title":"Step 5: Run Calibration (5-30 minutes)","text":"<pre><code>python scripts/run_xaj_calibration.py --config configs/example_config.yaml\n</code></pre> <p>You'll see output like:</p> <pre><code>================================================================================\nXAJ Model Calibration using UnifiedCalibrateor\n================================================================================\n\n[1/3] Loading configuration...\n  Dataset: camels_us\n  Basins: ['01013500']\n  Model: xaj_mz\n  Algorithm: SCE_UA\n  Training period: 1990-10-01 to 2000-09-30\n\n[2/3] Loading data...\n  Input shape: (3653, 1, 2)\n  Qobs shape: (3653, 1, 1)\n\n[3/3] Starting calibration...\n  Basin 01013500: SCE-UA optimization\n  Progress: [====================] 100% Complete\n  Best RMSE: 0.2534\n\n\u2713 Calibration complete! Results saved to: results/quickstart_exp/\n</code></pre> <p>Results location: <pre><code>results/quickstart_exp/\n\u251c\u2500\u2500 calibration_results.json         # \u2b50 Best parameters (unified format, used by evaluation)\n\u251c\u2500\u2500 01013500_sceua.csv              # SCE-UA iteration history (if using SCE_UA)\n\u251c\u2500\u2500 01013500_ga.csv                 # GA generation history (if using GA)\n\u251c\u2500\u2500 01013500_scipy.csv              # scipy iteration history (if using scipy)\n\u251c\u2500\u2500 calibration_config.yaml          # Config used (for reproducibility)\n\u2514\u2500\u2500 param_range.yaml                 # Parameter ranges (optional, for denormalization)\n</code></pre></p> <p>Understanding the results: - calibration_results.json: Contains best parameters for all basins, works with all algorithms - Algorithm-specific CSV: Detailed iteration/generation history with all parameter values - param_range.yaml: Defines parameter physical ranges (not calibration results)</p>"},{"location":"quickstart/#step-6-evaluate-model-performance-1-minute","title":"Step 6: Evaluate Model Performance (1 minute)","text":"<p>Evaluate on the test period:</p> <pre><code>python scripts/run_xaj_evaluate.py \\\n    --calibration-dir results/quickstart_exp \\\n    --eval-period test\n</code></pre> <p>Output:</p> <pre><code>================================================================================\nXAJ Model Evaluation\n================================================================================\n\n[1/3] Loading calibrated parameters...\n  Found parameters for 1 basin(s)\n\n[2/3] Running evaluation on test period...\n  Basin 01013500: Simulating...\n  \u2713 Simulation complete\n\n[3/3] Calculating metrics...\n\n================================================================================\nEvaluation Results (Test Period: 2000-10-01 to 2010-09-30)\n================================================================================\nBasin: 01013500\n  NSE:   0.756\n  KGE:   0.721\n  RMSE:  0.312\n  PBIAS: -5.23 %\n\n\u2713 Results saved to: results/quickstart_exp/evaluation_test/\n</code></pre> <p>Results include: <pre><code>results/quickstart_exp/evaluation_test/\n\u251c\u2500\u2500 basins_metrics.csv                  # Performance metrics\n\u251c\u2500\u2500 basins_denorm_params.csv            # Calibrated parameters\n\u2514\u2500\u2500 xaj_mz_evaluation_results.nc        # Full simulation results (NetCDF)\n</code></pre></p>"},{"location":"quickstart/#step-7-run-custom-simulation-1-minute","title":"Step 7: Run Custom Simulation (1 minute)","text":"<p>Important: Simulation does NOT require calibration! You can run simulations with any parameters.</p>"},{"location":"quickstart/#option-a-use-calibrated-parameters","title":"Option A: Use Calibrated Parameters","text":"<pre><code>python scripts/run_xaj_simulate.py \\\n    --param-file results/quickstart_exp/01013500_sceua.csv \\\n    --plot\n</code></pre>"},{"location":"quickstart/#option-b-use-custom-parameters","title":"Option B: Use Custom Parameters","text":"<p>Edit <code>configs/example_xaj_params.yaml</code>:</p> <pre><code># XAJ model parameters\nK: 0.75\nB: 0.25\nIM: 0.06\nUM: 18.0\nLM: 80.0\nDM: 95.0\nC: 0.18\nSM: 120.0\nEX: 1.5\nKI: 0.35\nKG: 0.45\nA: 0.85\nTHETA: 0.012\nCI: 0.85\nCG: 0.95\n</code></pre> <p>Then run:</p> <pre><code>python scripts/run_xaj_simulate.py \\\n    --config configs/example_simulate_config.yaml \\\n    --param-file configs/example_xaj_params.yaml \\\n    --output simulation_results.csv \\\n    --plot\n</code></pre> <p>Output:</p> <pre><code>================================================================================\nXAJ Model Simulation using UnifiedSimulator\n================================================================================\n\n[1/4] Loading configuration from: configs/example_simulate_config.yaml\n  Model: xaj_mz\n  Basin: 01013500 (index 0)\n  Period: ['2000-10-01', '2010-09-30']\n\n[2/4] Loading parameters from: configs/example_xaj_params.yaml\n  Parameters:\n    K        = 0.750000\n    B        = 0.250000\n    IM       = 0.060000\n    ...\n\n[3/4] Loading data and initializing simulator\n  Input shape: (3653, 1, 2)\n  Qobs shape: (3653, 1, 1)\n  \u2713 UnifiedSimulator initialized\n\n[4/4] Running simulation (warmup=365 days)\n  \u2713 Simulation completed (3288 time steps)\n\n================================================================================\nSimulation Results\n================================================================================\nBasin: 01013500\nTime steps: 3288\n\nPerformance Metrics:\n  NSE        =   0.7234\n  KGE        =   0.6912\n  RMSE       =   0.3456\n\n\u2713 Results saved to: simulation_results.csv\n</code></pre>"},{"location":"quickstart/#step-8-visualize-results-1-minute","title":"Step 8: Visualize Results (1 minute)","text":"<pre><code>python scripts/visualize.py --eval-dir results/quickstart_exp/evaluation_test\n</code></pre> <p>This creates plots showing: - Observed vs. simulated streamflow - Flow duration curves - Monthly aggregated comparison - Residual analysis</p> <p>Output files: <pre><code>results/quickstart_exp/evaluation_test/\n\u251c\u2500\u2500 01013500_timeseries.png          # Time series plot\n\u251c\u2500\u2500 01013500_flow_duration.png       # FDC plot\n\u2514\u2500\u2500 01013500_monthly.png             # Monthly comparison\n</code></pre></p>"},{"location":"quickstart/#common-workflows","title":"Common Workflows","text":""},{"location":"quickstart/#workflow-1-calibration-evaluation-visualization","title":"Workflow 1: Calibration \u2192 Evaluation \u2192 Visualization","text":"<pre><code># 1. Calibrate\npython scripts/run_xaj_calibration.py --config configs/example_config.yaml\n\n# 2. Evaluate\npython scripts/run_xaj_evaluate.py \\\n    --calibration-dir results/quickstart_exp \\\n    --eval-period test\n\n# 3. Visualize\npython scripts/visualize.py --eval-dir results/quickstart_exp/evaluation_test\n</code></pre>"},{"location":"quickstart/#workflow-2-simulation-only-no-calibration","title":"Workflow 2: Simulation Only (No Calibration)","text":"<pre><code># Use custom parameters directly\npython scripts/run_xaj_simulate.py \\\n    --config configs/example_simulate_config.yaml \\\n    --param-file configs/example_xaj_params.yaml \\\n    --plot\n</code></pre>"},{"location":"quickstart/#workflow-3-multi-basin-calibration","title":"Workflow 3: Multi-Basin Calibration","text":"<p>Edit config to include multiple basins:</p> <pre><code>data_cfgs:\n  basin_ids: [\"01013500\", \"01022500\", \"01030500\"]  # Multiple basins\n</code></pre> <p>Then run:</p> <pre><code>python scripts/run_xaj_calibration.py --config configs/multi_basin_config.yaml\n</code></pre>"},{"location":"quickstart/#tips-for-success","title":"Tips for Success","text":""},{"location":"quickstart/#1-start-small","title":"1. Start Small","text":"<ul> <li>Use one basin initially</li> <li>Use shorter periods for testing</li> <li>Use fewer iterations (<code>rep: 1000, ngs: 20</code>)</li> </ul>"},{"location":"quickstart/#2-check-data-quality","title":"2. Check Data Quality","text":"<pre><code># Verify your data before calibration\nfrom hydrodataset.camels_us import CamelsUs\n\nds = CamelsUs(data_path)\ndata = ds.read_timeseries(\n    gage_id_lst=[\"01013500\"],\n    t_range=[\"1990-10-01\", \"2000-09-30\"],\n    var_lst=[\"precipitation\", \"streamflow\"]\n)\n\nprint(f\"Data shape: {data.shape}\")\nprint(f\"Missing values: {data.isna().sum()}\")\n</code></pre>"},{"location":"quickstart/#3-monitor-progress","title":"3. Monitor Progress","text":"<ul> <li>Calibration saves checkpoints every N iterations</li> <li>Check partial results in <code>results/</code> directory</li> <li>Use <code>--verbose</code> flag for detailed output</li> </ul>"},{"location":"quickstart/#4-production-settings","title":"4. Production Settings","text":"<p>For real research, use longer calibration:</p> <pre><code>training_cfgs:\n  algorithm_params:\n    rep: 10000        # More iterations\n    ngs: 100          # More complexes\n    kstop: 50         # Stricter convergence\n</code></pre>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quickstart/#issue-1-data-not-found-error","title":"Issue 1: \"Data not found\" Error","text":"<p>Solution: Check data path and basin IDs</p> <pre><code>from hydrodataset import SETTING\nfrom hydrodataset.camels_us import CamelsUs\n\n# Check data path\nprint(SETTING[\"local_data_path\"][\"datasets-origin\"])\n\n# Check basin IDs\nds = CamelsUs(SETTING[\"local_data_path\"][\"datasets-origin\"])\nbasin_ids = ds.read_object_ids()\nprint(f\"Available basins: {len(basin_ids)}\")\n</code></pre>"},{"location":"quickstart/#issue-2-slow-calibration","title":"Issue 2: Slow Calibration","text":"<p>Solutions: - Use fewer basins initially - Reduce <code>rep</code> and <code>ngs</code> for testing - Use xaj_mz (faster than full XAJ) - Consider using GA or scipy algorithms</p>"},{"location":"quickstart/#issue-3-poor-model-performance","title":"Issue 3: Poor Model Performance","text":"<p>Check: - Data quality (missing values, outliers) - Training period length (need \u22655 years) - Warmup period (use 365 days minimum) - Parameter ranges (default ranges may not suit all basins)</p>"},{"location":"quickstart/#issue-4-memory-issues","title":"Issue 4: Memory Issues","text":"<p>Solutions: - Process fewer basins at once - Reduce time period length - Use data caching (see Data Guide)</p>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":""},{"location":"quickstart/#1-explore-advanced-features","title":"1. Explore Advanced Features","text":"<ul> <li> <p>Different Datasets: Try CAMELS-GB, CAMELS-AUS, etc.   <pre><code>data_cfgs:\n  data_source_type: \"camels_gb\"\n  basin_ids: [\"28015\"]\n</code></pre></p> </li> <li> <p>Different Algorithms: Try GA or scipy   <pre><code>training_cfgs:\n  algorithm_name: \"GA\"\n</code></pre></p> </li> <li> <p>Custom Periods: Evaluate on different periods   <pre><code>python scripts/run_xaj_evaluate.py \\\n    --calibration-dir results/quickstart_exp \\\n    --eval-period custom \\\n    --custom-period 2010-01-01 2015-12-31\n</code></pre></p> </li> </ul>"},{"location":"quickstart/#2-use-your-own-data","title":"2. Use Your Own Data","text":"<p>Follow Data Guide to prepare custom basin data.</p>"},{"location":"quickstart/#3-compare-algorithms","title":"3. Compare Algorithms","text":"<p>Try different algorithms to see which works best for your case:</p> <pre><code># Method 1: Edit algorithm in config\n# In configs/example_config.yaml, change:\n#   algorithm: \"SCE_UA\"  # to \"GA\" or \"scipy\"\n\n# Method 2: Run comparisons\npython scripts/run_xaj_calibration.py --config configs/example_config_sceua.yaml\npython scripts/run_xaj_calibration.py --config configs/example_config_ga.yaml\npython scripts/run_xaj_calibration.py --config configs/example_config_scipy.yaml\n</code></pre> <p>Algorithm comparison:</p> Algorithm Speed Robustness Memory Best For SCE-UA Slow \u2b50\u2b50\u2b50\u2b50\u2b50 High Complex landscapes, global optimum GA Medium \u2b50\u2b50\u2b50\u2b50 Medium Flexible, good balance scipy \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Low Smooth objectives, quick tests <p>Convergence analysis:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load iteration history\ndf_ga = pd.read_csv(\"results/xaj_GA/01013500_ga.csv\")\ndf_scipy = pd.read_csv(\"results/xaj_scipy/01013500_scipy.csv\")\n\n# Plot convergence\nplt.figure(figsize=(10, 6))\nplt.plot(df_ga[\"generation\"], df_ga[\"objective_value\"], label=\"GA\")\nplt.plot(df_scipy[\"iteration\"], df_scipy[\"objective_value\"], label=\"scipy\")\nplt.xlabel(\"Iteration/Generation\")\nplt.ylabel(\"Objective Value (RMSE)\")\nplt.legend()\nplt.title(\"Convergence Comparison\")\nplt.show()\n</code></pre>"},{"location":"quickstart/#4-learn-code-architecture","title":"4. Learn Code Architecture","text":"<p>For deeper understanding, read Usage Guide - the developer documentation.</p>"},{"location":"quickstart/#5-api-usage","title":"5. API Usage","text":"<p>Transition from scripts to Python API for more flexibility:</p> <pre><code>from hydromodel.trainers.unified_calibrate import calibrate\nfrom hydromodel.trainers.unified_evaluate import evaluate\n\n# Load configuration\nconfig = {...}\n\n# Run calibration\nresults = calibrate(config)\n\n# Run evaluation\neval_results = evaluate(config, param_dir=\"results/exp\", eval_period=\"test\")\n</code></pre> <p>See Usage Guide for API details.</p>"},{"location":"quickstart/#summary","title":"Summary","text":"<p>Congratulations! You've learned how to:</p> <ul> <li>\u2705 Install hydromodel and prepare data</li> <li>\u2705 Configure experiments using YAML files</li> <li>\u2705 Calibrate models using command-line scripts</li> <li>\u2705 Evaluate model performance on test periods</li> <li>\u2705 Run simulations with custom parameters</li> <li>\u2705 Visualize results</li> </ul> <p>Key Commands:</p> <pre><code># Calibration\npython scripts/run_xaj_calibration.py --config configs/example_config.yaml\n\n# Evaluation\npython scripts/run_xaj_evaluate.py --calibration-dir results/exp --eval-period test\n\n# Simulation\npython scripts/run_xaj_simulate.py --param-file results/exp/basin_sceua.csv --plot\n\n# Visualization\npython scripts/visualize.py --eval-dir results/exp/evaluation_test\n</code></pre>"},{"location":"quickstart/#additional-resources","title":"Additional Resources","text":"<ul> <li>Usage Guide: usage.md - Developer documentation with code architecture details</li> <li>Data Guide: data_guide.md - Comprehensive data preparation guide</li> <li>FAQ: faq.md - Common questions and solutions</li> <li>GitHub: https://github.com/OuyangWenyu/hydromodel</li> <li>Issues: https://github.com/OuyangWenyu/hydromodel/issues</li> </ul> <p>Happy modeling! \ud83c\udf0a</p>"},{"location":"usage/","title":"Usage Guide","text":"<p>Document Purpose: This guide is designed for developers who need detailed understanding of hydromodel's code architecture, API design, and internal workflows. For end users who want to quickly start using hydromodel, please refer to Quick Start Guide.</p> <p>This guide demonstrates how to use hydromodel's unified API architecture for hydrological modeling, calibration, evaluation, and simulation. It provides comprehensive documentation on the codebase structure and design principles.</p>"},{"location":"usage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Architecture</li> <li>Data Loading</li> <li>Model Calibration</li> <li>Model Evaluation</li> <li>Model Simulation</li> <li>Results Visualization</li> <li>Flood Event Data</li> <li>Configuration System</li> <li>Advanced Topics</li> </ul>"},{"location":"usage/#overview","title":"Overview","text":""},{"location":"usage/#unified-api-design","title":"Unified API Design","text":"<p>hydromodel provides a completely unified interface for all hydrological models:</p> <pre><code>Data \u2192 UnifiedDataLoader \u2192 UnifiedCalibrator \u2192 UnifiedEvaluator\n                                \u2193\n                           UnifiedSimulator\n</code></pre> <p>Core Design Principles:</p> <ol> <li>Unified Interfaces: All models (XAJ, GR series, etc.) use the same API</li> <li>Configuration-Based: YAML configs for reproducibility</li> <li>Decoupled Components: Calibration, evaluation, and simulation are independent</li> <li>Flexible Integration: Works with CAMELS datasets and custom data</li> </ol>"},{"location":"usage/#key-components","title":"Key Components","text":"Component Purpose Module <code>UnifiedDataLoader</code> Load and preprocess data <code>datasets.unified_data_loader</code> <code>calibrate()</code> Model calibration <code>trainers.unified_calibrate</code> <code>evaluate()</code> Model evaluation <code>trainers.unified_evaluate</code> <code>UnifiedSimulator</code> Direct model simulation <code>trainers.unified_simulate</code> <code>MODEL_DICT</code> Model registry <code>models.model_dict</code>"},{"location":"usage/#architecture","title":"Architecture","text":""},{"location":"usage/#code-structure","title":"Code Structure","text":"<pre><code>hydromodel/\n\u251c\u2500\u2500 models/                         # Model implementations\n\u2502   \u251c\u2500\u2500 xaj.py                     # XAJ model core\n\u2502   \u251c\u2500\u2500 xaj_mz.py                  # XAJ with Muskingum routing\n\u2502   \u251c\u2500\u2500 gr4j.py                    # GR4J model\n\u2502   \u251c\u2500\u2500 model_dict.py              # Model registry (MODEL_DICT)\n\u2502   \u2514\u2500\u2500 model_config.py            # Parameter ranges and configs\n\u2502\n\u251c\u2500\u2500 trainers/                       # Calibration, evaluation, simulation\n\u2502   \u251c\u2500\u2500 unified_calibrate.py       # Unified calibration interface\n\u2502   \u251c\u2500\u2500 unified_evaluate.py        # Unified evaluation interface\n\u2502   \u251c\u2500\u2500 unified_simulate.py        # Unified simulation interface\n\u2502   \u251c\u2500\u2500 calibrate_sceua.py         # SCE-UA algorithm\n\u2502   \u2514\u2500\u2500 calibrate_ga.py            # Genetic algorithm\n\u2502\n\u251c\u2500\u2500 datasets/                       # Data loading and preprocessing\n\u2502   \u251c\u2500\u2500 unified_data_loader.py     # Unified data loading interface\n\u2502   \u2514\u2500\u2500 data_preprocess.py         # Data preprocessing utilities\n\u2502\n\u2514\u2500\u2500 scripts/                        # Command-line interfaces\n    \u251c\u2500\u2500 run_xaj_calibration.py     # Calibration script\n    \u251c\u2500\u2500 run_xaj_evaluate.py        # Evaluation script\n    \u2514\u2500\u2500 run_xaj_simulate.py        # Simulation script\n</code></pre>"},{"location":"usage/#data-flow","title":"Data Flow","text":"<pre><code>1. Configuration (YAML)\n   \u2193\n2. UnifiedDataLoader\n   \u2193 (load time series, attributes)\n3. Data Tensors [time, basin, features]\n   \u2193\n4. UnifiedCalibrator/UnifiedSimulator\n   \u2193 (run model with parameters)\n5. Results [time, basin, outputs]\n   \u2193\n6. UnifiedEvaluator\n   \u2193 (calculate metrics)\n7. Performance Metrics + NetCDF outputs\n</code></pre>"},{"location":"usage/#data-loading","title":"Data Loading","text":""},{"location":"usage/#unifieddataloader","title":"UnifiedDataLoader","text":"<p><code>UnifiedDataLoader</code> provides a consistent interface for loading data from multiple sources:</p> <pre><code>from hydromodel.datasets.unified_data_loader import UnifiedDataLoader\n\n# Configuration\ndata_config = {\n    \"data_source_type\": \"camels_us\",  # or \"selfmadehydrodataset\"\n    \"basin_ids\": [\"01013500\"],\n    \"test_period\": [\"2000-10-01\", \"2010-09-30\"],\n    \"warmup_length\": 365,\n    \"variables\": [\"precipitation\", \"potential_evapotranspiration\", \"streamflow\"]\n}\n\n# Load data\ndata_loader = UnifiedDataLoader(data_config, is_train_val_test=\"test\")\np_and_e, qobs = data_loader.load_data()\n\n# Data shapes\nprint(f\"Inputs: {p_and_e.shape}\")   # [time, basins, 2] (prcp + pet)\nprint(f\"Qobs: {qobs.shape}\")         # [time, basins, 1]\n\n# Get basin attributes\nbasin_configs = data_loader.get_basin_configs()\nbasin_area = basin_configs[\"01013500\"][\"area\"]  # km\u00b2\n</code></pre>"},{"location":"usage/#data-format","title":"Data Format","text":"<p>All data is returned in standardized tensors:</p> <ul> <li>Shape: <code>[time_steps, num_basins, num_features]</code></li> <li>Type: <code>numpy.ndarray</code> (float32 or float64)</li> <li>Order: Time-major for efficiency</li> </ul> <p>Example: <pre><code># p_and_e shape: [3653, 2, 2]\n# - 3653 time steps\n# - 2 basins\n# - 2 features (precipitation, PET)\n\n# Access basin 0, all times, precipitation:\nprcp_basin0 = p_and_e[:, 0, 0]\n\n# Access all basins, time 100, PET:\npet_t100 = p_and_e[100, :, 1]\n</code></pre></p>"},{"location":"usage/#supported-data-sources","title":"Supported Data Sources","text":"Data Source <code>data_source_type</code> Package CAMELS-US <code>camels_us</code> hydrodataset CAMELS-GB <code>camels_gb</code> hydrodataset CAMELS-AUS <code>camels_aus</code> hydrodataset ... ... hydrodataset Custom Data <code>selfmadehydrodataset</code> hydrodatasource <p>See Data Guide for detailed data preparation instructions.</p>"},{"location":"usage/#model-calibration","title":"Model Calibration","text":""},{"location":"usage/#unified-calibration-api","title":"Unified Calibration API","text":"<p>The <code>calibrate()</code> function provides a completely unified calibration interface:</p> <pre><code>from hydromodel.trainers.unified_calibrate import calibrate\n\nconfig = {\n    \"data_cfgs\": {\n        \"data_source_type\": \"camels_us\",\n        \"basin_ids\": [\"01013500\"],\n        \"train_period\": [\"1990-10-01\", \"2000-09-30\"],\n        \"test_period\": [\"2000-10-01\", \"2010-09-30\"],\n        \"warmup_length\": 365,\n    },\n    \"model_cfgs\": {\n        \"model_name\": \"xaj_mz\",\n        \"model_params\": {\n            \"source_type\": \"sources\",\n            \"source_book\": \"HF\",\n        },\n    },\n    \"training_cfgs\": {\n        \"algorithm_name\": \"SCE_UA\",\n        \"algorithm_params\": {\n            \"rep\": 10000,\n            \"ngs\": 100,\n            \"random_seed\": 1234,\n        },\n        \"loss_config\": {\n            \"type\": \"time_series\",\n            \"obj_func\": \"RMSE\",\n        },\n        \"output_dir\": \"results\",\n        \"experiment_name\": \"my_experiment\",\n    },\n    \"evaluation_cfgs\": {\n        \"metrics\": [\"NSE\", \"KGE\", \"RMSE\"],\n    },\n}\n\n# Run calibration\nresults = calibrate(config)\n</code></pre>"},{"location":"usage/#internal-workflow","title":"Internal Workflow","text":"<pre><code>calibrate()\n  \u2193\n1. Parse configuration\n  \u2193\n2. UnifiedDataLoader.load_data()\n  \u2193\n3. Create UnifiedCalibrator (wraps MODEL_DICT)\n  \u2193\n4. Select algorithm (SCE_UA, GA, scipy)\n  \u2193\n5. For each basin:\n     a. Initialize parameters (normalized [0,1])\n     b. Run optimization loop\n     c. For each iteration:\n        - Denormalize parameters\n        - Call MODEL_DICT[model_name](inputs, params, ...)\n        - Calculate objective function\n        - Update parameters\n     d. Save best parameters\n  \u2193\n6. Save results to output_dir\n</code></pre>"},{"location":"usage/#algorithm-implementations","title":"Algorithm Implementations","text":""},{"location":"usage/#sce-ua-recommended","title":"SCE-UA (Recommended)","text":"<p>Uses <code>spotpy</code> library:</p> <pre><code>training_cfgs = {\n    \"algorithm_name\": \"SCE_UA\",\n    \"algorithm_params\": {\n        \"rep\": 10000,         # Maximum iterations\n        \"ngs\": 100,           # Number of complexes\n        \"kstop\": 50,          # Stopping criteria\n        \"peps\": 0.1,          # Convergence threshold\n        \"pcento\": 0.1,        # Convergence percentage\n        \"random_seed\": 1234,\n    },\n    \"loss_config\": {\n        \"type\": \"time_series\",\n        \"obj_func\": \"RMSE\",   # or \"NSE\", \"KGE\"\n    },\n}\n</code></pre> <p>Output: <code>{basin_id}_sceua.csv</code> with columns: - <code>like1</code>: Objective function value - <code>parK</code>, <code>parB</code>, ...: Parameter values (with <code>par</code> prefix) - <code>simulation1_1</code>, ...: Simulation results for each iteration</p>"},{"location":"usage/#genetic-algorithm","title":"Genetic Algorithm","text":"<p>Uses <code>DEAP</code> library:</p> <pre><code>training_cfgs = {\n    \"algorithm_name\": \"GA\",\n    \"algorithm_params\": {\n        \"run_counts\": 2,      # Number of evolutionary runs\n        \"pop_num\": 50,        # Population size\n        \"cross_prob\": 0.5,    # Crossover probability\n        \"mut_prob\": 0.5,      # Mutation probability\n        \"save_freq\": 1,       # Save frequency\n        \"random_seed\": 1234,\n    },\n}\n</code></pre> <p>Output: Pickled checkpoints (<code>epoch{N}.pkl</code>) containing: - <code>population</code>: Current population - <code>halloffame</code>: Best individuals - <code>logbook</code>: Optimization history</p>"},{"location":"usage/#scipy-optimizers","title":"Scipy Optimizers","text":"<pre><code>training_cfgs = {\n    \"algorithm_name\": \"scipy\",\n    \"algorithm_params\": {\n        \"method\": \"Nelder-Mead\",  # or \"Powell\", \"COBYLA\"\n        \"options\": {\n            \"maxiter\": 1000,\n            \"disp\": True,\n        },\n    },\n}\n</code></pre>"},{"location":"usage/#parameter-management","title":"Parameter Management","text":"<p>Parameters are always normalized to [0, 1] during optimization:</p> <pre><code>from hydromodel.models.model_config import read_model_param_dict\n\n# Get parameter ranges\nparam_dict = read_model_param_dict(None)  # Uses default\nparam_ranges = param_dict[\"xaj_mz\"]\n\nprint(param_ranges[\"param_name\"])   # ['K', 'B', 'IM', ...]\nprint(param_ranges[\"param_range\"])  # [[min, max], [min, max], ...]\n\n# During optimization:\n# 1. Optimizer works with normalized params [0, 1]\n# 2. Before model call: denormalize to physical range\n# 3. Run model with physical parameters\n# 4. Calculate objective function\n</code></pre>"},{"location":"usage/#output-files","title":"Output Files","text":"<pre><code>results/{experiment_name}/\n\u251c\u2500\u2500 {basin_id}_sceua.csv            # SCE-UA calibration history\n\u251c\u2500\u2500 calibration_config.yaml          # Config used (for reproducibility)\n\u2514\u2500\u2500 param_range.yaml                 # Parameter ranges used\n</code></pre>"},{"location":"usage/#model-evaluation","title":"Model Evaluation","text":""},{"location":"usage/#unified-evaluation-api","title":"Unified Evaluation API","text":"<pre><code>from hydromodel.trainers.unified_evaluate import evaluate\n\n# Evaluate on test period\nresults = evaluate(\n    config,\n    param_dir=\"results/my_experiment\",\n    eval_period=\"test\"  # \"train\", \"test\", or \"custom\"\n)\n\n# Evaluate on custom period\nresults = evaluate(\n    config,\n    param_dir=\"results/my_experiment\",\n    eval_period=\"custom\",\n    custom_period=[\"2010-10-01\", \"2015-09-30\"]\n)\n</code></pre>"},{"location":"usage/#internal-workflow_1","title":"Internal Workflow","text":"<pre><code>evaluate()\n  \u2193\n1. Load configuration and calibrated parameters\n  \u2193\n2. Load data for evaluation period\n  \u2193\n3. For each basin:\n     a. Load best parameters from calibration\n     b. Create UnifiedSimulator\n     c. Run simulation\n     d. Calculate metrics\n  \u2193\n4. Save results:\n     - basins_metrics.csv\n     - basins_denorm_params.csv\n     - {model_name}_evaluation_results.nc\n</code></pre>"},{"location":"usage/#metrics","title":"Metrics","text":"<p>All metrics are calculated using <code>hydroutils.hydro_stat</code>:</p> Metric Description Range Optimal NSE Nash-Sutcliffe Efficiency (-\u221e, 1] 1.0 KGE Kling-Gupta Efficiency (-\u221e, 1] 1.0 RMSE Root Mean Square Error [0, \u221e) 0.0 PBIAS Percent Bias (-\u221e, \u221e) 0.0 FHV High Flow Volume Error [0, \u221e) 0.0 FLV Low Flow Volume Error [0, \u221e) 0.0 FMS Mid-segment Slope of FDC (-\u221e, \u221e) 0.0 <p>Implementation: <pre><code>from hydroutils import hydro_stat\n\n# qobs, qsim shape: [n_basins, n_time]\nmetrics = hydro_stat.stat_error(qobs, qsim)\n\nprint(metrics[\"NSE\"])    # [n_basins]\nprint(metrics[\"KGE\"])    # [n_basins]\n</code></pre></p>"},{"location":"usage/#output-files_1","title":"Output Files","text":"<pre><code>results/{experiment_name}/evaluation_{period}/\n\u251c\u2500\u2500 basins_metrics.csv                    # Performance metrics for all basins\n\u251c\u2500\u2500 basins_norm_params.csv                # Normalized parameters [0,1]\n\u251c\u2500\u2500 basins_denorm_params.csv              # Physical parameters\n\u2514\u2500\u2500 {model_name}_evaluation_results.nc    # Full simulation results (NetCDF)\n</code></pre> <p>NetCDF structure: <pre><code>import xarray as xr\n\nds = xr.open_dataset(\"xaj_mz_evaluation_results.nc\")\n\nprint(ds.dims)      # {'basin': N, 'time': T}\nprint(ds.data_vars) # qsim, qobs, prcp, pet, ...\n\n# Access data\nqsim = ds['qsim'].values  # [basin, time]\nqobs = ds['qobs'].values  # [basin, time]\n</code></pre></p>"},{"location":"usage/#model-simulation","title":"Model Simulation","text":""},{"location":"usage/#important-design-principle","title":"Important Design Principle","text":"<p>\u26a0\ufe0f Simulation does NOT require prior calibration!</p> <p><code>UnifiedSimulator</code> is a completely independent simulation interface. You can: - Run simulations with any parameter values - Use parameters from literature - Use calibrated parameters - Perform sensitivity analysis</p> <p>Simulation and calibration are fully decoupled - this is a core design principle.</p>"},{"location":"usage/#unifiedsimulator-api","title":"UnifiedSimulator API","text":"<pre><code>from hydromodel.trainers.unified_simulate import UnifiedSimulator\nfrom hydromodel.datasets.unified_data_loader import UnifiedDataLoader\n\n# Step 1: Load data\ndata_loader = UnifiedDataLoader(data_config, is_train_val_test=\"test\")\np_and_e, qobs = data_loader.load_data()\nbasin_configs = data_loader.get_basin_configs()\n\n# Step 2: Define parameters (from anywhere!)\nparameters = {\n    \"K\": 0.75, \"B\": 0.25, \"IM\": 0.06,\n    \"UM\": 18.0, \"LM\": 80.0, \"DM\": 95.0,\n    \"C\": 0.18, \"SM\": 120.0, \"EX\": 1.5,\n    \"KI\": 0.35, \"KG\": 0.45, \"A\": 0.85,\n    \"THETA\": 0.012, \"CI\": 0.85, \"CG\": 0.95\n}\n\n# Step 3: Create simulator\nmodel_config = {\n    \"model_name\": \"xaj_mz\",\n    \"model_params\": {\n        \"source_type\": \"sources\",\n        \"source_book\": \"HF\",\n    },\n    \"parameters\": parameters\n}\n\nbasin_id = data_config[\"basin_ids\"][0]\nsimulator = UnifiedSimulator(model_config, basin_configs[basin_id])\n\n# Step 4: Run simulation\nresults = simulator.simulate(\n    inputs=p_and_e,\n    qobs=qobs,\n    warmup_length=365,\n    return_intermediate=False\n)\n\n# Step 5: Extract results\nqsim = results[\"qsim\"]  # [time, basin, 1] simulated streamflow\n\n# Calculate metrics\nfrom hydroutils import hydro_stat\nqsim_2d = qsim[365:, 0, 0].reshape(1, -1)\nqobs_2d = qobs[365:, 0, 0].reshape(1, -1)\nmetrics = hydro_stat.stat_error(qobs_2d, qsim_2d)\nprint(f\"NSE: {metrics['NSE'][0]:.3f}\")\n</code></pre>"},{"location":"usage/#unifiedsimulator-design","title":"UnifiedSimulator Design","text":"<p>Core Philosophy: All models use the same interface regardless of internal complexity.</p> <pre><code>class UnifiedSimulator:\n    def __init__(self, model_config, basin_config):\n        \"\"\"\n        Parameters\n        ----------\n        model_config : dict\n            - model_name: str (e.g., \"xaj_mz\", \"gr4j\")\n            - model_params: dict (model-specific configs)\n            - parameters: OrderedDict (calibratable parameters)\n\n        basin_config : dict (optional)\n            - basin_area: float (km\u00b2)\n            - other basin attributes\n        \"\"\"\n        self.model_name = model_config[\"model_name\"]\n        self.parameters = model_config[\"parameters\"]\n        # Initialize model from MODEL_DICT\n\n    def simulate(self, inputs, qobs=None, warmup_length=0, return_intermediate=False):\n        \"\"\"\n        Run model simulation.\n\n        Parameters\n        ----------\n        inputs : np.ndarray\n            Shape [time, basin, features] (e.g., [T, N, 2] for prcp+pet)\n        qobs : np.ndarray, optional\n            Shape [time, basin, 1], observed streamflow\n        warmup_length : int\n            Number of warmup time steps\n        return_intermediate : bool\n            Return intermediate states?\n\n        Returns\n        -------\n        dict\n            Model-specific outputs (e.g., {\"qsim\": [...], \"es\": [...]})\n        \"\"\"\n        # Normalize parameters to [0,1] if needed\n        # Call MODEL_DICT[model_name](inputs, params, ...)\n        # Return results in unified format\n</code></pre>"},{"location":"usage/#parameter-loading","title":"Parameter Loading","text":"<p>UnifiedSimulator accepts parameters from any source:</p>"},{"location":"usage/#1-from-calibration-csv","title":"1. From Calibration (CSV)","text":"<pre><code>import pandas as pd\nfrom collections import OrderedDict\n\n# Load SCE-UA results\ndf = pd.read_csv(\"results/exp/01013500_sceua.csv\")\nbest_idx = df[\"like1\"].idxmin()\nbest_row = df.iloc[best_idx]\n\n# Extract parameters\nparam_names = [\"K\", \"B\", \"IM\", \"UM\", \"LM\", \"DM\", \"C\", \"SM\", \"EX\", \"KI\", \"KG\", \"A\", \"THETA\", \"CI\", \"CG\"]\nparameters = OrderedDict()\nfor name in param_names:\n    parameters[name] = float(best_row[f\"par{name}\"])\n</code></pre>"},{"location":"usage/#2-from-yaml","title":"2. From YAML","text":"<pre><code># configs/my_params.yaml\nK: 0.75\nB: 0.25\nIM: 0.06\n# ...\n</code></pre> <pre><code>import yaml\nfrom collections import OrderedDict\n\nwith open(\"configs/my_params.yaml\", \"r\") as f:\n    parameters = OrderedDict(yaml.safe_load(f))\n</code></pre>"},{"location":"usage/#3-from-literature-or-expert-knowledge","title":"3. From Literature or Expert Knowledge","text":"<pre><code>from collections import OrderedDict\n\n# Parameters from published study\nparameters = OrderedDict({\n    \"K\": 0.8,\n    \"B\": 0.3,\n    # ... other parameters\n})\n</code></pre>"},{"location":"usage/#command-line-simulation-script","title":"Command-Line Simulation Script","text":"<p>The <code>scripts/run_xaj_simulate.py</code> is a minimal template for users to customize:</p> <pre><code># Using custom parameters (recommended)\npython scripts/run_xaj_simulate.py \\\n    --config configs/example_simulate_config.yaml \\\n    --param-file configs/example_xaj_params.yaml \\\n    --output results.csv \\\n    --plot\n\n# Using calibrated parameters (CSV format, SCE-UA only)\npython scripts/run_xaj_simulate.py \\\n    --param-file results/exp/01013500_sceua.csv \\\n    --plot\n\n# Specify basin and warmup\npython scripts/run_xaj_simulate.py \\\n    --param-file configs/params.yaml \\\n    --basin-id 01013500 \\\n    --warmup 730\n</code></pre> <p>Script design: - Simple, readable code for users to understand - Easy to modify for custom workflows - Demonstrates UnifiedSimulator usage</p>"},{"location":"usage/#common-use-cases","title":"Common Use Cases","text":""},{"location":"usage/#1-parameter-sensitivity-analysis","title":"1. Parameter Sensitivity Analysis","text":"<pre><code># Vary one parameter, observe impact\nbase_params = load_parameters(...)\n\nresults_dict = {}\nfor k_value in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    params = base_params.copy()\n    params[\"K\"] = k_value\n\n    # Update simulator\n    simulator.parameters = params\n    results = simulator.simulate(inputs, qobs, warmup_length=365)\n\n    # Store results\n    results_dict[k_value] = results[\"qsim\"]\n\n# Analyze sensitivity\nimport matplotlib.pyplot as plt\nfor k, qsim in results_dict.items():\n    plt.plot(qsim[:, 0, 0], label=f\"K={k}\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"usage/#2-model-comparison","title":"2. Model Comparison","text":"<pre><code>models = [\"xaj\", \"xaj_mz\", \"gr4j\"]\nresults_comparison = {}\n\nfor model_name in models:\n    model_config[\"model_name\"] = model_name\n    # Adjust parameters for each model as needed\n\n    simulator = UnifiedSimulator(model_config, basin_config)\n    results = simulator.simulate(inputs, qobs, warmup_length=365)\n    results_comparison[model_name] = results\n\n# Compare performance\nfor model_name, results in results_comparison.items():\n    qsim_2d = results[\"qsim\"][365:, 0, 0].reshape(1, -1)\n    metrics = hydro_stat.stat_error(qobs_2d, qsim_2d)\n    print(f\"{model_name}: NSE={metrics['NSE'][0]:.3f}\")\n</code></pre>"},{"location":"usage/#3-ensemble-simulations","title":"3. Ensemble Simulations","text":"<pre><code># Run multiple parameter sets (e.g., from different calibration runs)\nparameter_sets = [params1, params2, params3, ...]\nensemble_results = []\n\nfor params in parameter_sets:\n    simulator.parameters = params\n    results = simulator.simulate(inputs, qobs, warmup_length=365)\n    ensemble_results.append(results[\"qsim\"])\n\n# Calculate ensemble mean and spread\nensemble_array = np.array(ensemble_results)  # [n_members, time, basin, 1]\nensemble_mean = ensemble_array.mean(axis=0)\nensemble_std = ensemble_array.std(axis=0)\n</code></pre>"},{"location":"usage/#relationship-with-evaluation","title":"Relationship with Evaluation","text":"<pre><code>run_xaj_simulate.py       # Simple, flexible user template\n    \u2191\n    | (demonstrates API usage)\n    \u2193\nUnifiedSimulator          # Core simulation interface\n    \u2191\n    | (used by)\n    \u2193\nrun_xaj_evaluate.py       # Complete evaluation workflow\n                           # (with NetCDF saving, batch processing, etc.)\n</code></pre> <ul> <li><code>run_xaj_simulate.py</code>: Simple script for custom workflows</li> <li><code>run_xaj_evaluate.py</code>: Standardized evaluation pipeline</li> </ul>"},{"location":"usage/#results-visualization","title":"Results Visualization","text":""},{"location":"usage/#overview_1","title":"Overview","text":"<p>The <code>visualize.py</code> script provides a simple command-line interface for visualizing evaluation results. It generates time series plots with precipitation and streamflow comparisons.</p> <p>Key Features: - Time series plots with observed vs simulated streamflow - Precipitation displayed as inverted bars (top-down) - Automatic loading from NetCDF evaluation results - Basin-level or multi-basin visualization</p>"},{"location":"usage/#command-line-usage","title":"Command-Line Usage","text":"<p>Basic usage (visualize all basins):</p> <pre><code>python scripts/visualize.py --eval-dir results/xaj_mz_SCE_UA/evaluation_test\n</code></pre> <p>Visualize specific basins:</p> <pre><code>python scripts/visualize.py \\\n    --eval-dir results/xaj_mz_SCE_UA/evaluation_test \\\n    --basins 01013500 01022500\n</code></pre> <p>Custom output directory:</p> <pre><code>python scripts/visualize.py \\\n    --eval-dir results/xaj_mz_SCE_UA/evaluation_test \\\n    --output-dir my_figures\n</code></pre>"},{"location":"usage/#python-api-usage","title":"Python API Usage","text":"<p>For programmatic visualization:</p> <pre><code>from hydromodel.datasets.data_visualize import visualize_evaluation\n\n# Visualize all basins\nvisualize_evaluation(\n    eval_dir=\"results/xaj_mz_SCE_UA/evaluation_test\",\n    output_dir=\"figures\",  # Optional, defaults to eval_dir/figures\n    basins=None  # Optional, defaults to all basins\n)\n\n# Visualize specific basins\nvisualize_evaluation(\n    eval_dir=\"results/xaj_mz_SCE_UA/evaluation_test\",\n    basins=[\"01013500\", \"01022500\"]\n)\n</code></pre>"},{"location":"usage/#input-requirements","title":"Input Requirements","text":"<p>The visualization script expects:</p> <ol> <li>NetCDF file: <code>*_evaluation_results.nc</code> containing:</li> <li><code>qobs</code>: Observed streamflow <code>[time, basin]</code></li> <li><code>qsim</code>: Simulated streamflow <code>[time, basin]</code></li> <li><code>prcp</code>: Precipitation <code>[time, basin]</code> (optional)</li> <li><code>basin</code>: Basin IDs</li> <li> <p><code>time</code>: Time coordinates</p> </li> <li> <p>Directory structure:    <pre><code>results/xaj_mz_SCE_UA/\n\u2514\u2500\u2500 evaluation_test/\n    \u251c\u2500\u2500 test_evaluation_results.nc\n    \u2514\u2500\u2500 figures/  # Output directory (auto-created)\n        \u251c\u2500\u2500 01013500_timeseries.png\n        \u251c\u2500\u2500 01022500_timeseries.png\n        \u2514\u2500\u2500 ...\n</code></pre></p> </li> </ol>"},{"location":"usage/#output","title":"Output","text":"<p>For each basin, generates: - Time series plot: <code>{basin_id}_timeseries.png</code>   - Upper panel: Precipitation (inverted bars)   - Lower panel: Observed vs simulated streamflow</p> <p>Plot features: - Date formatting (YYYY-MM) - Dual-axis precipitation/streamflow - Legend with simulation vs observation - PNG format with 300 DPI</p>"},{"location":"usage/#advanced-visualization","title":"Advanced Visualization","text":"<p>For custom plots beyond the CLI tool, use the core plotting functions directly:</p> <pre><code>from hydromodel.datasets.data_visualize import (\n    plot_sim_and_obs,\n    plot_sim_and_obs_streamflow,\n    plot_precipitation\n)\nimport xarray as xr\n\n# Load evaluation results\nds = xr.open_dataset(\"results/xaj_mz_SCE_UA/evaluation_test/test_evaluation_results.nc\")\n\n# Extract data for a specific basin\nbasin_idx = 0\ntime = ds['time'].values\nqobs = ds['qobs'].values[:, basin_idx]\nqsim = ds['qsim'].values[:, basin_idx]\nprcp = ds['prcp'].sel(basin=ds['basin'].values[basin_idx])\n\n# Create custom plot\nplot_sim_and_obs(\n    date=time,\n    prcp=prcp,\n    sim=qsim,\n    obs=qobs,\n    save_fig=\"my_custom_plot.png\",\n    ylabel=\"Streamflow (m\u00b3/s)\"\n)\n</code></pre>"},{"location":"usage/#flood-event-data","title":"Flood Event Data","text":""},{"location":"usage/#overview_2","title":"Overview","text":"<p>hydromodel provides specialized support for flood event datasets, where data consists of discrete flood events rather than continuous time series. This is particularly useful for:</p> <ul> <li>Event-based model calibration and validation</li> <li>Flood forecasting applications</li> <li>Multi-peak flood event analysis</li> <li>Studies focusing on extreme hydrological conditions</li> </ul> <p>Key Features:</p> <ul> <li>Multi-basin support with correct time alignment (no padding issues)</li> <li>Automatic event grouping for multi-peak floods</li> <li>Event-specific visualizations showing only flood periods</li> <li>Proper handling of gaps and warmup periods</li> <li>Backward compatible with all existing hydromodel APIs</li> </ul>"},{"location":"usage/#data-format_1","title":"Data Format","text":"<p>Flood event data uses the <code>floodevent</code> data source type from <code>hydrodatasource</code>:</p> <pre><code>data_config = {\n    \"data_source_type\": \"floodevent\",  # or \"selfmadehydrodataset\"\n    \"dataset_name\": \"my_flood_data\",   # Dataset folder name\n    \"basin_ids\": [\"basin_001\"],\n    # ... other parameters\n}\n</code></pre> <p>Required Data Structure:</p> <pre><code>my_flood_data/\n\u251c\u2500\u2500 attributes/\n\u2502   \u2514\u2500\u2500 attributes.csv              # Basin metadata\n\u251c\u2500\u2500 timeseries/\n\u2502   \u251c\u2500\u2500 1h/                         # Hourly flood event data\n\u2502   \u2502   \u251c\u2500\u2500 basin_001.csv          # Time series with marker column\n\u2502   \u2502   \u251c\u2500\u2500 basin_002.csv\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 1h_units_info.json          # Variable units\n</code></pre> <p>Time series CSV format:</p> <pre><code>time,prcp,PET,streamflow,marker,event_id\n2020-08-01 00:00,0.5,0.1,10.2,1,25\n2020-08-01 01:00,1.2,0.1,12.5,1,25\n...\n2020-08-05 23:00,0.2,0.05,8.1,1,25\n2020-08-06 00:00,0.0,0.0,0.0,0,0\n...\n2020-09-01 00:00,0.8,0.12,15.3,1,26\n</code></pre> <p>Special columns:</p> <ul> <li><code>marker</code>:</li> <li><code>1</code> = flood event data (valid)</li> <li><code>0</code> = gap between events (invalid, not used in calibration/evaluation)</li> <li><code>NaN</code> = warmup period (used for model spinup, excluded from metrics)</li> <li><code>event_id</code>: Integer identifier grouping related flood peaks together</li> </ul>"},{"location":"usage/#configuration","title":"Configuration","text":"<p>YAML Configuration Example:</p> <pre><code># configs/flood_event_config.yaml\ndata_cfgs:\n  data_source_type: \"floodevent\"\n  dataset_name: \"songliao_flood_events\"\n  basin_ids: [\"songliao_21401550\", \"songliao_21100150\"]\n  train_period: [\"2019-01-01\", \"2020-12-31\"]  # Filter events by date range\n  test_period: [\"2021-01-01\", \"2022-12-31\"]\n  warmup_length: 15  # Hours for event-based data\n  time_unit: [\"1h\"]  # Hourly resolution\n  variables: [\"prcp\", \"PET\", \"streamflow\"]\n\n  # Optional: Filter by event_id\n  event_ids: [25, 26, 27]  # Only use these events\n\nmodel_cfgs:\n  model_name: \"xaj_mz\"\n  model_params:\n    source_type: \"sources\"\n    source_book: \"HF\"\n    kernel_size: 15\n\ntraining_cfgs:\n  algorithm_name: \"SCE_UA\"\n  algorithm_params:\n    rep: 5000\n    ngs: 1000\n    random_seed: 1234\n  loss_config:\n    type: \"time_series\"\n    obj_func: \"RMSE\"\n  output_dir: \"results\"\n  experiment_name: \"flood_event_calibration\"\n\nevaluation_cfgs:\n  metrics: [\"NSE\", \"KGE\", \"RMSE\", \"PBIAS\"]\n</code></pre>"},{"location":"usage/#command-line-scripts","title":"Command-Line Scripts","text":""},{"location":"usage/#quick-start-with-default-configuration","title":"Quick Start with Default Configuration","text":"<p>The <code>run_event_calibration.py</code> script provides sensible defaults for flood event calibration:</p> <pre><code># Use default configuration\npython scripts/run_event_calibration.py --default\n\n# Verify configuration without running\npython scripts/run_event_calibration.py --default --dry-run\n\n# Customize basin IDs and algorithm\npython scripts/run_event_calibration.py --default \\\n    --basin-ids songliao_21401550 songliao_21100150 \\\n    --algorithm GA \\\n    --output-dir results/flood_ga\n</code></pre> <p>Default configuration: - Data source: <code>floodevent</code> - Dataset: <code>songliao_flood_events</code> - Basin: <code>songliao_21401550</code> - Algorithm: <code>SCE_UA</code> - Warmup: 15 hours - Model: <code>xaj_mz</code></p>"},{"location":"usage/#using-configuration-files","title":"Using Configuration Files","text":"<pre><code># Calibration\npython scripts/run_event_calibration.py --config configs/flood_event_config.yaml\n\n# Or use standard calibration script (works identically)\npython scripts/run_xaj_calibration.py --config configs/flood_event_config.yaml\n\n# Evaluation (same as continuous data)\npython scripts/run_xaj_evaluate.py \\\n    --calibration-dir results/flood_event_calibration \\\n    --eval-period test\n\n# Visualization (event-specific plots)\npython scripts/visualize.py \\\n    --eval-dir results/flood_event_calibration/evaluation_test\n</code></pre>"},{"location":"usage/#python-api-usage_1","title":"Python API Usage","text":"<p>Flood event data works seamlessly with the unified API:</p> <pre><code>from hydromodel.trainers.unified_calibrate import calibrate\nfrom hydromodel.trainers.unified_evaluate import evaluate\n\n# Configuration\nconfig = {\n    \"data_cfgs\": {\n        \"data_source_type\": \"floodevent\",\n        \"dataset_name\": \"songliao_flood_events\",\n        \"basin_ids\": [\"songliao_21401550\"],\n        \"train_period\": [\"2019-01-01\", \"2020-12-31\"],\n        \"test_period\": [\"2021-01-01\", \"2022-12-31\"],\n        \"warmup_length\": 15,\n        \"time_unit\": [\"1h\"],\n        \"variables\": [\"prcp\", \"PET\", \"streamflow\"],\n    },\n    \"model_cfgs\": {\n        \"model_name\": \"xaj_mz\",\n    },\n    \"training_cfgs\": {\n        \"algorithm_name\": \"SCE_UA\",\n        \"algorithm_params\": {\"rep\": 5000, \"ngs\": 1000},\n        \"loss_config\": {\"type\": \"time_series\", \"obj_func\": \"RMSE\"},\n        \"output_dir\": \"results\",\n        \"experiment_name\": \"flood_event_exp\",\n    },\n    \"evaluation_cfgs\": {\n        \"metrics\": [\"NSE\", \"KGE\", \"RMSE\"],\n    },\n}\n\n# Calibration\nresults = calibrate(config)\n\n# Evaluation\nmetrics = evaluate(config, param_dir=\"results/flood_event_exp\", eval_period=\"test\")\n</code></pre>"},{"location":"usage/#multi-basin-time-alignment","title":"Multi-Basin Time Alignment","text":"<p>Problem: Different basins may have flood events at different times, creating challenges for multi-basin calibration.</p> <p>Solution: hydromodel automatically handles time alignment:</p> <pre><code># Example: Two basins with different event times\n# Basin A: Events on 2020-08-01 to 2020-08-05 (Event 25)\n# Basin B: Events on 2020-09-01 to 2020-09-05 (Event 26)\n\n# hydromodel creates a unified time array:\n# - Merges all unique timestamps from both basins\n# - Maps each basin's data to correct time positions\n# - Fills gaps with marker=0 (invalid data, excluded from loss)\n# - Preserves event_id for visualization\n\n# No manual intervention required!\n</code></pre> <p>Internal Workflow:</p> <ol> <li>Load each basin's event data separately</li> <li>Extract unique timestamps across all basins</li> <li>Create unified sorted time array</li> <li>Remap each basin's data to unified time indices</li> <li>Mark missing periods with <code>marker=0</code>, <code>event_id=0</code></li> <li>Calibration/evaluation uses only <code>marker=1</code> data</li> </ol> <p>NetCDF Output Structure:</p> <pre><code>import xarray as xr\n\nds = xr.open_dataset(\"results/flood_event_exp/evaluation_test/test_evaluation_results.nc\")\n\nprint(ds.dims)\n# {'basin': 2, 'time': 1500}  # Unified time array\n\nprint(ds['event_id'])\n# Shows event_id for each time step and basin\n# event_id=0 indicates gaps (not used in metrics)\n\nprint(ds['qsim'])\n# Simulated streamflow [time, basin]\n# Zero values where marker=0 (gaps)\n</code></pre>"},{"location":"usage/#event-specific-visualization","title":"Event-Specific Visualization","text":"<p>Flood event plots automatically highlight event periods:</p> <pre><code>from hydromodel.datasets.data_visualize import visualize_evaluation\n\n# Visualize flood events\nvisualize_evaluation(\n    eval_dir=\"results/flood_event_calibration/evaluation_test\",\n    basins=[\"songliao_21401550\"]\n)\n</code></pre> <p>Plot Features:</p> <ul> <li>Only shows periods where <code>marker=1</code> (flood events)</li> <li>Gaps between events are excluded from visualization</li> <li>Event IDs displayed in plot titles</li> <li>Precipitation and streamflow on separate panels</li> <li>Performance metrics (NSE, RMSE, PBIAS) shown in text box</li> </ul>"},{"location":"usage/#multi-peak-event-grouping","title":"Multi-Peak Event Grouping","text":"<p>Use <code>event_id</code> to group related flood peaks:</p> <pre><code># Example: Typhoon with multiple peaks\n# Event 25:\n#   - Peak 1: 2020-08-01 to 2020-08-03\n#   - Gap:    2020-08-04 to 2020-08-05 (marker=0)\n#   - Peak 2: 2020-08-06 to 2020-08-08\n#   - All marked as event_id=25\n\n# During calibration:\n# - Model sees both peaks with correct warmup\n# - Gap period (marker=0) excluded from loss calculation\n# - event_id preserved for analysis\n\n# During visualization:\n# - Both peaks plotted together as \"Event 25\"\n# - Gap period shown but marked differently\n</code></pre>"},{"location":"usage/#filtering-events","title":"Filtering Events","text":"<p>By Time Range:</p> <pre><code>config[\"data_cfgs\"][\"train_period\"] = [\"2019-07-01\", \"2020-09-30\"]\n# Only loads events within this date range\n</code></pre> <p>By Event ID:</p> <pre><code>config[\"data_cfgs\"][\"event_ids\"] = [25, 26, 27]\n# Only loads these specific events\n</code></pre> <p>By Basin:</p> <pre><code>config[\"data_cfgs\"][\"basin_ids\"] = [\"songliao_21401550\"]\n# Single basin calibration\n</code></pre>"},{"location":"usage/#comparison-with-continuous-data","title":"Comparison with Continuous Data","text":"Feature Continuous Data Flood Event Data Data source CAMELS, selfmadehydrodataset floodevent Time structure Continuous time series Discrete events with gaps Warmup Days (typically 365) Hours (typically 15) marker column Not used Required (1=valid, 0=gap) event_id column Not used Required for grouping API usage Identical Identical Output format NetCDF with continuous time NetCDF with unified time array"},{"location":"usage/#best-practices","title":"Best Practices","text":"<p>1. Adequate Warmup:</p> <pre><code># For hourly data, use at least 15 hours warmup\nconfig[\"data_cfgs\"][\"warmup_length\"] = 15\n\n# For event data with long recessions, increase warmup\nconfig[\"data_cfgs\"][\"warmup_length\"] = 30\n</code></pre> <p>2. Event Selection:</p> <pre><code># Start with well-observed, significant events\nconfig[\"data_cfgs\"][\"event_ids\"] = [25, 26, 27]  # Major floods only\n\n# Avoid events with missing data or measurement errors\n</code></pre> <p>3. Multi-Basin Calibration:</p> <pre><code># Ensure basins have overlapping events for better parameter transfer\nbasin_ids = [\"basin_A\", \"basin_B\", \"basin_C\"]\n\n# Check event coverage before calibration\nfrom hydrodatasource import FloodEventDataSource\nds = FloodEventDataSource(...)\nfor basin in basin_ids:\n    events = ds.get_events(basin)\n    print(f\"{basin}: {len(events)} events\")\n</code></pre> <p>4. Validation:</p> <pre><code># Always validate on independent events\nconfig[\"data_cfgs\"][\"train_period\"] = [\"2018-01-01\", \"2020-12-31\"]\nconfig[\"data_cfgs\"][\"test_period\"] = [\"2021-01-01\", \"2022-12-31\"]\n\n# Or use event-based split\ntrain_events = [20, 21, 22, 23, 24]\ntest_events = [25, 26, 27]\n</code></pre>"},{"location":"usage/#troubleshooting","title":"Troubleshooting","text":"<p>Issue 1: Time Misalignment in Multi-Basin Results</p> <p>Symptoms: NetCDF shows incorrect time ranges for events (e.g., Event 26 spanning years instead of days)</p> <p>Solution: This has been fixed in v0.3.0. Ensure you're using the latest version.</p> <p>Issue 2: AttributeError in Calibration</p> <p>Symptoms: <code>'NoneType' object has no attribute 'shape'</code></p> <p>Solution: This has been fixed in v0.3.0. The issue occurred when accessing basin data in separate mode.</p> <p>Issue 3: Missing Event Data</p> <p>Symptoms: Fewer events loaded than expected</p> <p>Check: <pre><code># Verify event filtering\nprint(f\"Time range: {config['data_cfgs']['train_period']}\")\nprint(f\"Event IDs: {config['data_cfgs'].get('event_ids', 'All')}\")\n\n# Check raw data\nfrom hydrodatasource import FloodEventDataSource\nds = FloodEventDataSource(data_path=..., time_unit=[\"1h\"])\nevents = ds.get_events(basin_id)\nprint(f\"Available events: {len(events)}\")\n</code></pre></p> <p>Issue 4: Poor Calibration Performance</p> <p>Possible causes: - Insufficient warmup period - Events too short for model spinup - Mixed event types (e.g., snowmelt and rainfall floods)</p> <p>Solutions: - Increase warmup: <code>warmup_length = 30</code> - Filter events by type or magnitude - Check data quality (missing values, outliers)</p>"},{"location":"usage/#example-workflow","title":"Example Workflow","text":"<p>Complete workflow for flood event calibration:</p> <pre><code>from hydromodel.trainers.unified_calibrate import calibrate\nfrom hydromodel.trainers.unified_evaluate import evaluate\nfrom hydromodel.datasets.data_visualize import visualize_evaluation\n\n# 1. Configuration\nconfig = {\n    \"data_cfgs\": {\n        \"data_source_type\": \"floodevent\",\n        \"dataset_name\": \"songliao_flood_events\",\n        \"basin_ids\": [\"songliao_21401550\", \"songliao_21100150\"],\n        \"train_period\": [\"2019-01-01\", \"2020-12-31\"],\n        \"test_period\": [\"2021-01-01\", \"2022-12-31\"],\n        \"warmup_length\": 15,\n        \"time_unit\": [\"1h\"],\n        \"variables\": [\"prcp\", \"PET\", \"streamflow\"],\n    },\n    \"model_cfgs\": {\"model_name\": \"xaj_mz\"},\n    \"training_cfgs\": {\n        \"algorithm_name\": \"SCE_UA\",\n        \"algorithm_params\": {\"rep\": 5000, \"ngs\": 1000},\n        \"loss_config\": {\"type\": \"time_series\", \"obj_func\": \"RMSE\"},\n        \"output_dir\": \"results\",\n        \"experiment_name\": \"flood_2basin\",\n    },\n    \"evaluation_cfgs\": {\"metrics\": [\"NSE\", \"KGE\", \"RMSE\", \"PBIAS\"]},\n}\n\n# 2. Calibration\nprint(\"Starting calibration...\")\nresults = calibrate(config)\nprint(f\"Calibration completed: {results}\")\n\n# 3. Evaluation on test period\nprint(\"Evaluating on test period...\")\ntest_metrics = evaluate(\n    config,\n    param_dir=\"results/flood_2basin\",\n    eval_period=\"test\"\n)\nprint(f\"Test NSE: {test_metrics}\")\n\n# 4. Visualization\nprint(\"Creating visualizations...\")\nvisualize_evaluation(\n    eval_dir=\"results/flood_2basin/evaluation_test\",\n    output_dir=\"figures/flood_events\"\n)\nprint(\"Done!\")\n</code></pre>"},{"location":"usage/#related-documentation","title":"Related Documentation","text":"<ul> <li>Data Preparation: Data Guide - How to prepare flood event data</li> <li>API Reference: Unified API - Core API documentation</li> <li>Configuration: Configuration System - Full configuration options</li> <li>hydrodatasource: GitHub - Data source package</li> </ul>"},{"location":"usage/#configuration-system","title":"Configuration System","text":""},{"location":"usage/#configuration-structure","title":"Configuration Structure","text":"<p>All APIs use a consistent configuration format:</p> <pre><code>config = {\n    \"data_cfgs\": {\n        # Data source and loading\n        \"data_source_type\": str,\n        \"data_source_path\": str,  # Optional for CAMELS\n        \"basin_ids\": list[str],\n        \"train_period\": [str, str],\n        \"test_period\": [str, str],\n        \"warmup_length\": int,\n        \"variables\": list[str],\n    },\n    \"model_cfgs\": {\n        # Model configuration\n        \"model_name\": str,\n        \"model_params\": dict,\n    },\n    \"training_cfgs\": {\n        # Calibration settings\n        \"algorithm_name\": str,\n        \"algorithm_params\": dict,\n        \"loss_config\": dict,\n        \"output_dir\": str,\n        \"experiment_name\": str,\n    },\n    \"evaluation_cfgs\": {\n        # Evaluation metrics\n        \"metrics\": list[str],\n    },\n}\n</code></pre>"},{"location":"usage/#yaml-configuration","title":"YAML Configuration","text":"<p>For reproducibility, use YAML files:</p> <pre><code># config.yaml\ndata_cfgs:\n  data_source_type: \"camels_us\"\n  basin_ids: [\"01013500\"]\n  train_period: [\"1990-10-01\", \"2000-09-30\"]\n  test_period: [\"2000-10-01\", \"2010-09-30\"]\n  warmup_length: 365\n  variables: [\"precipitation\", \"potential_evapotranspiration\", \"streamflow\"]\n\nmodel_cfgs:\n  model_name: \"xaj_mz\"\n  model_params:\n    source_type: \"sources\"\n    source_book: \"HF\"\n\ntraining_cfgs:\n  algorithm_name: \"SCE_UA\"\n  algorithm_params:\n    rep: 10000\n    ngs: 100\n    random_seed: 1234\n  loss_config:\n    type: \"time_series\"\n    obj_func: \"RMSE\"\n  output_dir: \"results\"\n  experiment_name: \"my_exp\"\n\nevaluation_cfgs:\n  metrics: [\"NSE\", \"KGE\", \"RMSE\", \"PBIAS\"]\n</code></pre> <p>Load and use:</p> <pre><code>import yaml\nfrom hydromodel.trainers.unified_calibrate import calibrate\n\nwith open(\"config.yaml\", \"r\") as f:\n    config = yaml.safe_load(f)\n\nresults = calibrate(config)\n</code></pre>"},{"location":"usage/#advanced-topics","title":"Advanced Topics","text":""},{"location":"usage/#model_dict-registry","title":"MODEL_DICT Registry","text":"<p>All models are registered in <code>models/model_dict.py</code>:</p> <pre><code>from hydromodel.models.model_dict import MODEL_DICT\n\n# Available models\nprint(MODEL_DICT.keys())  # ['xaj', 'xaj_mz', 'gr4j', ...]\n\n# Model signature\nmodel_func = MODEL_DICT[\"xaj_mz\"]\nqsim, intermediates = model_func(\n    p_and_e,           # [time, basin, 2]\n    params,            # [basin, n_params]\n    warmup_length=365,\n    **model_params\n)\n</code></pre> <p>Adding a new model:</p> <ol> <li>Implement model in <code>models/my_model.py</code></li> <li>Register in <code>MODEL_DICT</code></li> <li>Add parameter ranges to <code>model_config.py</code></li> <li>Model immediately works with all APIs</li> </ol>"},{"location":"usage/#custom-loss-functions","title":"Custom Loss Functions","text":"<pre><code>def my_custom_loss(obs, sim):\n    \"\"\"\n    Custom objective function.\n\n    Parameters\n    ----------\n    obs, sim : np.ndarray\n        Shape [time, basin, 1]\n\n    Returns\n    -------\n    float\n        Loss value (to minimize)\n    \"\"\"\n    # Example: Combine NSE and PBIAS\n    nse = calculate_nse(obs, sim)\n    pbias = calculate_pbias(obs, sim)\n    return -nse + abs(pbias) / 100\n\n# Use in configuration\nconfig[\"training_cfgs\"][\"loss_config\"] = {\n    \"type\": \"custom\",\n    \"obj_func\": my_custom_loss\n}\n</code></pre>"},{"location":"usage/#batch-processing","title":"Batch Processing","text":"<p>Process multiple experiments programmatically:</p> <pre><code>experiments = [\n    {\"name\": \"exp1\", \"basins\": [\"01013500\"], \"algorithm\": \"SCE_UA\"},\n    {\"name\": \"exp2\", \"basins\": [\"01022500\"], \"algorithm\": \"GA\"},\n    {\"name\": \"exp3\", \"basins\": [\"01030500\"], \"algorithm\": \"scipy\"},\n]\n\nfor exp in experiments:\n    config[\"data_cfgs\"][\"basin_ids\"] = exp[\"basins\"]\n    config[\"training_cfgs\"][\"algorithm_name\"] = exp[\"algorithm\"]\n    config[\"training_cfgs\"][\"experiment_name\"] = exp[\"name\"]\n\n    results = calibrate(config)\n    print(f\"Completed {exp['name']}\")\n</code></pre>"},{"location":"usage/#parallel-basin-calibration","title":"Parallel Basin Calibration","text":"<p>Calibrate multiple basins in parallel:</p> <pre><code>from multiprocessing import Pool\n\ndef calibrate_basin(basin_id):\n    \"\"\"Calibrate single basin.\"\"\"\n    config_copy = config.copy()\n    config_copy[\"data_cfgs\"][\"basin_ids\"] = [basin_id]\n    config_copy[\"training_cfgs\"][\"experiment_name\"] = f\"exp_{basin_id}\"\n    return calibrate(config_copy)\n\n# Parallel execution\nbasin_ids = [\"01013500\", \"01022500\", \"01030500\"]\nwith Pool(processes=3) as pool:\n    results = pool.map(calibrate_basin, basin_ids)\n</code></pre>"},{"location":"usage/#custom-parameter-ranges","title":"Custom Parameter Ranges","text":"<p>Override default parameter ranges:</p> <pre><code># param_range.yaml\nxaj_mz:\n  param_name:\n    - K\n    - B\n    - IM\n  param_range:\n    - [0.5, 1.5]    # K range\n    - [0.1, 0.5]    # B range\n    - [0.01, 0.1]   # IM range\n</code></pre> <pre><code>config[\"training_cfgs\"][\"param_range_file\"] = \"param_range.yaml\"\n</code></pre>"},{"location":"usage/#intermediate-states","title":"Intermediate States","text":"<p>Return intermediate model states:</p> <pre><code>results = simulator.simulate(\n    inputs=p_and_e,\n    qobs=qobs,\n    warmup_length=365,\n    return_intermediate=True  # \u2190 Enable intermediate outputs\n)\n\n# Access intermediate states (model-specific)\nif \"EU\" in results:\n    eu = results[\"EU\"]  # Upper layer soil moisture\nif \"EL\" in results:\n    el = results[\"EL\"]  # Lower layer soil moisture\n</code></pre>"},{"location":"usage/#best-practices_1","title":"Best Practices","text":""},{"location":"usage/#1-configuration-management","title":"1. Configuration Management","text":"<ul> <li>\u2705 Use YAML files for all experiments</li> <li>\u2705 Save configs with results (<code>output_dir/calibration_config.yaml</code>)</li> <li>\u2705 Version control configurations in git</li> <li>\u2705 Document parameter choices in comments</li> </ul>"},{"location":"usage/#2-data-quality","title":"2. Data Quality","text":"<pre><code># Always verify data before calibration\ndata = data_loader.load_data()\nprint(f\"Data shape: {data[0].shape}\")\nprint(f\"Missing values: {np.isnan(data[0]).sum()}\")\nprint(f\"Data range: [{data[0].min():.2f}, {data[0].max():.2f}]\")\n</code></pre>"},{"location":"usage/#3-warmup-period","title":"3. Warmup Period","text":"<ul> <li>\u2705 Always use adequate warmup (typically 365 days)</li> <li>\u2705 Exclude warmup from evaluation metrics</li> <li>\u2705 Longer warmup for longer memory models</li> </ul>"},{"location":"usage/#4-reproducibility","title":"4. Reproducibility","text":"<pre><code># Set random seeds\nimport numpy as np\nimport random\n\nnp.random.seed(1234)\nrandom.seed(1234)\n\n# Save exact package versions\n# requirements.txt or environment.yml\n</code></pre>"},{"location":"usage/#5-result-validation","title":"5. Result Validation","text":"<pre><code># After calibration, always evaluate on independent test period\nresults_train = evaluate(config, eval_period=\"train\")\nresults_test = evaluate(config, eval_period=\"test\")\n\n# Compare performance\nprint(f\"Train NSE: {results_train['metrics']['01013500']['NSE']:.3f}\")\nprint(f\"Test NSE: {results_test['metrics']['01013500']['NSE']:.3f}\")\n</code></pre>"},{"location":"usage/#troubleshooting_1","title":"Troubleshooting","text":""},{"location":"usage/#common-issues","title":"Common Issues","text":"<p>1. Shape mismatch errors:</p> <pre><code># Check data shapes\nprint(f\"p_and_e: {p_and_e.shape}\")  # Should be [time, basin, 2]\nprint(f\"params: {params.shape}\")     # Should be [basin, n_params]\n</code></pre> <p>2. Parameter out of bounds:</p> <pre><code># Check parameter ranges\nfrom hydromodel.models.model_config import read_model_param_dict\nparam_dict = read_model_param_dict(None)\nprint(param_dict[\"xaj_mz\"][\"param_range\"])\n</code></pre> <p>3. Memory issues:</p> <pre><code># Process basins in batches\nbatch_size = 10\nfor i in range(0, len(all_basins), batch_size):\n    batch = all_basins[i:i+batch_size]\n    config[\"data_cfgs\"][\"basin_ids\"] = batch\n    calibrate(config)\n</code></pre> <p>4. Slow calibration:</p> <ul> <li>Use <code>xaj_mz</code> instead of full <code>xaj</code> (fewer parameters)</li> <li>Reduce <code>rep</code> and <code>ngs</code> for testing</li> <li>Consider faster algorithms (GA, scipy)</li> <li>Profile code to find bottlenecks</li> </ul>"},{"location":"usage/#summary","title":"Summary","text":""},{"location":"usage/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Unified Design: All models, algorithms, and data sources use the same API</li> <li>Configuration-Based: YAML configs ensure reproducibility</li> <li>Decoupled Components: Calibration, evaluation, and simulation are independent</li> <li>Flexible Integration: Works with CAMELS and custom data</li> <li>Extensible: Easy to add new models, algorithms, and metrics</li> </ol>"},{"location":"usage/#core-apis","title":"Core APIs","text":"<pre><code># Data loading\nfrom hydromodel.datasets.unified_data_loader import UnifiedDataLoader\ndata_loader = UnifiedDataLoader(config)\np_and_e, qobs = data_loader.load_data()\n\n# Calibration\nfrom hydromodel.trainers.unified_calibrate import calibrate\nresults = calibrate(config)\n\n# Evaluation\nfrom hydromodel.trainers.unified_evaluate import evaluate\nmetrics = evaluate(config, param_dir=\"results/exp\", eval_period=\"test\")\n\n# Simulation\nfrom hydromodel.trainers.unified_simulate import UnifiedSimulator\nsimulator = UnifiedSimulator(model_config, basin_config)\nresults = simulator.simulate(inputs, qobs, warmup_length=365)\n</code></pre>"},{"location":"usage/#additional-resources","title":"Additional Resources","text":"<ul> <li>Quick Start: quickstart.md - End user guide for quick setup</li> <li>Data Guide: data_guide.md - Data preparation and management</li> <li>FAQ: faq.md - Common questions and solutions</li> <li>API Reference: Full API documentation (auto-generated)</li> <li>GitHub: https://github.com/OuyangWenyu/hydromodel</li> <li>Issues: https://github.com/OuyangWenyu/hydromodel/issues</li> </ul>"},{"location":"usage/#contributing","title":"Contributing","text":"<p>For developers interested in contributing:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Follow the unified API design principles</li> <li>Add tests for new features</li> <li>Update documentation</li> <li>Submit a pull request</li> </ol> <p>See contributing.md for detailed guidelines.</p>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>print('Hello World!')\n</pre> print('Hello World!') <pre>Hello World!\n</pre>"},{"location":"models/dhf/","title":"Dahuofang (DHF) Model","text":"<p>The Dahuofang (DHF) model is a lumped conceptual hydrological model proposed by the Dahuofang Reservoir Administration in 1973. It consists of two main components: an 8-parameter infiltration excess runoff calculation model and an 8-parameter empirical unit hydrograph routing model with variable intensity and variable confluence velocity.</p> <p>The model employs a dual-layer infiltration curve for loss calculation and uses a parabolic function to describe the upper layer water storage and dual-layer infiltration rate distribution. This is a lumped conceptual model specifically designed for flood forecasting applications.</p>"},{"location":"models/dhf/#references","title":"References","text":"<p>The implementation is primarily based on the Chinese textbook: - \"\u6c34\u5e93\u9632\u6d2a\u9884\u62a5\u8c03\u5ea6\u65b9\u6cd5\u53ca\u5e94\u7528\" (Reservoir Flood Control Forecasting and Dispatching Methods and Applications), edited by Dalian University of Technology and the Office of State Flood Control and Drought Relief Headquarters, published by China Water &amp; Power Press.</p> <p>For English references, see: - Li, X., Ye, L., Gu, X. et al. Development of A Distributed Modeling Framework Considering Spatiotemporally Varying Hydrological Processes for Sub-Daily Flood Forecasting in Semi-Humid and Semi-Arid Watersheds. Water Resour Manage 38, 3725\u20133754 (2024). https://doi.org/10.1007/s11269-024-03837-5</p>"},{"location":"models/dhf/#model-overview","title":"Model Overview","text":"<p>The main entry point for the DHF model is the <code>dhf()</code> function, which provides a complete implementation of the Dahuofang hydrological model. This function integrates both runoff generation and routing components in a single, vectorized interface that can process multiple basins simultaneously.</p> <p>Key features of the <code>dhf()</code> function: - Vectorized computation: Processes all basins simultaneously using NumPy operations - Complete water balance: Handles both runoff generation and channel routing - State management: Supports warmup periods and state variable tracking - Flexible configuration: Configurable time intervals, basin parameters, and routing options - High performance: Optimized for large-scale flood forecasting applications</p>"},{"location":"models/dhf/#model-components","title":"Model Components","text":""},{"location":"models/dhf/#1-runoff-generation","title":"1. Runoff Generation (\u4ea7\u6d41\u8ba1\u7b97)","text":"<p>The runoff generation component uses an 8-parameter infiltration excess model that:</p> <ul> <li>Dual-layer infiltration curve: Models water loss through a two-layer soil structure</li> <li>Parabolic distribution: Describes upper layer water storage capacity using parabolic functions</li> <li>Surface and subsurface separation: Distinguishes between surface runoff, interflow, and groundwater components</li> <li>Evapotranspiration handling: Accounts for both net precipitation and evaporation deficit conditions</li> <li>Storage dynamics: Updates soil moisture states for surface, subsurface, and deep layers</li> </ul> <p>Key parameters include surface storage capacity (S0), subsurface storage capacity (U0), deep storage capacity (D0), and various coefficients controlling infiltration and flow generation.</p>"},{"location":"models/dhf/#2-flow-routing","title":"2. Flow Routing (\u6c47\u6d41\u8ba1\u7b97)","text":"<p>The routing component implements an 8-parameter empirical unit hydrograph model featuring:</p> <ul> <li>Variable intensity routing: Adapts routing parameters based on antecedent conditions and current runoff intensity  </li> <li>Variable confluence velocity: Adjusts flow velocity according to flow magnitude and basin characteristics</li> <li>Empirical unit hydrograph: Uses trigonometric functions to describe the unit hydrograph shape</li> <li>Surface and subsurface separation: Routes surface flow and subsurface flow with different time constants</li> <li>Channel parameters: Incorporates main channel length, basin area, and routing coefficients</li> </ul> <p>The routing process considers both immediate surface flow response and delayed subsurface contributions, providing realistic flood hydrograph simulation for Chinese watershed conditions.</p>"},{"location":"models/dhf/#api-reference","title":"API Reference","text":"<p>Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.</p>"},{"location":"models/dhf/#hydromodel.models.dhf.calculate_dhf_evaporation_deficit","title":"<code>calculate_dhf_evaporation_deficit(precipitation, edt, sa, ua, s0, u0, a)</code>","text":"<p>Calculate evaporation when precipitation is insufficient.</p> <p>Parameters:</p> Name Type Description Default <code>precipitation</code> <code>ndarray</code> <p>Precipitation values</p> required <code>edt</code> <code>ndarray</code> <p>Evapotranspiration demand</p> required <code>sa</code> <code>ndarray</code> <p>Surface water storage</p> required <code>ua</code> <code>ndarray</code> <p>Subsurface water storage</p> required <code>s0</code> <code>ndarray</code> <p>Surface storage capacity</p> required <code>u0</code> <code>ndarray</code> <p>Subsurface storage capacity</p> required <code>a</code> <code>ndarray</code> <p>Surface storage exponent</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Updated surface storage (sa_new) and subsurface storage (ua_new) after evaporation</p> Source code in <code>hydromodel/models/dhf.py</code> <pre><code>@jit(nopython=True)\ndef calculate_dhf_evaporation_deficit(\n    precipitation: np.ndarray,\n    edt: np.ndarray,\n    sa: np.ndarray,\n    ua: np.ndarray,\n    s0: np.ndarray,\n    u0: np.ndarray,\n    a: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Calculate evaporation when precipitation is insufficient.\n\n    Args:\n        precipitation (np.ndarray): Precipitation values\n        edt (np.ndarray): Evapotranspiration demand\n        sa (np.ndarray): Surface water storage\n        ua (np.ndarray): Subsurface water storage\n        s0 (np.ndarray): Surface storage capacity\n        u0 (np.ndarray): Subsurface storage capacity\n        a (np.ndarray): Surface storage exponent\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Updated surface storage (sa_new) and subsurface storage (ua_new) after evaporation\n    \"\"\"\n    ec = edt - precipitation\n    eb = ec  # accumulated deficit\n\n    # Calculate surface evaporation\n    temp1 = (1 - (eb - ec) / (a * s0)) ** a\n    temp2 = (1 - eb / (a * s0)) ** a\n\n    eu = np.where(\n        (eb / (a * s0) &lt;= 0.999999) &amp; ((eb - ec) / (a * s0) &lt;= 0.999999),\n        s0 * (temp1 - temp2),\n        np.where(\n            (eb / (a * s0) &gt;= 1.00001) &amp; ((eb - ec) / (a * s0) &lt;= 0.999999),\n            s0 * temp1,\n            0.00001,\n        ),\n    )\n\n    # Update storages after evaporation\n    el = np.where(sa - eu &lt; 0.0, (ec - sa) * ua / u0, (ec - eu) * ua / u0)\n    sa_new = np.where(sa - eu &lt; 0.0, 0.0, sa - eu)\n    ua_new = ua - el\n    ua_new = np.maximum(ua_new, 0.0)\n\n    return sa_new, ua_new\n</code></pre>"},{"location":"models/dhf/#hydromodel.models.dhf.calculate_dhf_evapotranspiration","title":"<code>calculate_dhf_evapotranspiration(precipitation, potential_evapotranspiration, kc)</code>","text":"<p>Calculate evapotranspiration and net precipitation for DHF model.</p> <p>TODO: We writes some atomic functions for DHF model, but it is not used yet.</p> <p>Parameters:</p> Name Type Description Default <code>precipitation</code> <code>ndarray</code> <p>Precipitation values</p> required <code>potential_evapotranspiration</code> <code>ndarray</code> <p>Potential evapotranspiration values</p> required <code>kc</code> <code>ndarray</code> <p>Evaporation coefficient</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Net precipitation (pe) and evapotranspiration (edt)</p> Source code in <code>hydromodel/models/dhf.py</code> <pre><code>@jit(nopython=True)\ndef calculate_dhf_evapotranspiration(\n    precipitation: np.ndarray,\n    potential_evapotranspiration: np.ndarray,\n    kc: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Calculate evapotranspiration and net precipitation for DHF model.\n\n    TODO: We writes some atomic functions for DHF model, but it is not used yet.\n\n    Args:\n        precipitation (np.ndarray): Precipitation values\n        potential_evapotranspiration (np.ndarray): Potential evapotranspiration values\n        kc (np.ndarray): Evaporation coefficient\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Net precipitation (pe) and evapotranspiration (edt)\n    \"\"\"\n    edt = kc * potential_evapotranspiration\n    pe = precipitation - edt  # net precipitation\n    return pe, edt\n</code></pre>"},{"location":"models/dhf/#hydromodel.models.dhf.calculate_dhf_routing","title":"<code>calculate_dhf_routing(runoff_sim, rl, tm, k3, k3l, aa, aal, tt, ts, dd, cc, ddl, ccl, time_steps, pai)</code>","text":"<p>Calculate routing for DHF model.</p> <p>Parameters:</p> Name Type Description Default <code>runoff_sim</code> <code>ndarray</code> <p>Simulated runoff</p> required <code>rl</code> <code>ndarray</code> <p>Subsurface flow</p> required <code>tm</code> <code>ndarray</code> <p>Time parameter</p> required <code>k3</code> <code>ndarray</code> <p>Surface routing coefficient</p> required <code>k3l</code> <code>ndarray</code> <p>Subsurface routing coefficient</p> required <code>aa</code> <code>ndarray</code> <p>Surface routing parameter aa</p> required <code>aal</code> <code>ndarray</code> <p>Subsurface routing parameter aal</p> required <code>tt</code> <code>ndarray</code> <p>Time index for subsurface flow</p> required <code>ts</code> <code>ndarray</code> <p>Time index for surface flow</p> required <code>dd</code> <code>ndarray</code> <p>Surface routing parameter dd</p> required <code>cc</code> <code>ndarray</code> <p>Surface routing parameter cc</p> required <code>ddl</code> <code>ndarray</code> <p>Subsurface routing parameter ddl</p> required <code>ccl</code> <code>ndarray</code> <p>Subsurface routing parameter ccl</p> required <code>time_steps</code> <code>int</code> <p>Number of time steps</p> required <code>pai</code> <code>float</code> <p>Pi constant</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Surface flow (qs) and subsurface flow (ql)</p> Source code in <code>hydromodel/models/dhf.py</code> <pre><code>@jit(nopython=True)\ndef calculate_dhf_routing(\n    runoff_sim: np.ndarray,\n    rl: np.ndarray,\n    tm: np.ndarray,\n    k3: np.ndarray,\n    k3l: np.ndarray,\n    aa: np.ndarray,\n    aal: np.ndarray,\n    tt: np.ndarray,\n    ts: np.ndarray,\n    dd: np.ndarray,\n    cc: np.ndarray,\n    ddl: np.ndarray,\n    ccl: np.ndarray,\n    time_steps: int,\n    pai: float,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Calculate routing for DHF model.\n\n    Args:\n        runoff_sim (np.ndarray): Simulated runoff\n        rl (np.ndarray): Subsurface flow\n        tm (np.ndarray): Time parameter\n        k3 (np.ndarray): Surface routing coefficient\n        k3l (np.ndarray): Subsurface routing coefficient\n        aa (np.ndarray): Surface routing parameter aa\n        aal (np.ndarray): Subsurface routing parameter aal\n        tt (np.ndarray): Time index for subsurface flow\n        ts (np.ndarray): Time index for surface flow\n        dd (np.ndarray): Surface routing parameter dd\n        cc (np.ndarray): Surface routing parameter cc\n        ddl (np.ndarray): Subsurface routing parameter ddl\n        ccl (np.ndarray): Subsurface routing parameter ccl\n        time_steps (int): Number of time steps\n        pai (float): Pi constant\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Surface flow (qs) and subsurface flow (ql)\n    \"\"\"\n    qs = np.zeros_like(runoff_sim)\n    ql = np.zeros_like(runoff_sim)\n\n    for i in range(time_steps):\n        tl = tt[i] + ts[i] - 1\n        tl = max(tl, 0)\n\n        for j in range(int(tl) + 1):\n            if i + j &gt;= time_steps:\n                break\n\n            # Surface routing\n            if tm[i] &gt; 0:\n                temp0 = pai * j / tm[i]\n                temp1 = temp0 ** dd[i]\n                temp2 = np.exp(-aa[i] * temp1)\n                temp3 = (np.sin(temp0)) ** cc[i]\n                qs_contrib = (\n                    (runoff_sim[i] - rl[i]) * k3[i] / tm[i] * temp2 * temp3\n                )\n            else:\n                qs_contrib = 0.0\n\n            # Subsurface routing\n            if tt[i] &gt; 0 and j &gt;= ts[i]:\n                temp00 = pai * (j - ts[i]) / tt[i]\n                temp10 = temp00 ** ddl[i]\n                temp20 = np.exp(-aal[i] * temp10)\n                temp30 = (np.sin(temp00)) ** ccl[i]\n                ql_contrib = rl[i] * k3l[i] / tt[i] * temp20 * temp30\n            else:\n                ql_contrib = 0.0\n\n            # Add contributions based on timing conditions\n            if j &lt;= tm[i]:\n                if j &lt;= ts[i]:\n                    qs[i + j] += qs_contrib\n                else:\n                    qs[i + j] += qs_contrib\n                    ql[i + j] += ql_contrib\n            else:\n                ql[i + j] += ql_contrib\n\n    return qs, ql\n</code></pre>"},{"location":"models/dhf/#hydromodel.models.dhf.calculate_dhf_routing_params","title":"<code>calculate_dhf_routing_params(ya, runoff_sim, l, b0, k0, n, coe, dd, cc, ddl, ccl, time_interval, pai)</code>","text":"<p>Calculate routing parameters for DHF model.</p> <p>Parameters:</p> Name Type Description Default <code>ya</code> <code>ndarray</code> <p>Precedent rain parameter</p> required <code>runoff_sim</code> <code>ndarray</code> <p>Simulated runoff</p> required <code>l</code> <code>ndarray</code> <p>Main channel length</p> required <code>b0</code> <code>ndarray</code> <p>Routing parameter b0</p> required <code>k0</code> <code>ndarray</code> <p>Routing parameter k0</p> required <code>n</code> <code>ndarray</code> <p>Routing parameter n</p> required <code>coe</code> <code>ndarray</code> <p>Routing coefficient</p> required <code>dd</code> <code>ndarray</code> <p>Surface routing parameter dd</p> required <code>cc</code> <code>ndarray</code> <p>Surface routing parameter cc</p> required <code>ddl</code> <code>ndarray</code> <p>Subsurface routing parameter ddl</p> required <code>ccl</code> <code>ndarray</code> <p>Subsurface routing parameter ccl</p> required <code>time_interval</code> <code>float</code> <p>Time interval in hours</p> required <code>pai</code> <code>float</code> <p>Pi constant</p> required <p>Returns:</p> Type Description <p>Tuple[np.ndarray, ...]: Routing parameters including tm, k3, k3l, aa, aal, tt, ts</p> Source code in <code>hydromodel/models/dhf.py</code> <pre><code>@jit(nopython=True)\ndef calculate_dhf_routing_params(\n    ya: np.ndarray,\n    runoff_sim: np.ndarray,\n    l: np.ndarray,\n    b0: np.ndarray,\n    k0: np.ndarray,\n    n: np.ndarray,\n    coe: np.ndarray,\n    dd: np.ndarray,\n    cc: np.ndarray,\n    ddl: np.ndarray,\n    ccl: np.ndarray,\n    time_interval: float,\n    pai: float,\n):\n    \"\"\"Calculate routing parameters for DHF model.\n\n    Args:\n        ya (np.ndarray): Precedent rain parameter\n        runoff_sim (np.ndarray): Simulated runoff\n        l (np.ndarray): Main channel length\n        b0 (np.ndarray): Routing parameter b0\n        k0 (np.ndarray): Routing parameter k0\n        n (np.ndarray): Routing parameter n\n        coe (np.ndarray): Routing coefficient\n        dd (np.ndarray): Surface routing parameter dd\n        cc (np.ndarray): Surface routing parameter cc\n        ddl (np.ndarray): Subsurface routing parameter ddl\n        ccl (np.ndarray): Subsurface routing parameter ccl\n        time_interval (float): Time interval in hours\n        pai (float): Pi constant\n\n    Returns:\n        Tuple[np.ndarray, ...]: Routing parameters including tm, k3, k3l, aa, aal, tt, ts\n    \"\"\"\n    # Ensure ya &gt;= 0.5 for stability\n    ya = np.maximum(ya, 0.5)\n\n    # Calculate timing parameters\n    temp_tm = (ya + runoff_sim) ** (-k0)\n    lb = l / b0\n    tm = lb * temp_tm\n\n    tt = (n * tm).astype(np.int32)\n    ts = (coe * tm).astype(np.int32)\n\n    # Surface flow routing coefficient\n    w0 = 1.0 / time_interval\n\n    # Calculate surface routing coefficient K3\n    k3 = np.zeros_like(tm)\n    aa = np.zeros_like(tm)\n\n    mask = tm &gt; 0\n    if np.any(mask):\n        temp_aa = (pai * coe[mask]) ** (dd[mask] - 1)\n        aa[mask] = cc[mask] / (dd[mask] * temp_aa * np.tan(pai * coe[mask]))\n\n        for j in range(int(np.max(tm[mask])) + 1):\n            j_mask = mask &amp; (j &lt; tm)\n            if np.any(j_mask):\n                temp = (pai * j / tm[j_mask]) ** dd[j_mask]\n                temp1 = (np.sin(pai * j / tm[j_mask])) ** cc[j_mask]\n                k3[j_mask] += np.exp(-aa[j_mask] * temp) * temp1\n\n        k3[mask] = tm[mask] * w0 / k3[mask]\n\n    # Calculate subsurface routing coefficient K3L\n    k3l = np.zeros_like(tm)\n    aal = np.zeros_like(tm)\n\n    tt_mask = tt &gt; 0\n    if np.any(tt_mask):\n        temp_aal = (pai * coe[tt_mask] / n[tt_mask]) ** (ddl[tt_mask] - 1)\n        aal[tt_mask] = ccl[tt_mask] / (\n            ddl[tt_mask] * temp_aal * np.tan(pai * coe[tt_mask] / n[tt_mask])\n        )\n\n        for j in range(int(np.max(tt[tt_mask])) + 1):\n            j_mask = tt_mask &amp; (j &lt; tt)\n            if np.any(j_mask):\n                temp = (pai * j / tt[j_mask]) ** ddl[j_mask]\n                temp1 = (np.sin(pai * j / tt[j_mask])) ** ccl[j_mask]\n                k3l[j_mask] += np.exp(-aal[j_mask] * temp) * temp1\n\n        k3l[tt_mask] = tt[tt_mask] * w0 / k3l[tt_mask]\n\n    return tm, k3, k3l, aa, aal, tt, ts\n</code></pre>"},{"location":"models/dhf/#hydromodel.models.dhf.calculate_dhf_storage_update","title":"<code>calculate_dhf_storage_update(sa, ua, pc, rr, y, s0, u0, a)</code>","text":"<p>Update water storage states.</p> <p>Parameters:</p> Name Type Description Default <code>sa</code> <code>ndarray</code> <p>Surface water storage</p> required <code>ua</code> <code>ndarray</code> <p>Subsurface water storage</p> required <code>pc</code> <code>ndarray</code> <p>Net infiltration</p> required <code>rr</code> <code>ndarray</code> <p>Surface runoff</p> required <code>y</code> <code>ndarray</code> <p>Total flow</p> required <code>s0</code> <code>ndarray</code> <p>Surface storage capacity</p> required <code>u0</code> <code>ndarray</code> <p>Subsurface storage capacity</p> required <code>a</code> <code>ndarray</code> <p>Surface storage exponent</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Updated surface storage (sa_new) and subsurface storage (ua_new)</p> Source code in <code>hydromodel/models/dhf.py</code> <pre><code>@jit(nopython=True)\ndef calculate_dhf_storage_update(\n    sa: np.ndarray,\n    ua: np.ndarray,\n    pc: np.ndarray,\n    rr: np.ndarray,\n    y: np.ndarray,\n    s0: np.ndarray,\n    u0: np.ndarray,\n    a: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Update water storage states.\n\n    Args:\n        sa (np.ndarray): Surface water storage\n        ua (np.ndarray): Subsurface water storage\n        pc (np.ndarray): Net infiltration\n        rr (np.ndarray): Surface runoff\n        y (np.ndarray): Total flow\n        s0 (np.ndarray): Surface storage capacity\n        u0 (np.ndarray): Subsurface storage capacity\n        a (np.ndarray): Surface storage exponent\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Updated surface storage (sa_new) and subsurface storage (ua_new)\n    \"\"\"\n    # Calculate surface water storage parameters\n    temp = np.where(sa &gt; 0, (1 - sa / s0) ** (1 / a), 0.0)\n    sm = a * s0 * (1 - temp)\n\n    # Update surface storage\n    sa_new = np.where(\n        pc &gt; 0.0,\n        np.where(\n            sm + pc &lt; a * s0,\n            s0 * (1 - (1 - (sm + pc) / (a * s0)) ** a),\n            sa + pc - rr,\n        ),\n        sa,\n    )\n    sa_new = np.clip(sa_new, 0.0, s0)\n\n    # Update subsurface storage\n    ua_new = ua + rr - y\n    ua_new = np.clip(ua_new, 0.0, u0)\n\n    return sa_new, ua_new\n</code></pre>"},{"location":"models/dhf/#hydromodel.models.dhf.calculate_dhf_subsurface_flow","title":"<code>calculate_dhf_subsurface_flow(rr, ua, u0, d0, b, k2, kw, time_interval)</code>","text":"<p>Calculate subsurface flow components.</p> <p>Parameters:</p> Name Type Description Default <code>rr</code> <code>ndarray</code> <p>Surface runoff</p> required <code>ua</code> <code>ndarray</code> <p>Subsurface water storage</p> required <code>u0</code> <code>ndarray</code> <p>Subsurface storage capacity</p> required <code>d0</code> <code>ndarray</code> <p>Deep storage capacity</p> required <code>b</code> <code>ndarray</code> <p>Subsurface storage exponent</p> required <code>k2</code> <code>ndarray</code> <p>Percolation coefficient</p> required <code>kw</code> <code>ndarray</code> <p>Subsurface flow coefficient</p> required <code>time_interval</code> <code>float</code> <p>Time interval in hours</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: Total flow (y), interflow (yu), and groundwater runoff (yl)</p> Source code in <code>hydromodel/models/dhf.py</code> <pre><code>@jit(nopython=True)\ndef calculate_dhf_subsurface_flow(\n    rr: np.ndarray,\n    ua: np.ndarray,\n    u0: np.ndarray,\n    d0: np.ndarray,\n    b: np.ndarray,\n    k2: np.ndarray,\n    kw: np.ndarray,\n    time_interval: float,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Calculate subsurface flow components.\n\n    Args:\n        rr (np.ndarray): Surface runoff\n        ua (np.ndarray): Subsurface water storage\n        u0 (np.ndarray): Subsurface storage capacity\n        d0 (np.ndarray): Deep storage capacity\n        b (np.ndarray): Subsurface storage exponent\n        k2 (np.ndarray): Percolation coefficient\n        kw (np.ndarray): Subsurface flow coefficient\n        time_interval (float): Time interval in hours\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: Total flow (y), interflow (yu), and groundwater runoff (yl)\n    \"\"\"\n    # Calculate subsurface flow parameters\n    temp = np.where(ua &gt; 0, (1 - ua / u0) ** (1 / b), 0.0)\n    un = b * u0 * (1 - temp)\n    temp = np.where(ua &gt; 0, (1 - ua / u0) ** (u0 / (b * d0)), 0.0)\n    dn = b * d0 * (1 - temp)\n\n    z1 = 1 - np.exp(-k2 * time_interval * u0 / d0)\n    z2 = 1 - np.exp(-k2 * time_interval)\n\n    # Calculate total flow\n    y = np.where(\n        rr + z2 * un &lt; z2 * b * u0,\n        rr\n        + z2 * (ua - u0)\n        + z2 * u0 * (1 - (z2 * un + rr) / (z2 * b * u0)) ** b,\n        rr + z2 * (ua - u0),\n    )\n\n    # Calculate interflow\n    temp = np.where(ua &gt; 0, (1 - ua / u0) ** (u0 / d0), 0.0)\n    yu = np.where(\n        z1 * dn + rr &lt; z1 * b * d0,\n        rr\n        - z1 * d0 * temp\n        + z1 * d0 * (1 - (z1 * dn + rr) / (z1 * b * d0)) ** b,\n        rr - z1 * d0 * temp,\n    )\n\n    # Calculate groundwater runoff\n    yl = (y - yu) * kw\n\n    return y, yu, yl\n</code></pre>"},{"location":"models/dhf/#hydromodel.models.dhf.calculate_dhf_surface_runoff","title":"<code>calculate_dhf_surface_runoff(pe, sa, s0, a, g)</code>","text":"<p>Calculate surface runoff and impervious area runoff.</p> <p>Parameters:</p> Name Type Description Default <code>pe</code> <code>ndarray</code> <p>Net precipitation</p> required <code>sa</code> <code>ndarray</code> <p>Surface water storage</p> required <code>s0</code> <code>ndarray</code> <p>Surface storage capacity</p> required <code>a</code> <code>ndarray</code> <p>Surface storage exponent</p> required <code>g</code> <code>ndarray</code> <p>Impervious area ratio</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: Impervious area runoff (y0), net infiltration (pc), and surface runoff (rr)</p> Source code in <code>hydromodel/models/dhf.py</code> <pre><code>@jit(nopython=True)\ndef calculate_dhf_surface_runoff(\n    pe: np.ndarray,\n    sa: np.ndarray,\n    s0: np.ndarray,\n    a: np.ndarray,\n    g: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Calculate surface runoff and impervious area runoff.\n\n    Args:\n        pe (np.ndarray): Net precipitation\n        sa (np.ndarray): Surface water storage\n        s0 (np.ndarray): Surface storage capacity\n        a (np.ndarray): Surface storage exponent\n        g (np.ndarray): Impervious area ratio\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: Impervious area runoff (y0), net infiltration (pc), and surface runoff (rr)\n    \"\"\"\n    y0 = g * pe  # impervious area runoff\n    pc = pe - y0  # net infiltration\n\n    # Calculate surface water storage contribution (vectorized)\n    temp = np.where(sa &gt; 0, (1 - sa / s0) ** (1 / a), 0.0)\n    sm = a * s0 * (1 - temp)\n\n    # Calculate runoff from surface storage\n    rr = np.where(\n        pc &gt; 0.0,\n        np.where(\n            sm + pc &lt; a * s0,\n            pc + sa - s0 + s0 * (1 - (sm + pc) / (a * s0)) ** a,\n            pc - (s0 - sa),\n        ),\n        0.0,\n    )\n\n    return y0, pc, rr\n</code></pre>"},{"location":"models/dhf/#hydromodel.models.dhf.dhf","title":"<code>dhf(p_and_e, parameters, warmup_length=365, return_state=False, return_warmup_states=False, normalized_params='auto', **kwargs)</code>","text":"<p>Vectorized DHF (Dahuofang) hydrological model - fully parallelized version.</p> <p>This function implements the DHF model with full NumPy vectorization, processing all basins simultaneously using [seq, basin, feature] tensor operations.</p> <p>Parameters:</p> Name Type Description Default <code>p_and_e</code> <code>ndarray</code> <p>Precipitation and potential evapotranspiration, 3-dim: [time, basin, feature=2] where feature=0 is precipitation, feature=1 is potential evapotranspiration</p> required <code>parameters</code> <code>ndarray</code> <p>Model parameters, 2-dim: [basin, parameter] Parameters: [S0, U0, D0, KC, KW, K2, KA, G, A, B, B0, K0, N, DD, CC, COE, DDL, CCL]</p> required <code>warmup_length</code> <code>int</code> <p>The length of warmup period (default: 365)</p> <code>365</code> <code>return_state</code> <code>bool</code> <p>If True, return internal state variables (default: False)</p> <code>False</code> <code>return_warmup_states</code> <code>bool</code> <p>If True, return initial states after warmup period (default: False) Returns a dict with keys: \"sa0\", \"ua0\", \"ya0\" containing initial states</p> <code>False</code> <code>normalized_params</code> <code>Union[bool, str]</code> <p>Parameter format specification: - \"auto\": automatically detect parameter format (default) - True: parameters are normalized (0-1 range), convert to original scale - False: parameters are already in original scale, use as-is</p> <code>'auto'</code> <code>**kwargs</code> <p>Additional keyword arguments, including: - time_interval_hours (default: 3.0) - main_river_length (default: None) means length of the main channel (km), for example, dahuofang's is 155.763 - basin_area (default: None) means basin area (km^2), for example, dahuofang's is 5482.0 - initial_states (default: None) dict to override specific initial state values after warmup,   e.g., {\"sa0\": 10, \"ya0\": 15} will set sa0=10 and ya0=15 for all basins after warmup</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, Dict[str, ndarray]], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, Dict[str, ndarray]]]</code> <p>Union[np.ndarray, Tuple]: Depends on return_state and return_warmup_states parameters:</p> <code>Union[ndarray, Tuple[ndarray, Dict[str, ndarray]], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, Dict[str, ndarray]]]</code> <ul> <li>return_state=False, return_warmup_states=False: QSim array [time, basin, 1]</li> </ul> <code>Union[ndarray, Tuple[ndarray, Dict[str, ndarray]], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, Dict[str, ndarray]]]</code> <ul> <li>return_state=False, return_warmup_states=True: (QSim, warmup_states_dict) where warmup_states_dict contains</li> </ul> <code>Union[ndarray, Tuple[ndarray, Dict[str, ndarray]], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, Dict[str, ndarray]]]</code> <ul> <li>return_state=True, return_warmup_states=False: (QSim, runoffSim, y0, yu, yl, y, sa, ua, ya)</li> </ul> <code>Union[ndarray, Tuple[ndarray, Dict[str, ndarray]], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray], Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, ndarray, Dict[str, ndarray]]]</code> <ul> <li>return_state=True, return_warmup_states=True: (QSim, runoffSim, y0, yu, yl, y, sa, ua, ya, warmup_states_dict)</li> </ul> Source code in <code>hydromodel/models/dhf.py</code> <pre><code>def dhf(\n    p_and_e: np.ndarray,\n    parameters: np.ndarray,\n    warmup_length: int = 365,\n    return_state: bool = False,\n    return_warmup_states: bool = False,\n    normalized_params: Union[bool, str] = \"auto\",\n    **kwargs,\n) -&gt; Union[\n    np.ndarray,  # return_state=False, return_warmup_states=False\n    Tuple[\n        np.ndarray, Dict[str, np.ndarray]\n    ],  # return_state=False, return_warmup_states=True\n    Tuple[\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n    ],  # return_state=True, return_warmup_states=False\n    Tuple[\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        np.ndarray,\n        Dict[str, np.ndarray],\n    ],  # return_state=True, return_warmup_states=True\n]:\n    \"\"\"Vectorized DHF (Dahuofang) hydrological model - fully parallelized version.\n\n    This function implements the DHF model with full NumPy vectorization,\n    processing all basins simultaneously using [seq, basin, feature] tensor operations.\n\n    Args:\n        p_and_e (np.ndarray): Precipitation and potential evapotranspiration, 3-dim: [time, basin, feature=2]\n            where feature=0 is precipitation, feature=1 is potential evapotranspiration\n        parameters (np.ndarray): Model parameters, 2-dim: [basin, parameter]\n            Parameters: [S0, U0, D0, KC, KW, K2, KA, G, A, B, B0, K0, N, DD, CC, COE, DDL, CCL]\n        warmup_length (int, optional): The length of warmup period (default: 365)\n        return_state (bool, optional): If True, return internal state variables (default: False)\n        return_warmup_states (bool, optional): If True, return initial states after warmup period (default: False)\n            Returns a dict with keys: \"sa0\", \"ua0\", \"ya0\" containing initial states\n        normalized_params (Union[bool, str], optional): Parameter format specification:\n            - \"auto\": automatically detect parameter format (default)\n            - True: parameters are normalized (0-1 range), convert to original scale\n            - False: parameters are already in original scale, use as-is\n        **kwargs: Additional keyword arguments, including:\n            - time_interval_hours (default: 3.0)\n            - main_river_length (default: None) means length of the main channel (km), for example, dahuofang's is 155.763\n            - basin_area (default: None) means basin area (km^2), for example, dahuofang's is 5482.0\n            - initial_states (default: None) dict to override specific initial state values after warmup,\n              e.g., {\"sa0\": 10, \"ya0\": 15} will set sa0=10 and ya0=15 for all basins after warmup\n\n    Returns:\n        Union[np.ndarray, Tuple]: Depends on return_state and return_warmup_states parameters:\n\n        - return_state=False, return_warmup_states=False:\n            QSim array [time, basin, 1]\n\n        - return_state=False, return_warmup_states=True:\n            (QSim, warmup_states_dict) where warmup_states_dict contains\n            {\"sa0\": [basin], \"ua0\": [basin], \"ya0\": [basin]}\n\n        - return_state=True, return_warmup_states=False:\n            (QSim, runoffSim, y0, yu, yl, y, sa, ua, ya)\n\n        - return_state=True, return_warmup_states=True:\n            (QSim, runoffSim, y0, yu, yl, y, sa, ua, ya, warmup_states_dict)\n    \"\"\"\n\n    # Get data dimensions\n    time_steps, num_basins, _ = p_and_e.shape\n    time_interval = kwargs.get(\"time_interval_hours\", 3.0)\n    pai = np.pi\n    l = kwargs.get(\"main_river_length\", None)  # km\n    f = kwargs.get(\"basin_area\", None)  # km^2\n\n    if l is None or f is None:\n        raise ValueError(\n            \"main_river_length (l) and basin_area (f) must be provided for DHF model\"\n        )\n\n    # Process parameters using unified parameter handling\n    processed_parameters = parameters.copy()\n    if normalized_params != False:\n        model_param_dict = MODEL_PARAM_DICT.get(\"dhf\")\n        if model_param_dict is not None:\n            param_ranges = model_param_dict[\"param_range\"]\n            processed_parameters = process_parameters(\n                parameters, param_ranges, normalized=normalized_params\n            )\n\n    # Extract parameters - all are [basin] arrays\n    s0 = processed_parameters[:, 0]  # Surface storage capacity\n    u0 = processed_parameters[:, 1]  # Subsurface storage capacity\n    d0 = processed_parameters[:, 2]  # Deep storage capacity\n    kc = processed_parameters[:, 3]  # Evaporation coefficient\n    kw = processed_parameters[:, 4]  # Subsurface flow coefficient\n    k2 = processed_parameters[:, 5]  # Percolation coefficient\n    ka = processed_parameters[:, 6]  # Total runoff adjustment coefficient\n    g = processed_parameters[:, 7]  # Impervious area ratio\n    a = processed_parameters[:, 8]  # Surface storage exponent\n    b = processed_parameters[:, 9]  # Subsurface storage exponent\n    b0 = processed_parameters[:, 10]  # Routing parameter\n    k0 = processed_parameters[:, 11]  # Routing parameter\n    n = processed_parameters[:, 12]  # Routing parameter\n    dd = processed_parameters[:, 13]  # Surface routing parameter\n    cc = processed_parameters[:, 14]  # Surface routing parameter\n    coe = processed_parameters[:, 15]  # Routing parameter\n    ddl = processed_parameters[:, 16]  # Subsurface routing parameter\n    ccl = processed_parameters[:, 17]  # Subsurface routing parameter\n\n    # Handle warmup period\n    if warmup_length &gt; 0:\n        p_and_e_warmup = p_and_e[0:warmup_length, :, :]\n        # Remove initial_states from kwargs for warmup period to avoid applying override during warmup\n        warmup_kwargs = {\n            k: v for k, v in kwargs.items() if k != \"initial_states\"\n        }\n        *_, sa, ua, ya = dhf(\n            p_and_e_warmup,\n            parameters,\n            warmup_length=0,\n            return_state=True,\n            normalized_params=False,  # Already processed\n            **warmup_kwargs,\n        )\n        sa0 = sa[-1, :, 0].copy()\n        ua0 = ua[-1, :, 0].copy()\n        ya0 = ya[-1, :, 0].copy()\n    else:\n        # Default initial states\n        sa0 = np.zeros(s0.shape)\n        ua0 = np.zeros(u0.shape)\n        # just use d0's shape, ya0 is not d0, it is Pa, while d0 is the deep storage capacity\n        ya0 = np.full(d0.shape, 0.5)\n\n    # Apply initial state overrides if provided (only after warmup in main call)\n    initial_states = kwargs.get(\"initial_states\", None)\n    if initial_states is not None:\n        # Only apply initial_states when we just finished a warmup period\n        if \"sa0\" in initial_states:\n            sa0.fill(initial_states[\"sa0\"])\n        if \"ua0\" in initial_states:\n            ua0.fill(initial_states[\"ua0\"])\n        if \"ya0\" in initial_states:\n            ya0.fill(initial_states[\"ya0\"])\n\n    # Save warmup states before applying overrides (for return_warmup_states)\n    # NOTE: this part must be set after the initial_states override, otherwise override initial states will be ignored\n    warmup_states = None\n    if return_warmup_states:\n        warmup_states = {\n            \"sa0\": sa0.copy(),  # [basin] array\n            \"ua0\": ua0.copy(),  # [basin] array\n            \"ya0\": ya0.copy(),  # [basin] array\n        }\n\n    inputs = p_and_e[warmup_length:, :, :]\n    # Get actual time steps after warmup\n    actual_time_steps = inputs.shape[0]\n\n    # Initialize state and output arrays - [time, basin]\n    sa = np.zeros((actual_time_steps, num_basins))\n    ua = np.zeros((actual_time_steps, num_basins))\n    ya = np.zeros((actual_time_steps, num_basins))\n    # to store the accumulated deficit\n    ebs = np.zeros((actual_time_steps, num_basins))\n\n    # Initialize output arrays\n    runoff_sim = np.zeros((actual_time_steps, num_basins))\n    q_sim = np.zeros((actual_time_steps, num_basins))\n    y0_out = np.zeros((actual_time_steps, num_basins))\n    yu_out = np.zeros((actual_time_steps, num_basins))\n    yl_out = np.zeros((actual_time_steps, num_basins))\n    y_out = np.zeros((actual_time_steps, num_basins))\n\n    # Main time loop - DHF generation (runoff production)\n    for i in range(actual_time_steps):\n        # Current precipitation and PET for all basins\n        prcp = inputs[i, :, 0]\n        pet = inputs[i, :, 1]\n        if i == 0:\n            eb = np.zeros(kc.shape)\n        else:\n            sa0 = sa[i - 1, :]\n            ua0 = ua[i - 1, :]\n            # NOTE: Because Chu version init eb as 0 at every time step, we keep the same now\n            # if we want to make it same as the book, we need to value ebs after a time step's calculation\n            eb = ebs[i - 1, :]\n\n        # Limit current states\n        sa0 = np.minimum(sa0, s0)\n        ua0 = np.minimum(ua0, u0)\n\n        # Calculate evapotranspiration and net precipitation (vectorized)\n        edt = kc * pet\n        pe = prcp - edt\n        # Surface runoff calculation (vectorized)\n        y0 = g * pe  # impervious area runoff\n        pc = pe - y0  # net infiltration\n        # Process based on whether we have net precipitation or evaporation\n        # Actually, we should use pe &gt; 0.0, but we use pc &gt; 0.0 to make it same as Chu's version\n        # as g&lt;1, hence pe&gt;0 means pc&gt;0, so it is fine.\n        net_precip_mask = pc &gt; 0.0\n\n        # For basins with net precipitation (pe &gt; 0) - vectorized operations\n        if np.any(net_precip_mask):\n            # Apply mask to get relevant basin data\n            sa_pos = sa0[net_precip_mask]\n            ua_pos = ua0[net_precip_mask]\n            s0_pos = s0[net_precip_mask]\n            u0_pos = u0[net_precip_mask]\n            d0_pos = d0[net_precip_mask]\n            a_pos = a[net_precip_mask]\n            b_pos = b[net_precip_mask]\n            k2_pos = k2[net_precip_mask]\n            kw_pos = kw[net_precip_mask]\n\n            # Surface water storage calculation\n            temp = (1 - sa_pos / s0_pos) ** (1 / a_pos)\n            sm = a_pos * s0_pos * (1 - temp)\n\n            # Calculate surface runoff\n            rr = np.where(\n                pc &gt; 0.0,\n                np.where(\n                    sm + pc &lt; a_pos * s0_pos,\n                    pc\n                    + sa_pos\n                    - s0_pos\n                    + s0_pos * (1 - (sm + pc) / (a_pos * s0_pos)) ** a_pos,\n                    pc - (s0_pos - sa_pos),\n                ),\n                0.0,\n            )\n\n            # Subsurface flow calculation (vectorized)\n            temp = (1 - ua_pos / u0_pos) ** (1 / b_pos)\n            un = b_pos * u0_pos * (1 - temp)\n            temp = (1 - ua_pos / u0_pos) ** (u0_pos / (b_pos * d0_pos))\n            dn = b_pos * d0_pos * (1 - temp)\n\n            z1 = 1 - np.exp(-k2_pos * time_interval * u0_pos / d0_pos)\n            z2 = 1 - np.exp(-k2_pos * time_interval)\n\n            # Calculate total flow\n            y = np.where(\n                rr + z2 * un &lt; z2 * b_pos * u0_pos,\n                rr\n                + z2 * (ua_pos - u0_pos)\n                + z2\n                * u0_pos\n                * (1 - (z2 * un + rr) / (z2 * b_pos * u0_pos)) ** b_pos,\n                rr + z2 * (ua_pos - u0_pos),\n            )\n\n            # Calculate interflow\n            temp = (1 - ua_pos / u0_pos) ** (u0_pos / d0_pos)\n            yu = np.where(\n                z1 * dn + rr &lt; z1 * b_pos * d0_pos,\n                rr\n                - z1 * d0_pos * temp\n                + z1\n                * d0_pos\n                * (1 - (z1 * dn + rr) / (z1 * b_pos * d0_pos)) ** b_pos,\n                rr - z1 * d0_pos * temp,\n            )\n\n            # Calculate groundwater runoff\n            yl = (y - yu) * kw_pos\n\n            # Update storage states (vectorized)\n            sa_new = np.where(\n                pc &gt; 0.0,\n                np.where(\n                    sm + pc &lt; a_pos * s0_pos,\n                    s0_pos * (1 - (1 - (sm + pc) / (a_pos * s0_pos)) ** a_pos),\n                    sa_pos + pc - rr,\n                ),\n                sa_pos,\n            )\n            sa_new = np.clip(sa_new, 0.0, s0_pos)\n\n            ua_new = ua_pos + rr - y\n            ua_new = np.clip(ua_new, 0.0, u0_pos)\n            # eb will be set to 0 when pc &gt; 0\n            eb = np.where(pc &gt; 0.0, 0.0, eb)\n\n            # Store results for basins with net precipitation\n            y0_out[i, net_precip_mask] = y0[net_precip_mask]\n            yu_out[i, net_precip_mask] = yu\n            yl_out[i, net_precip_mask] = yl\n            y_out[i, net_precip_mask] = y\n            sa[i, net_precip_mask] = sa_new\n            ua[i, net_precip_mask] = ua_new\n\n        # For basins with evaporation deficit (pe &lt;= 0) - vectorized operations\n        evap_mask = ~net_precip_mask\n        if np.any(evap_mask):\n            prcp_neg = prcp[evap_mask]\n            edt_neg = edt[evap_mask]\n            sa_neg = sa0[evap_mask]\n            ua_neg = ua0[evap_mask]\n            s0_neg = s0[evap_mask]\n            u0_neg = u0[evap_mask]\n            a_neg = a[evap_mask]\n\n            ec = edt_neg - prcp_neg\n            eb = eb + ec  # accumulated deficit\n\n            # Calculate surface evaporation (vectorized)\n            temp1 = (1 - (eb - ec) / (a_neg * s0_neg)) ** a_neg\n            temp2 = (1 - eb / (a_neg * s0_neg)) ** a_neg\n\n            eu = np.where(\n                (eb / (a_neg * s0_neg) &lt;= 0.999999)\n                &amp; ((eb - ec) / (a_neg * s0_neg) &lt;= 0.999999),\n                s0_neg * (temp1 - temp2),\n                np.where(\n                    (eb / (a_neg * s0_neg) &gt;= 1.00001)\n                    &amp; ((eb - ec) / (a_neg * s0_neg) &lt;= 0.999999),\n                    s0_neg * temp1,\n                    0.00001,\n                ),\n            )\n\n            # Update storages after evaporation\n            el = np.where(\n                sa_neg - eu &lt; 0.0,\n                (ec - sa_neg) * ua_neg / u0_neg,\n                (ec - eu) * ua_neg / u0_neg,\n            )\n            sa_new = np.where(sa_neg - eu &lt; 0.0, 0.0, sa_neg - eu)\n            ua_new = ua_neg - el\n            ua_new = np.maximum(ua_new, 0.0)\n\n            # Set runoff components to zero for evaporation basins\n            y0_out[i, evap_mask] = 0.0\n            yu_out[i, evap_mask] = 0.0\n            yl_out[i, evap_mask] = 0.0\n            y_out[i, evap_mask] = 0.0\n            sa[i, evap_mask] = sa_new\n            ua[i, evap_mask] = ua_new\n\n        # Ensure states are within bounds\n        sa[i, :] = np.clip(sa[i, :], 0.0, s0)\n        ua[i, :] = np.clip(ua[i, :], 0.0, u0)\n\n    # get total runoff\n    runoff_sim = np.maximum(y_out + y0_out, 0.0)\n\n    # DHF routing calculation - vectorized version without basin loop\n    qs = np.zeros((actual_time_steps, num_basins))\n    ql = np.zeros((actual_time_steps, num_basins))\n\n    w0 = f / (3.6 * time_interval)  # tmp value used for testing\n\n    # Main time loop for routing (before convolution, getting ya)\n    for i in range(actual_time_steps):\n        # ya is precedent rain -- Pa\n        if i &gt; 0:\n            ya0 = ya[i - 1, :]\n        ya[i, :] = np.maximum((ya0 + runoff_sim[i, :]) * ka, 0.0)\n        # Get current state for all basins (vectorized)\n        # here we keep the same as Chu's version\n        # You can see that when getting temp_tm, we use ya_val + runoff_sim[i, :]\n        ya_val = np.maximum(ya0, 0.5)  # Ensure stability for all basins\n        # yl is rL in Chu's version; it's subsurface reservoir's infiltration\n        rl = yl_out[i, :]\n        # keep same as Chu's version\n        rl = np.maximum(rl, 0.0)\n\n        # Calculate routing parameters for all basins (vectorized)\n        temp_tm = (ya_val + runoff_sim[i, :]) ** (-k0)\n        lb = l / b0\n        tm = lb * temp_tm\n\n        # Time indices for all basins (vectorized)\n        tt = (n * tm).astype(int)\n        ts = (coe * tm).astype(int)\n\n        # Surface routing coefficient calculation (vectorized)\n        k3 = np.zeros(num_basins)\n        aa_val = np.zeros(num_basins)\n\n        # Calculate routing coefficients for all basins\n        temp_aa = (pai * coe) ** (dd - 1)\n        aa_val = cc / (dd * temp_aa * np.tan(pai * coe))\n\n        # Calculate k3 for all basins\n        max_tm = int(np.ceil(np.max(tm)))\n        # we use max_tm for all basins\n        for j in range(max_tm):\n            # Then, we only process basins for its periods, where j &lt; tm\n            j_mask = j &lt; tm\n            if np.any(j_mask):\n                temp = (pai * j / tm[j_mask]) ** dd[j_mask]\n                temp1 = (np.sin(pai * j / tm[j_mask])) ** cc[j_mask]\n                k3[j_mask] += np.exp(-aa_val[j_mask] * temp) * temp1\n\n        # Final k3 calculation with division check\n        nonzero_k3 = k3 != 0\n        if np.any(nonzero_k3):\n            k3[nonzero_k3] = tm[nonzero_k3] * w0 / k3[nonzero_k3]\n\n        # Subsurface routing coefficient calculation (vectorized)\n        k3l = np.zeros(num_basins)\n        aal_val = np.zeros(num_basins)\n\n        # Calculate subsurface routing coefficients for all basins\n        temp_aal = (pai * coe / n) ** (ddl - 1)\n        aal_val = ccl / (ddl * temp_aal * np.tan(pai * coe / n))\n\n        # Calculate k3l for all basins\n        max_tt = int(np.ceil(np.max(tt)))\n        for j in range(max_tt):\n            # Only process basins where j &lt; tt\n            j_mask = j &lt; tt\n            if np.any(j_mask):\n                temp = (pai * j / tt[j_mask]) ** ddl[j_mask]\n                temp1 = (np.sin(pai * j / tt[j_mask])) ** ccl[j_mask]\n                k3l[j_mask] += np.exp(-aal_val[j_mask] * temp) * temp1\n\n        # Final k3l calculation with division check\n        nonzero_k3l = k3l != 0\n        if np.any(nonzero_k3l):\n            k3l[nonzero_k3l] = tt[nonzero_k3l] * w0 / k3l[nonzero_k3l]\n\n        # Calculate tl for all basins (vectorized)\n        tl = np.maximum(tt + ts - 1, 0)\n\n        # Process routing time steps (still need this loop for convolution)\n        max_tl = int(np.ceil(np.max(tl)))\n        for j in range(max_tl):\n            # Check bounds to prevent overflow\n            valid_idx = i + j &lt; actual_time_steps\n            if not valid_idx:\n                break\n\n            # Surface routing calculation (vectorized)\n            q_surface = np.zeros(num_basins)\n            # Subsurface routing calculation (vectorized)\n            q_subsurface = np.zeros(num_basins)\n            _j_mask = j &lt;= tl\n            if np.any(_j_mask):\n                temp0 = pai * j / tm[_j_mask]\n                temp1 = temp0 ** dd[_j_mask]\n                temp2 = np.exp(-aa_val[_j_mask] * temp1)\n                temp3 = (np.sin(temp0)) ** cc[_j_mask]\n                q_surface[_j_mask] = (\n                    (runoff_sim[i, _j_mask] - rl[_j_mask])\n                    * k3[_j_mask]\n                    / tm[_j_mask]\n                    * temp2\n                    * temp3\n                )\n                # Handle NaN values\n                q_surface[np.isnan(q_surface)] = 0.0\n\n                temp00 = pai * (j - ts[_j_mask]) / tt[_j_mask]\n                temp10 = temp00 ** ddl[_j_mask]\n                temp20 = np.exp(-aal_val[_j_mask] * temp10)\n                temp30 = (np.sin(temp00)) ** ccl[_j_mask]\n                q_subsurface[_j_mask] = (\n                    rl[_j_mask] * k3l[_j_mask] / tt[_j_mask] * temp20 * temp30\n                )\n\n            # Add contributions based on timing conditions (vectorized)\n            # Case 1: j &lt;= tm and j &lt;= ts\n            mask1 = (j &lt;= tm) &amp; (j &lt;= ts)\n            qs[i + j, mask1] += q_surface[mask1]\n\n            # Case 2: j &lt;= tm and j &gt; ts\n            mask2 = (j &lt;= tm) &amp; (j &gt; ts)\n            qs[i + j, mask2] += q_surface[mask2]\n            ql[i + j, mask2] += q_subsurface[mask2]\n\n            # Case 3: j &gt; tm\n            mask3 = j &gt; tm\n            ql[i + j, mask3] += q_subsurface[mask3]\n\n    # Total discharge\n    q_sim = qs + ql\n    q_sim = np.maximum(q_sim, 0.0)\n\n    # seq, batch, feature\n    q_sim = np.expand_dims(q_sim, axis=2)\n    runoff_sim = np.expand_dims(runoff_sim, axis=2)\n    y0_out = np.expand_dims(y0_out, axis=2)\n    yu_out = np.expand_dims(yu_out, axis=2)\n    yl_out = np.expand_dims(yl_out, axis=2)\n    y_out = np.expand_dims(y_out, axis=2)\n    sa = np.expand_dims(sa, axis=2)\n    ua = np.expand_dims(ua, axis=2)\n    ya = np.expand_dims(ya, axis=2)\n\n    if return_state:\n        result = (\n            q_sim,\n            runoff_sim,\n            y0_out,\n            yu_out,\n            yl_out,\n            y_out,\n            sa,\n            ua,\n            ya,\n        )\n        # If warmup states are requested, add them as the last element\n        if return_warmup_states and warmup_states is not None:\n            return result + (warmup_states,)\n        else:\n            return result\n    else:\n        # For non-state return, only return warmup states if specifically requested\n        if return_warmup_states and warmup_states is not None:\n            return q_sim, warmup_states\n        else:\n            return q_sim\n</code></pre>"},{"location":"models/xaj/","title":"XinAnJiang (XAJ) Model","text":"<p>XinAnJiang (XAJ) model is one of the most famous conceptual hydrological models, especially popular in Southern China. It was developed by Prof. Renjun Zhao and has been widely used for rainfall-runoff simulation and flood forecasting.</p>"},{"location":"models/xaj/#model-structure","title":"Model Structure","text":"<p>The XAJ model consists of three main components:</p> <ol> <li>Evapotranspiration Module: Three-layer evaporation model</li> <li>Runoff Generation Module: Soil moisture accounting and runoff calculation  </li> <li>Flow Routing Module: Unit hydrograph and flow routing</li> </ol> <p></p>"},{"location":"models/xaj/#api-reference","title":"API Reference","text":"<p>Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"models/xaj/#hydromodel.models.xaj.calculate_evap","title":"<code>calculate_evap(lm, c, wu0, wl0, prcp, pet)</code>","text":"<p>Three-layers evaporation model from \"Watershed Hydrologic Simulation\" written by Prof. RenJun Zhao.</p> <p>The book is Chinese, and its name is \u300a\u6d41\u57df\u6c34\u6587\u6a21\u62df\u300b; The three-layers evaporation model is described in Page 76; The method is same with that in Page 22-23 in \"Hydrologic Forecasting (5-th version)\" written by Prof. Weimin Bao. This book's Chinese name is \u300a\u6c34\u6587\u9884\u62a5\u300b</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <p>Average soil moisture storage capacity of lower layer</p> required <code>c</code> <p>Coefficient of deep layer</p> required <code>wu0</code> <p>Initial soil moisture of upper layer; update in each time step</p> required <code>wl0</code> <p>Initial soil moisture of lower layer; update in each time step</p> required <code>prcp</code> <p>Basin mean precipitation</p> required <code>pet</code> <p>Potential evapotranspiration</p> required <p>Returns:</p> Type Description <code>tuple[array, array, array]</code> <p>tuple[np.array, np.array, np.array]: eu/el/ed are evaporation from upper/lower/deeper layer, respectively</p> Source code in <code>hydromodel/models/xaj.py</code> <pre><code>def calculate_evap(\n    lm, c, wu0, wl0, prcp, pet\n) -&gt; tuple[np.array, np.array, np.array]:\n    \"\"\"Three-layers evaporation model from \"Watershed Hydrologic Simulation\" written by Prof. RenJun Zhao.\n\n    The book is Chinese, and its name is \u300a\u6d41\u57df\u6c34\u6587\u6a21\u62df\u300b;\n    The three-layers evaporation model is described in Page 76;\n    The method is same with that in Page 22-23 in \"Hydrologic Forecasting (5-th version)\" written by Prof. Weimin Bao.\n    This book's Chinese name is \u300a\u6c34\u6587\u9884\u62a5\u300b\n\n    Args:\n        lm: Average soil moisture storage capacity of lower layer\n        c: Coefficient of deep layer\n        wu0: Initial soil moisture of upper layer; update in each time step\n        wl0: Initial soil moisture of lower layer; update in each time step\n        prcp: Basin mean precipitation\n        pet: Potential evapotranspiration\n\n    Returns:\n        tuple[np.array, np.array, np.array]: eu/el/ed are evaporation from upper/lower/deeper layer, respectively\n    \"\"\"\n    eu = np.where(wu0 + prcp &gt;= pet, pet, wu0 + prcp)\n    ed = np.where(\n        (wl0 &lt; c * lm) &amp; (wl0 &lt; c * (pet - eu)), c * (pet - eu) - wl0, 0.0\n    )\n    el = np.where(\n        wu0 + prcp &gt;= pet,\n        0.0,\n        np.where(\n            wl0 &gt;= c * lm,\n            (pet - eu) * wl0 / lm,\n            np.where(wl0 &gt;= c * (pet - eu), c * (pet - eu), wl0),\n        ),\n    )\n    return eu, el, ed\n</code></pre>"},{"location":"models/xaj/#hydromodel.models.xaj.calculate_prcp_runoff","title":"<code>calculate_prcp_runoff(b, im, wm, w0, pe)</code>","text":"<p>Calculates the amount of runoff generated from rainfall after entering the underlying surface.</p> <p>Same in \"Watershed Hydrologic Simulation\" and \"Hydrologic Forecasting (5-th version)\"</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <p>B; exponent coefficient</p> required <code>im</code> <p>IMP; imperiousness coefficient</p> required <code>wm</code> <p>Average soil moisture storage capacity</p> required <code>w0</code> <p>Initial soil moisture</p> required <code>pe</code> <p>Net precipitation</p> required <p>Returns:</p> Type Description <code>tuple[array, array]</code> <p>tuple[np.array, np.array]: r -- runoff; r_im -- runoff of impervious part</p> Source code in <code>hydromodel/models/xaj.py</code> <pre><code>def calculate_prcp_runoff(b, im, wm, w0, pe) -&gt; tuple[np.array, np.array]:\n    \"\"\"Calculates the amount of runoff generated from rainfall after entering the underlying surface.\n\n    Same in \"Watershed Hydrologic Simulation\" and \"Hydrologic Forecasting (5-th version)\"\n\n    Args:\n        b: B; exponent coefficient\n        im: IMP; imperiousness coefficient\n        wm: Average soil moisture storage capacity\n        w0: Initial soil moisture\n        pe: Net precipitation\n\n    Returns:\n        tuple[np.array, np.array]: r -- runoff; r_im -- runoff of impervious part\n    \"\"\"\n    wmm = wm * (1.0 + b)\n    a = wmm * (1.0 - (1.0 - w0 / wm) ** (1.0 / (1.0 + b)))\n    if np.isnan(a).any():\n        raise ArithmeticError(\n            \"Please check if w0&gt;wm or b is a negative value!\"\n        )\n    r_cal = np.where(\n        pe &gt; 0.0,\n        np.where(\n            pe + a &lt; wmm,\n            # 1e-5 is a precision which we set to guarantee float's calculation is correct\n            pe\n            - (wm - w0)\n            + wm * (1.0 - np.minimum(a + pe, wmm) / wmm) ** (1.0 + b),\n            pe - (wm - w0),\n        ),\n        np.full(pe.shape, 0.0),\n    )\n    r = np.maximum(r_cal, 0.0)\n    # separate impervious part with the other\n    r_im_cal = pe * im\n    r_im = np.maximum(r_im_cal, 0.0)\n    return r, r_im\n</code></pre>"},{"location":"models/xaj/#hydromodel.models.xaj.calculate_w_storage","title":"<code>calculate_w_storage(um, lm, dm, wu0, wl0, wd0, eu, el, ed, pe, r)</code>","text":"<p>Update the soil moisture values of the three layers.</p> <p>According to the equation 2.60 in the book\u300a\u6c34\u6587\u9884\u62a5\u300b</p> <p>Parameters:</p> Name Type Description Default <code>um</code> <p>Average soil moisture storage capacity of the upper layer</p> required <code>lm</code> <p>Average soil moisture storage capacity of the lower layer</p> required <code>dm</code> <p>Average soil moisture storage capacity of the deep layer</p> required <code>wu0</code> <p>Initial values of soil moisture in upper layer</p> required <code>wl0</code> <p>Initial values of soil moisture in lower layer</p> required <code>wd0</code> <p>Initial values of soil moisture in deep layer</p> required <code>eu</code> <p>Evaporation of the upper layer; it isn't used in this function</p> required <code>el</code> <p>Evaporation of the lower layer</p> required <code>ed</code> <p>Evaporation of the deep layer</p> required <code>pe</code> <p>Net precipitation; it is able to be negative value in this function</p> required <code>r</code> <p>Runoff</p> required <p>Returns:</p> Type Description <code>tuple[array, array, array]</code> <p>tuple[np.array, np.array, np.array]: wu,wl,wd -- soil moisture in upper, lower and deep layer</p> Source code in <code>hydromodel/models/xaj.py</code> <pre><code>def calculate_w_storage(\n    um, lm, dm, wu0, wl0, wd0, eu, el, ed, pe, r\n) -&gt; tuple[np.array, np.array, np.array]:\n    \"\"\"Update the soil moisture values of the three layers.\n\n    According to the equation 2.60 in the book\u300a\u6c34\u6587\u9884\u62a5\u300b\n\n    Args:\n        um: Average soil moisture storage capacity of the upper layer\n        lm: Average soil moisture storage capacity of the lower layer\n        dm: Average soil moisture storage capacity of the deep layer\n        wu0: Initial values of soil moisture in upper layer\n        wl0: Initial values of soil moisture in lower layer\n        wd0: Initial values of soil moisture in deep layer\n        eu: Evaporation of the upper layer; it isn't used in this function\n        el: Evaporation of the lower layer\n        ed: Evaporation of the deep layer\n        pe: Net precipitation; it is able to be negative value in this function\n        r: Runoff\n\n    Returns:\n        tuple[np.array, np.array, np.array]: wu,wl,wd -- soil moisture in upper, lower and deep layer\n    \"\"\"\n    # pe&gt;0: the upper soil moisture was added firstly, then lower layer, and the final is deep layer\n    # pe&lt;=0: no additional water, just remove evapotranspiration,\n    # but note the case: e &gt;= p &gt; 0\n    # (1) if wu0 + p &gt; e, then e = eu (2) else, wu must be zero\n    wu = np.where(\n        pe &gt; 0.0,\n        np.where(wu0 + pe - r &lt; um, wu0 + pe - r, um),\n        np.where(wu0 + pe &gt; 0.0, wu0 + pe, 0.0),\n    )\n    # calculate wd before wl because it is easier to cal using where statement\n    wd = np.where(\n        pe &gt; 0.0,\n        np.where(\n            wu0 + wl0 + pe - r &gt; um + lm,\n            wu0 + wl0 + wd0 + pe - r - um - lm,\n            wd0,\n        ),\n        wd0 - ed,\n    )\n    # water balance (equation 2.2 in Page 13, also shown in Page 23)\n    # if wu0 + p &gt; e, then e = eu; else p must be used in upper layer,\n    # so no matter what the case is, el didn't include p, neither ed\n    wl = np.where(pe &gt; 0.0, wu0 + wl0 + wd0 + pe - r - wu - wd, wl0 - el)\n    # the water storage should be in reasonable range\n    wu_ = np.clip(wu, a_min=0.0, a_max=um)\n    wl_ = np.clip(wl, a_min=0.0, a_max=lm)\n    wd_ = np.clip(wd, a_min=0.0, a_max=dm)\n    return wu_, wl_, wd_\n</code></pre>"},{"location":"models/xaj/#hydromodel.models.xaj.generation","title":"<code>generation(p_and_e, k, b, im, um, lm, dm, c, wu0=None, wl0=None, wd0=None)</code>","text":"<p>Single-step runoff generation in XAJ.</p> <p>Parameters:</p> Name Type Description Default <code>p_and_e</code> <p>Precipitation and potential evapotranspiration</p> required <code>k</code> <p>Ratio of potential evapotranspiration to reference crop evaporation</p> required <code>b</code> <p>Exponent parameter</p> required <code>um</code> <p>Average soil moisture storage capacity of the upper layer</p> required <code>lm</code> <p>Average soil moisture storage capacity of the lower layer</p> required <code>dm</code> <p>Average soil moisture storage capacity of the deep layer</p> required <code>im</code> <p>Impermeability coefficient</p> required <code>c</code> <p>Coefficient of deep layer</p> required <code>wu0</code> <p>Initial values of soil moisture in upper layer</p> <code>None</code> <code>wl0</code> <p>Initial values of soil moisture in lower layer</p> <code>None</code> <code>wd0</code> <p>Initial values of soil moisture in deep layer</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>tuple[tuple, tuple]: (r, rim, e, pe), (wu, wl, wd); all variables are np.array</p> Source code in <code>hydromodel/models/xaj.py</code> <pre><code>def generation(\n    p_and_e, k, b, im, um, lm, dm, c, wu0=None, wl0=None, wd0=None\n) -&gt; tuple:\n    \"\"\"Single-step runoff generation in XAJ.\n\n    Args:\n        p_and_e: Precipitation and potential evapotranspiration\n        k: Ratio of potential evapotranspiration to reference crop evaporation\n        b: Exponent parameter\n        um: Average soil moisture storage capacity of the upper layer\n        lm: Average soil moisture storage capacity of the lower layer\n        dm: Average soil moisture storage capacity of the deep layer\n        im: Impermeability coefficient\n        c: Coefficient of deep layer\n        wu0: Initial values of soil moisture in upper layer\n        wl0: Initial values of soil moisture in lower layer\n        wd0: Initial values of soil moisture in deep layer\n\n    Returns:\n        tuple[tuple, tuple]: (r, rim, e, pe), (wu, wl, wd); all variables are np.array\n    \"\"\"\n    # make sure physical variables' value ranges are correct\n    prcp = np.maximum(p_and_e[:, 0], 0.0)\n    # get potential evapotranspiration\n    pet = np.maximum(p_and_e[:, 1] * k, 0.0)\n    # wm\n    wm = um + lm + dm\n    if wu0 is None:\n        # just an initial value\n        wu0 = 0.6 * um\n    if wl0 is None:\n        wl0 = 0.6 * lm\n    if wd0 is None:\n        wd0 = 0.6 * dm\n    w0_ = wu0 + wl0 + wd0\n\n    # w0 need locate in correct range so that following calculation could be right\n    # To make sure float data's calculation is correct, we'd better minus a precision (1e-5)\n    w0 = np.minimum(w0_, wm - 1e-5)\n\n    # Calculate the amount of evaporation from storage\n    eu, el, ed = calculate_evap(lm, c, wu0, wl0, prcp, pet)\n    e = eu + el + ed\n\n    # Calculate the runoff generated by net precipitation\n    prcp_difference = prcp - e\n    pe = np.maximum(prcp_difference, 0.0)\n    r, rim = calculate_prcp_runoff(b, im, wm, w0, pe)\n    # Update wu, wl, wd\n    wu, wl, wd = calculate_w_storage(\n        um, lm, dm, wu0, wl0, wd0, eu, el, ed, prcp_difference, r\n    )\n\n    return (r, rim, e, pe), (wu, wl, wd)\n</code></pre>"},{"location":"models/xaj/#hydromodel.models.xaj.linear_reservoir","title":"<code>linear_reservoir(x, weight, last_y=None)</code>","text":"<p>Linear reservoir's release function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The input to the linear reservoir</p> required <code>weight</code> <p>The coefficient of linear reservoir</p> required <code>last_y</code> <p>The output of last period</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>np.array: One-step forward result</p> Source code in <code>hydromodel/models/xaj.py</code> <pre><code>def linear_reservoir(x, weight, last_y=None) -&gt; np.array:\n    \"\"\"Linear reservoir's release function.\n\n    Args:\n        x: The input to the linear reservoir\n        weight: The coefficient of linear reservoir\n        last_y: The output of last period\n\n    Returns:\n        np.array: One-step forward result\n    \"\"\"\n    weight1 = 1 - weight\n    if last_y is None:\n        last_y = np.full(weight.shape, 0.001)\n    return weight * last_y + weight1 * x\n</code></pre>"},{"location":"models/xaj/#hydromodel.models.xaj.sources","title":"<code>sources(pe, r, sm, ex, ki, kg, s0=None, fr0=None, book='HF')</code>","text":"<p>Divide the runoff to different sources.</p> <p>We use the initial version from the paper of the inventor of the XAJ model -- Prof. Renjun Zhao: \"Analysis of parameters of the XinAnJiang model\". Its Chinese name is &lt;&lt;\u65b0\u5b89\u6c5f\u6a21\u578b\u53c2\u6570\u7684\u5206\u6790&gt;&gt;, which could be found by searching in \"Baidu Xueshu\". The module's code can also be found in \"Watershed Hydrologic Simulation\" (WHS) Page 174. It is nearly same with that in \"Hydrologic Forecasting\" (HF) Page 148-149 We use the period average runoff as input and the unit period is day so we don't need to difference it as books show</p> <p>We also provide code for formula from\u300a\u6c34\u6587\u9884\u62a5\u300b\"Hydrologic Forecasting\" (HF) the fifth version. Page 40-41 and 150-151; the procedures in \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300b\"Engineering Hydrology\" (EH) the third version are different we also provide. they are in the \"sources5mm\" function.</p> <p>Parameters:</p> Name Type Description Default <code>pe</code> <p>Net precipitation</p> required <code>r</code> <p>Runoff from xaj_generation</p> required <code>sm</code> <p>Areal mean free water capacity of the surface layer</p> required <code>ex</code> <p>Exponent of the free water capacity curve</p> required <code>ki</code> <p>Outflow coefficients of the free water storage to interflow relationships</p> required <code>kg</code> <p>Outflow coefficients of the free water storage to groundwater relationships</p> required <code>s0</code> <p>Free water capacity of last period</p> <code>None</code> <code>fr0</code> <p>Runoff area of last period</p> <code>None</code> <code>book</code> <p>The methods implementation to use (\"HF\" or \"EH\")</p> <code>'HF'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>tuple[tuple, tuple]: rs -- surface runoff; ri-- interflow runoff; rg -- groundwater runoff; s1 -- final free water capacity; all variables are numpy array</p> Source code in <code>hydromodel/models/xaj.py</code> <pre><code>def sources(pe, r, sm, ex, ki, kg, s0=None, fr0=None, book=\"HF\") -&gt; tuple:\n    \"\"\"Divide the runoff to different sources.\n\n    We use the initial version from the paper of the inventor of the XAJ model -- Prof. Renjun Zhao:\n    \"Analysis of parameters of the XinAnJiang model\". Its Chinese name is &lt;&lt;\u65b0\u5b89\u6c5f\u6a21\u578b\u53c2\u6570\u7684\u5206\u6790&gt;&gt;,\n    which could be found by searching in \"Baidu Xueshu\".\n    The module's code can also be found in \"Watershed Hydrologic Simulation\" (WHS) Page 174.\n    It is nearly same with that in \"Hydrologic Forecasting\" (HF) Page 148-149\n    We use the period average runoff as input and the unit period is day so we don't need to difference it as books show\n\n    We also provide code for formula from\u300a\u6c34\u6587\u9884\u62a5\u300b\"Hydrologic Forecasting\" (HF) the fifth version. Page 40-41 and 150-151;\n    the procedures in \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300b\"Engineering Hydrology\" (EH) the third version are different we also provide.\n    they are in the \"sources5mm\" function.\n\n    Args:\n        pe: Net precipitation\n        r: Runoff from xaj_generation\n        sm: Areal mean free water capacity of the surface layer\n        ex: Exponent of the free water capacity curve\n        ki: Outflow coefficients of the free water storage to interflow relationships\n        kg: Outflow coefficients of the free water storage to groundwater relationships\n        s0: Free water capacity of last period\n        fr0: Runoff area of last period\n        book: The methods implementation to use (\"HF\" or \"EH\")\n\n    Returns:\n        tuple[tuple, tuple]: rs -- surface runoff; ri-- interflow runoff; rg -- groundwater runoff;\n            s1 -- final free water capacity; all variables are numpy array\n    \"\"\"\n    # maximum free water storage capacity in a basin\n    ms = sm * (1.0 + ex)\n    if fr0 is None:\n        fr0 = 0.1\n    if s0 is None:\n        s0 = 0.5 * sm\n    # For free water storage, because s is related to fr and s0 and fr0 are both values of last period,\n    # we have to trans the initial value of s from last period to this one.\n    # both WHS\uff08\u6d41\u57df\u6c34\u6587\u6a21\u62df\uff09's sample code and HF\uff08\u6c34\u6587\u9884\u62a5\uff09 use s = fr0 * s0 / fr.\n    # I think they both think free water reservoir as a cubic tank. Its height is s and area of bottom rectangle is fr\n    # we will have a cubic tank with varying bottom and height,\n    # and fixed boundary (in HF sm is fixed) or none-fixed boundary (in EH smmf is not fixed)\n    # but notice r's list like\" [1,0] which 1 is the 1st period's runoff and 0 is the 2nd period's runoff\n    # after 1st period, the s1 could not be zero, but in the 2nd period, fr=0, then we cannot set s=0, because some water still in the tank\n    # fr's formula could be found in Eq. 9 in \"Analysis of parameters of the XinAnJiang model\",\n    # Here our r doesn't include rim, so there is no need to remove rim from r; this is also the method in \u300a\u6c34\u6587\u9884\u62a5\u300b\uff08HF\uff09\n\n    # NOTE: when r is 0, fr should be 0, however, s1 may not be zero and it still hold some water,\n    # then fr can not be 0, otherwise when fr is used as denominator it lead to error,\n    # so we have to deal with this case later, for example, when r=0, we cannot use pe * fr to replace r\n    # because fr get the value of last period, and it is not 0\n    fr = np.copy(fr0)\n    if any(fr == 0.0):\n        raise ArithmeticError(\n            \"Please check fr's value, fr==0.0 will cause error in the next step!\"\n        )\n    # r=0, then r/pe must be 0\n    fr_mask = r &gt; 0.0\n    fr[fr_mask] = r[fr_mask] / pe[fr_mask]\n    if np.isnan(fr).any():\n        raise ArithmeticError(\"Please check pe's data! there may be 0.0\")\n\n    # if fr=0, then we cannot get ss, but ss should not be 0, because s1 of last period may not be 0 and it still hold some water\n    ss = np.copy(s0)\n    # same initialization for s\n    s = np.copy(s0)\n\n    ss[fr_mask] = fr0[fr_mask] * s0[fr_mask] / fr[fr_mask]\n\n    if book == \"HF\":\n        ss = np.minimum(ss, sm)\n        au = ms * (1.0 - (1.0 - ss / sm) ** (1.0 / (1.0 + ex)))\n        if np.isnan(au).any():\n            raise ValueError(\n                \"Error: NaN values detected. Try set clip function or check your data!!!\"\n            )\n        # the first condition should be r &gt; 0.0, when r=0, rs must be 0, fig 6-12 in EH or fig 5-4 in HF\n        # so we have to use \"pe\" carefully!!! when r&gt;0.0, we use pe, otherwise we don't use it!!!\n        rs = np.full(r.shape, 0.0)\n        rs[fr_mask] = np.where(\n            pe[fr_mask] + au[fr_mask] &lt; ms[fr_mask],\n            # equation 2-85 in HF\n            fr[fr_mask]\n            * (\n                pe[fr_mask]\n                - sm[fr_mask]\n                + ss[fr_mask]\n                + sm[fr_mask]\n                * (\n                    (\n                        1\n                        - np.minimum(pe[fr_mask] + au[fr_mask], ms[fr_mask])\n                        / ms[fr_mask]\n                    )\n                    ** (1 + ex[fr_mask])\n                )\n            ),\n            # equation 2-86 in HF\n            fr[fr_mask] * (pe[fr_mask] + ss[fr_mask] - sm[fr_mask]),\n        )\n        rs = np.minimum(rs, r)\n\n        # ri's mask is not same as rs's, because last period's s may not be 0\n        # and in this time, ri and rg could be larger than 0\n        # we need firstly calculate the updated s, s's mask is same as fr_mask,\n        # when r==0, then s will be equal to last period's\n        # equation 2-87 in HF, some free water leave or save, so we update free water storage\n        s[fr_mask] = ss[fr_mask] + (r[fr_mask] - rs[fr_mask]) / fr[fr_mask]\n        s = np.minimum(s, sm)\n        if np.isnan(s).any():\n            raise ArithmeticError(\"Please check fr's data! there may be 0.0\")\n\n    elif book == \"EH\":\n        # smmf should be with correpond with s\n        smmf = ms * (1 - (1 - fr) ** (1 / ex))\n        smf = smmf / (1 + ex)\n        ss = np.minimum(ss, smf)\n        au = smmf * (1 - (1 - ss / smf) ** (1 / (1 + ex)))\n        if np.isnan(au).any():\n            raise ValueError(\n                \"Error: NaN values detected. Try set clip function or check your data!!!\"\n            )\n\n        # rs's mask is keep with fr_mask\n        rs = np.full(r.shape, 0.0)\n        rs[fr_mask] = np.where(\n            pe[fr_mask] + au[fr_mask] &lt; smmf[fr_mask],\n            (\n                pe[fr_mask]\n                - smf[fr_mask]\n                + ss[fr_mask]\n                + smf[fr_mask]\n                * (\n                    1\n                    - np.minimum(pe[fr_mask] + au[fr_mask], smmf[fr_mask])\n                    / smmf[fr_mask]\n                )\n                ** (ex[fr_mask] + 1)\n            )\n            * fr[fr_mask],\n            (pe[fr_mask] + ss[fr_mask] - smf[fr_mask]) * fr[fr_mask],\n        )\n        rs = np.minimum(rs, r)\n        s[fr_mask] = ss[fr_mask] + (r[fr_mask] - rs[fr_mask]) / fr[fr_mask]\n        s = np.minimum(s, smf)\n    else:\n        raise ValueError(\"Please set book as 'HF' or 'EH'!\")\n    # the following part is same for both HF and EH. Even the formula is different, but their meaning is same\n    # equation 2-88 in HF, next interflow and ground water will be released from the updated free water storage\n    # We use the period average runoff as input and the general unit period is day.\n    # Hence, we directly use ki and kg rather than ki_{\u0394t} in books.\n    ri = ki * s * fr\n    rg = kg * s * fr\n    # ri = np.where(s &lt; PRECISION, np.full(r.shape, 0.0), ki * s * fr)\n    # rg = np.where(s &lt; PRECISION, np.full(r.shape, 0.0), kg * s * fr)\n    # equation 2-89 in HF; although it looks different with that in WHS, they are actually same\n    # Finally, calculate the final free water storage\n    s1 = s * (1 - ki - kg)\n    # s1 = np.where(s1 &lt; PRECISION, np.full(s1.shape, 0.0), s1)\n    return (rs, ri, rg), (s1, fr)\n</code></pre>"},{"location":"models/xaj/#hydromodel.models.xaj.sources5mm","title":"<code>sources5mm(pe, runoff, sm, ex, ki, kg, s0=None, fr0=None, time_interval_hours=1, book='HF')</code>","text":"<p>Divide the runoff to different sources according to books -- \u300a\u6c34\u6587\u9884\u62a5\u300bHF 5th edition and \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300bEH 3rd edition.</p> <p>Parameters:</p> Name Type Description Default <code>pe</code> <p>Net precipitation</p> required <code>runoff</code> <p>Runoff from xaj_generation</p> required <code>sm</code> <p>Areal mean free water capacity of the surface layer</p> required <code>ex</code> <p>Exponent of the free water capacity curve</p> required <code>ki</code> <p>Outflow coefficients of the free water storage to interflow relationships</p> required <code>kg</code> <p>Outflow coefficients of the free water storage to groundwater relationships</p> required <code>s0</code> <p>Initial free water capacity</p> <code>None</code> <code>fr0</code> <p>Initial area of generation</p> <code>None</code> <code>time_interval_hours</code> <p>\u7531\u4e8eKi\u3001Kg\u3001Ci\u3001Cg\u90fd\u662f\u4ee524\u5c0f\u65f6\u4e3a\u65f6\u6bb5\u957f\u5b9a\u4e49\u7684,\u9700\u6839\u636e\u65f6\u6bb5\u957f\u8f6c\u6362</p> <code>1</code> <code>book</code> <p>The methods in \u300a\u6c34\u6587\u9884\u62a5\u300bHF 5th edition and \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300bEH 3rd edition are different, hence, both are provided, and the default is the former -- \"ShuiWenYuBao\"; the other one is \"GongChengShuiWenXue\"</p> <code>'HF'</code> <p>Returns:</p> Type Description <p>tuple[tuple, tuple]: rs_s -- surface runoff; rss_s-- interflow runoff; rg_s -- groundwater runoff; (fr_ds[-1], s_ds[-1]): state variables' final value; all variables are numpy array</p> Source code in <code>hydromodel/models/xaj.py</code> <pre><code>def sources5mm(\n    pe,\n    runoff,\n    sm,\n    ex,\n    ki,\n    kg,\n    s0=None,\n    fr0=None,\n    time_interval_hours=1,\n    book=\"HF\",\n):\n    \"\"\"Divide the runoff to different sources according to books -- \u300a\u6c34\u6587\u9884\u62a5\u300bHF 5th edition and \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300bEH 3rd edition.\n\n    Args:\n        pe: Net precipitation\n        runoff: Runoff from xaj_generation\n        sm: Areal mean free water capacity of the surface layer\n        ex: Exponent of the free water capacity curve\n        ki: Outflow coefficients of the free water storage to interflow relationships\n        kg: Outflow coefficients of the free water storage to groundwater relationships\n        s0: Initial free water capacity\n        fr0: Initial area of generation\n        time_interval_hours: \u7531\u4e8eKi\u3001Kg\u3001Ci\u3001Cg\u90fd\u662f\u4ee524\u5c0f\u65f6\u4e3a\u65f6\u6bb5\u957f\u5b9a\u4e49\u7684,\u9700\u6839\u636e\u65f6\u6bb5\u957f\u8f6c\u6362\n        book: The methods in \u300a\u6c34\u6587\u9884\u62a5\u300bHF 5th edition and \u300a\u5de5\u7a0b\u6c34\u6587\u5b66\u300bEH 3rd edition are different,\n            hence, both are provided, and the default is the former -- \"ShuiWenYuBao\";\n            the other one is \"GongChengShuiWenXue\"\n\n    Returns:\n        tuple[tuple, tuple]: rs_s -- surface runoff; rss_s-- interflow runoff; rg_s -- groundwater runoff;\n            (fr_ds[-1], s_ds[-1]): state variables' final value; all variables are numpy array\n    \"\"\"\n    # Convert Ki and Kg according to the time interval, as they are defined based on a 24-hour time interval\n    hours_per_day = 24\n    # Non-divisible case, add 1 to the period\n    residue_temp = hours_per_day % time_interval_hours\n    if residue_temp != 0:\n        residue_temp = 1\n    period_num_1d = int(hours_per_day / time_interval_hours) + residue_temp\n    # When kss+kg&gt;1, the square root becomes a complex number during even root calculation, which will cause an error here.\n    # Also, be aware that the denominator may be 0, kss cannot be 0.\n    # Restrict the value of kss+kg.\n    kss_period = (1 - (1 - (ki + kg)) ** (1 / period_num_1d)) / (1 + kg / ki)\n    kg_period = kss_period * kg / ki\n\n    # Maximum free water storage capacity depth of the basin\n    smm = sm * (1 + ex)\n    if s0 is None:\n        s0 = 0.50 * sm\n    if fr0 is None:\n        fr0 = 0.1\n    # don't use np.where here, because it will cause some warning\n    fr = np.copy(fr0)\n    fr_mask = runoff &gt; 0.0\n    fr[fr_mask] = runoff[fr_mask] / pe[fr_mask]\n    if np.all(runoff &lt; 5):\n        n = 1\n    else:\n        # when modeling multiple basins, the number of divides is not the same, so we use the maximum number\n        r_max = np.max(runoff)\n        residue_temp = r_max % 5\n        if residue_temp != 0:\n            residue_temp = 1\n        n = int(r_max / 5) + residue_temp\n    rn = runoff / n\n    pen = pe / n\n    kss_d = (1 - (1 - (kss_period + kg_period)) ** (1 / n)) / (\n        1 + kg_period / kss_period\n    )\n    kg_d = kss_d * kg_period / kss_period\n    # kss_d = kss_period\n    # kg_d = kg_period\n    rs = np.full(runoff.shape, 0.0)\n    rss = np.full(runoff.shape, 0.0)\n    rg = np.full(runoff.shape, 0.0)\n\n    s_ds = []\n    fr_ds = []\n    s_ds.append(s0)\n    fr_ds.append(fr0)\n\n    for j in range(n):\n        fr0_d = fr_ds[j]\n        s0_d = s_ds[j]\n        # equation 5-32 in HF, but strange, cause each period, rn/pen is same, fr_d should be same\n        # fr_d = 1 - (1 - fr) ** (1 / n)\n        fr_d = fr\n\n        ss_d = np.copy(s0_d)\n        s_d = np.copy(s0_d)\n        s1_d = np.copy(s0_d)\n\n        ss_d[fr_mask] = fr0_d[fr_mask] * s0_d[fr_mask] / fr_d[fr_mask]\n\n        if book == \"HF\":\n            ss_d = np.minimum(ss_d, sm)\n            # ms = smm\n            au = smm * (1.0 - (1.0 - ss_d / sm) ** (1.0 / (1.0 + ex)))\n            if np.isnan(au).any():\n                raise ValueError(\n                    \"Error: NaN values detected. Try set clip function or check your data!!!\"\n                )\n            rs_j = np.full(rn.shape, 0.0)\n            rs_j[fr_mask] = np.where(\n                pen[fr_mask] + au[fr_mask] &lt; smm[fr_mask],\n                # equation 5-26 in HF\n                fr_d[fr_mask]\n                * (\n                    pen[fr_mask]\n                    - sm[fr_mask]\n                    + ss_d[fr_mask]\n                    + sm[fr_mask]\n                    * (\n                        (\n                            1\n                            - np.minimum(\n                                pen[fr_mask] + au[fr_mask], smm[fr_mask]\n                            )\n                            / smm[fr_mask]\n                        )\n                        ** (1 + ex[fr_mask])\n                    )\n                ),\n                # equation 5-27 in HF\n                fr_d[fr_mask] * (pen[fr_mask] + ss_d[fr_mask] - sm[fr_mask]),\n            )\n            rs_j = np.minimum(rs_j, rn)\n            s_d[fr_mask] = (\n                ss_d[fr_mask] + (rn[fr_mask] - rs_j[fr_mask]) / fr_d[fr_mask]\n            )\n            s_d = np.minimum(s_d, sm)\n\n        elif book == \"EH\":\n            smmf = smm * (1 - (1 - fr_d) ** (1 / ex))\n            smf = smmf / (1 + ex)\n            ss_d = np.minimum(ss_d, smf)\n            au = smmf * (1 - (1 - ss_d / smf) ** (1 / (1 + ex)))\n            if np.isnan(au).any():\n                raise ValueError(\n                    \"Error: NaN values detected. Try set clip function or check your data!!!\"\n                )\n            rs_j = np.full(rn.shape, 0.0)\n            rs_j[fr_mask] = np.where(\n                pen[fr_mask] + au[fr_mask] &lt; smmf[fr_mask],\n                (\n                    pen[fr_mask]\n                    - smf[fr_mask]\n                    + ss_d[fr_mask]\n                    + smf[fr_mask]\n                    * (\n                        1\n                        - np.minimum(pen[fr_mask] + au[fr_mask], smmf[fr_mask])\n                        / smmf[fr_mask]\n                    )\n                    ** (ex[fr_mask] + 1)\n                )\n                * fr_d[fr_mask],\n                (pen[fr_mask] + ss_d[fr_mask] - smf[fr_mask]) * fr_d[fr_mask],\n            )\n            rs_j = np.minimum(rs_j, rn)\n            s_d[fr_mask] = (\n                ss_d[fr_mask] + (rn[fr_mask] - rs_j[fr_mask]) / fr_d[fr_mask]\n            )\n            s_d = np.minimum(s_d, smf)\n\n        else:\n            raise NotImplementedError(\n                \"We don't have this implementation! Please chose 'HF' or 'EH'!!\"\n            )\n        rss_j = s_d * kss_d * fr_d\n        rg_j = s_d * kg_d * fr_d\n        s1_d = s_d * (1 - kss_d - kg_d)\n\n        rs = rs + rs_j\n        rss = rss + rss_j\n        rg = rg + rg_j\n        # Assign s_d and fr_d to the arrays as initial values for the next segment\n        s_ds.append(s1_d)\n        fr_ds.append(fr_d)\n\n    return (rs, rss, rg), (s_ds[-1], fr_ds[-1])\n</code></pre>"},{"location":"models/xaj/#hydromodel.models.xaj.uh_gamma","title":"<code>uh_gamma(a, theta, len_uh=15)</code>","text":"<p>A simple two-parameter Gamma distribution as a unit-hydrograph to route instantaneous runoff from a hydrologic model.</p> <p>The method comes from mizuRoute -- http://www.geosci-model-dev.net/9/2223/2016/</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <p>Shape parameter</p> required <code>theta</code> <p>Timescale parameter</p> required <code>len_uh</code> <p>The time length of the unit hydrograph</p> <code>15</code> <p>Returns:</p> Type Description <p>torch.Tensor: The unit hydrograph, dim: [seq, batch, feature]</p> Source code in <code>hydromodel/models/xaj.py</code> <pre><code>def uh_gamma(a, theta, len_uh=15):\n    \"\"\"A simple two-parameter Gamma distribution as a unit-hydrograph to route instantaneous runoff from a hydrologic model.\n\n    The method comes from mizuRoute -- http://www.geosci-model-dev.net/9/2223/2016/\n\n    Args:\n        a: Shape parameter\n        theta: Timescale parameter\n        len_uh: The time length of the unit hydrograph\n\n    Returns:\n        torch.Tensor: The unit hydrograph, dim: [seq, batch, feature]\n    \"\"\"\n    # dims of a: time_seq (same all time steps), batch, feature=1\n    m = a.shape\n    if len_uh &gt; m[0]:\n        raise RuntimeError(\n            \"length of unit hydrograph should be smaller than the whole length of input\"\n        )\n    # aa &gt; 0, here we set minimum 0.1 (min of a is 0, set when calling this func); First dimension of a is repeat\n    aa = np.maximum(0.0, a[0:len_uh, :, :]) + 0.1\n    # theta &gt; 0, here set minimum 0.5\n    theta = np.maximum(0.0, theta[0:len_uh, :, :]) + 0.5\n    # len_f, batch, feature\n    t = np.expand_dims(\n        np.swapaxes(np.tile(np.arange(0.5, len_uh * 1.0), (m[1], 1)), 0, 1),\n        axis=-1,\n    )\n    denominator = gamma(aa) * (theta**aa)\n    # [len_f, m[1], m[2]]\n    w = 1 / denominator * (t ** (aa - 1)) * (np.exp(-t / theta))\n    w = w / w.sum(0)  # scale to 1 for each UH\n    return w\n</code></pre>"},{"location":"models/xaj/#hydromodel.models.xaj.xaj","title":"<code>xaj(p_and_e, params, return_state=False, warmup_length=365, return_warmup_states=False, normalized_params='auto', **kwargs)</code>","text":"<p>Run XAJ model.</p> <p>Parameters:</p> Name Type Description Default <code>p_and_e</code> <p>Prcp and pet; sequence-first (time is the first dim) 3-d np array: [time, basin, feature=2]</p> required <code>params</code> <code>ndarray</code> <p>Parameters of XAJ model for basin(s); 2-dim variable -- [basin, parameter]: the parameters are B IM UM LM DM C SM EX KI KG A THETA CI CG (notice the sequence)</p> required <code>return_state</code> <p>If True, return state values, mainly for warmup periods</p> <code>False</code> <code>warmup_length</code> <p>Hydro models need a warm-up period to get good initial state values</p> <code>365</code> <code>return_warmup_states</code> <p>If True, return initial states after warmup period (default: False) Returns a dict with keys for XAJ state variables</p> <code>False</code> <code>normalized_params</code> <p>Parameter format specification: - \"auto\": automatically detect if parameters are normalized (0-1) or original scale (default) - True: parameters are normalized (0-1 range), will be converted to original scale - False: parameters are already in original scale, use as-is</p> <code>'auto'</code> <code>**kwargs</code> <p>Additional keyword arguments: name: Now we provide two ways: \"xaj\" (route:recession constant + lag time) and \"xaj_mz\" (route:method from mizuRoute) source_type: Default is \"sources\" and it will call \"sources\" function; the other is \"sources5mm\",     and we will divide the runoff to some &lt;5mm pieces according to the books in this case source_book: When source_type is \"sources5mm\" there are two implementions for dividing sources,     as the methods in \"ShuiWenYuBao\" and \"GongChengShuiWenXue\"\" are different.     Hence, both are provided, and the default is the former. kernel_size: The size of the kernel for the convolution operation, default is 15 periods     if time_interval_hours is 1, it is 15 hours; if time_interval_hours is 24, it is 15 days     It is the length of the unit hydrograph time_interval_hours: The time interval of the model, default is 1 hour, for daily case, it should be 24     this is only used when source_type is \"sources5mm\" initial_states (default: None): Dict to override specific initial state values after warmup,     e.g., {\"wu\": 10, \"wl\": 15, \"wd\": 20, \"s\": 5} will set these initial state values for all basins after warmup     Available states: \"wu\" (upper layer), \"wl\" (lower layer), \"wd\" (deep layer), \"s\" (free water storage),     \"fr\" (interflow), \"qi\" (interflow), \"qg\" (groundwater)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[tuple, ndarray, tuple[tuple, dict]]</code> <p>tuple or np.ndarray: Depends on return_state and return_warmup_states parameters:</p> <ul> <li> <p>return_state=False, return_warmup_states=False:   (q_sim, es) - streamflow and evapotranspiration</p> </li> <li> <p>return_state=False, return_warmup_states=True:   ((q_sim, es), warmup_states_dict) where warmup_states_dict contains   XAJ state variables after warmup</p> </li> <li> <p>return_state=True, return_warmup_states=False:   (q_sim, es, wu, wl, wd, s, fr, qi, qg) - full state variables</p> </li> <li> <p>return_state=True, return_warmup_states=True:   (q_sim, es, wu, wl, wd, s, fr, qi, qg, warmup_states_dict)</p> </li> </ul> Source code in <code>hydromodel/models/xaj.py</code> <pre><code>def xaj(\n    p_and_e,\n    params: np.ndarray,\n    return_state=False,\n    warmup_length=365,\n    return_warmup_states=False,\n    normalized_params=\"auto\",\n    **kwargs,\n) -&gt; Union[\n    tuple,  # (q_sim, es) - standard case\n    np.ndarray,  # This shouldn't happen but kept for legacy\n    tuple[\n        tuple, dict\n    ],  # ((q_sim, es), warmup_states) - return_state=False, return_warmup_states=True\n]:\n    \"\"\"Run XAJ model.\n\n    Args:\n        p_and_e: Prcp and pet; sequence-first (time is the first dim) 3-d np array: [time, basin, feature=2]\n        params: Parameters of XAJ model for basin(s);\n            2-dim variable -- [basin, parameter]:\n            the parameters are B IM UM LM DM C SM EX KI KG A THETA CI CG (notice the sequence)\n        return_state: If True, return state values, mainly for warmup periods\n        warmup_length: Hydro models need a warm-up period to get good initial state values\n        return_warmup_states: If True, return initial states after warmup period (default: False)\n            Returns a dict with keys for XAJ state variables\n        normalized_params: Parameter format specification:\n            - \"auto\": automatically detect if parameters are normalized (0-1) or original scale (default)\n            - True: parameters are normalized (0-1 range), will be converted to original scale\n            - False: parameters are already in original scale, use as-is\n        **kwargs: Additional keyword arguments:\n            name: Now we provide two ways: \"xaj\" (route:recession constant + lag time) and \"xaj_mz\" (route:method from mizuRoute)\n            source_type: Default is \"sources\" and it will call \"sources\" function; the other is \"sources5mm\",\n                and we will divide the runoff to some &lt;5mm pieces according to the books in this case\n            source_book: When source_type is \"sources5mm\" there are two implementions for dividing sources,\n                as the methods in \"ShuiWenYuBao\" and \"GongChengShuiWenXue\"\" are different.\n                Hence, both are provided, and the default is the former.\n            kernel_size: The size of the kernel for the convolution operation, default is 15 periods\n                if time_interval_hours is 1, it is 15 hours; if time_interval_hours is 24, it is 15 days\n                It is the length of the unit hydrograph\n            time_interval_hours: The time interval of the model, default is 1 hour, for daily case, it should be 24\n                this is only used when source_type is \"sources5mm\"\n            initial_states (default: None): Dict to override specific initial state values after warmup,\n                e.g., {\"wu\": 10, \"wl\": 15, \"wd\": 20, \"s\": 5} will set these initial state values for all basins after warmup\n                Available states: \"wu\" (upper layer), \"wl\" (lower layer), \"wd\" (deep layer), \"s\" (free water storage),\n                \"fr\" (interflow), \"qi\" (interflow), \"qg\" (groundwater)\n\n    Returns:\n        tuple or np.ndarray: Depends on return_state and return_warmup_states parameters:\n\n            - return_state=False, return_warmup_states=False:\n              (q_sim, es) - streamflow and evapotranspiration\n\n            - return_state=False, return_warmup_states=True:\n              ((q_sim, es), warmup_states_dict) where warmup_states_dict contains\n              XAJ state variables after warmup\n\n            - return_state=True, return_warmup_states=False:\n              (q_sim, es, wu, wl, wd, s, fr, qi, qg) - full state variables\n\n            - return_state=True, return_warmup_states=True:\n              (q_sim, es, wu, wl, wd, s, fr, qi, qg, warmup_states_dict)\n    \"\"\"\n    # default values for some function parameters\n    model_name = kwargs.get(\"name\", \"xaj\")\n    source_type = kwargs.get(\"source_type\", \"sources\")\n    source_book = kwargs.get(\"source_book\", \"HF\")\n    kernel_size = kwargs.get(\"kernel_size\", 15)\n    time_interval_hours = kwargs.get(\"time_interval_hours\", 24)\n    model_param_dict = kwargs.get(f\"{model_name}\", None)\n    if model_param_dict is None:\n        model_param_dict = MODEL_PARAM_DICT[f\"{model_name}\"]\n    # params\n    param_ranges = model_param_dict[\"param_range\"]\n    if model_name == \"xaj\":\n        route_method = \"CSL\"\n    elif model_name == \"xaj_mz\":\n        route_method = \"MZ\"\n    else:\n        raise NotImplementedError(\n            \"We don't provide this route method now! Please use 'CSL' or 'MZ'!\"\n        )\n    if np.isnan(params).any():\n        raise ValueError(\n            \"Parameters contain NaN values. Please check your opt algorithm\"\n        )\n    # Process parameters using unified parameter handling\n    processed_params = process_parameters(\n        params, param_ranges, normalized=normalized_params\n    )\n\n    # Extract individual parameters from processed array\n    k = processed_params[:, 0]\n    b = processed_params[:, 1]\n    im = processed_params[:, 2]\n    um = processed_params[:, 3]\n    lm = processed_params[:, 4]\n    dm = processed_params[:, 5]\n    c = processed_params[:, 6]\n    sm = processed_params[:, 7]\n    ex = processed_params[:, 8]\n    ki_ = processed_params[:, 9]\n    kg_ = processed_params[:, 10]\n\n    # ki+kg should be smaller than 1; if not, we scale them\n    ki = np.where(ki_ + kg_ &lt; 1.0, ki_, (1.0 - PRECISION) / (ki_ + kg_) * ki_)\n    kg = np.where(ki_ + kg_ &lt; 1.0, kg_, (1.0 - PRECISION) / (ki_ + kg_) * kg_)\n\n    if route_method == \"CSL\":\n        cs = processed_params[:, 11]\n        l = processed_params[:, 12]\n    elif route_method == \"MZ\":\n        # we will use routing method from mizuRoute -- http://www.geosci-model-dev.net/9/2223/2016/\n        a = processed_params[:, 11]\n        theta = processed_params[:, 12]\n    else:\n        raise NotImplementedError(\n            \"We don't provide this route method now! Please use 'CSL' or 'MZ'!\"\n        )\n\n    ci = processed_params[:, 13]\n    cg = processed_params[:, 14]\n\n    # initialize state values\n    if warmup_length &gt; 0:\n        p_and_e_warmup = p_and_e[0:warmup_length, :, :]\n        # Remove initial_states from kwargs for warmup period to avoid applying override during warmup\n        warmup_kwargs = {\n            k: v for k, v in kwargs.items() if k != \"initial_states\"\n        }\n        _q, _e, *w0, s0, fr0, qi0, qg0 = xaj(\n            p_and_e_warmup,\n            params,\n            return_state=True,\n            warmup_length=0,\n            **warmup_kwargs,\n        )\n    else:\n        w0 = (0.5 * um, 0.5 * lm, 0.5 * dm)\n        s0 = 0.5 * sm\n        fr0 = np.full(ex.shape, 0.1)\n        qi0 = np.full(ci.shape, 0.1)\n        qg0 = np.full(cg.shape, 0.1)\n\n    # Apply initial state overrides if provided (only after warmup in main call)\n    # TODO: not fully tested yet\n    initial_states = kwargs.get(\"initial_states\", None)\n    if initial_states is not None:\n        # Only apply initial_states when we just finished a warmup period or no warmup\n        if \"wu\" in initial_states:\n            w0 = (initial_states[\"wu\"] * np.ones_like(w0[0]), w0[1], w0[2])\n        if \"wl\" in initial_states:\n            w0 = (w0[0], initial_states[\"wl\"] * np.ones_like(w0[1]), w0[2])\n        if \"wd\" in initial_states:\n            w0 = (w0[0], w0[1], initial_states[\"wd\"] * np.ones_like(w0[2]))\n        if \"s\" in initial_states:\n            s0.fill(initial_states[\"s\"])\n        if \"fr\" in initial_states:\n            fr0.fill(initial_states[\"fr\"])\n        if \"qi\" in initial_states:\n            qi0.fill(initial_states[\"qi\"])\n        if \"qg\" in initial_states:\n            qg0.fill(initial_states[\"qg\"])\n\n    # Save warmup states before applying overrides (for return_warmup_states)\n    # NOTE: this part must be set after the initial_states override, otherwise override initial states will be ignored\n    warmup_states = None\n    if return_warmup_states:\n        warmup_states = {\n            \"wu\": w0[0].copy(),  # Upper layer moisture [basin] array\n            \"wl\": w0[1].copy(),  # Lower layer moisture [basin] array\n            \"wd\": w0[2].copy(),  # Deep layer moisture [basin] array\n            \"s\": s0.copy(),  # Free water storage [basin] array\n            \"fr\": fr0.copy(),  # Runoff fraction [basin] array\n            \"qi\": qi0.copy(),  # Interflow [basin] array\n            \"qg\": qg0.copy(),  # Groundwater flow [basin] array\n        }\n\n    # state_variables\n    inputs = p_and_e[warmup_length:, :, :]\n    runoff_ims_ = np.full(inputs.shape[:2], 0.0)\n    rss_ = np.full(inputs.shape[:2], 0.0)\n    ris_ = np.full(inputs.shape[:2], 0.0)\n    rgs_ = np.full(inputs.shape[:2], 0.0)\n    es_ = np.full(inputs.shape[:2], 0.0)\n    for i in range(inputs.shape[0]):\n        if i == 0:\n            (r, rim, e, pe), w = generation(\n                inputs[i, :, :], k, b, im, um, lm, dm, c, *w0\n            )\n            if source_type == \"sources\":\n                (rs, ri, rg), (s, fr) = sources(\n                    pe, r, sm, ex, ki, kg, s0, fr0, book=source_book\n                )\n            elif source_type == \"sources5mm\":\n                (rs, ri, rg), (s, fr) = sources5mm(\n                    pe,\n                    r,\n                    sm,\n                    ex,\n                    ki,\n                    kg,\n                    s0,\n                    fr0,\n                    time_interval_hours=time_interval_hours,\n                    book=source_book,\n                )\n            else:\n                raise NotImplementedError(\"No such divide-sources method\")\n        else:\n            (r, rim, e, pe), w = generation(\n                inputs[i, :, :], k, b, im, um, lm, dm, c, *w\n            )\n            if source_type == \"sources\":\n                (rs, ri, rg), (s, fr) = sources(\n                    pe, r, sm, ex, ki, kg, s, fr, book=source_book\n                )\n            elif source_type == \"sources5mm\":\n                (rs, ri, rg), (s, fr) = sources5mm(\n                    pe,\n                    r,\n                    sm,\n                    ex,\n                    ki,\n                    kg,\n                    s,\n                    fr,\n                    time_interval_hours=time_interval_hours,\n                    book=source_book,\n                )\n            else:\n                raise NotImplementedError(\"No such divide-sources method\")\n        # impevious part is pe * im\n        runoff_ims_[i, :] = rim\n        # so for non-imprvious part, the result should be corrected\n        rss_[i, :] = rs * (1 - im)\n        ris_[i, :] = ri * (1 - im)\n        rgs_[i, :] = rg * (1 - im)\n        es_[i, :] = e\n    # seq, batch, feature\n    runoff_im = np.expand_dims(runoff_ims_, axis=2)\n    rss = np.expand_dims(rss_, axis=2)\n    es = np.expand_dims(es_, axis=2)\n\n    qs = np.full(inputs.shape[:2], 0.0)\n    if route_method == \"CSL\":\n        qt = np.full(inputs.shape[:2], 0.0)\n        for i in range(inputs.shape[0]):\n            if i == 0:\n                qi = linear_reservoir(ris_[i], ci, qi0)\n                qg = linear_reservoir(rgs_[i], cg, qg0)\n            else:\n                qi = linear_reservoir(ris_[i], ci, qi)\n                qg = linear_reservoir(rgs_[i], cg, qg)\n            # Don't forget the runoff_im\n            qs_ = rss_[i] + runoff_ims_[i]\n            qt[i, :] = qs_ + qi + qg\n        for j in range(len(l)):\n            lag = int(l[j])\n            # Adjust lag if input is too short (for flood event data)\n            effective_lag = min(lag, inputs.shape[0] - 1)\n            for i in range(effective_lag):\n                qs[i, j] = qt[i, j]\n            for i in range(effective_lag, inputs.shape[0]):\n                qs[i, j] = (\n                    cs[j] * qs[i - 1, j]\n                    + (1 - cs[j]) * qt[i - effective_lag, j]\n                )\n    elif route_method == \"MZ\":\n        rout_a = a.repeat(rss.shape[0]).reshape(rss.shape)\n        rout_b = theta.repeat(rss.shape[0]).reshape(rss.shape)\n        # Adjust kernel_size if input is too short (for flood event data)\n        effective_kernel_size = min(kernel_size, inputs.shape[0])\n        conv_uh = uh_gamma(rout_a, rout_b, effective_kernel_size)\n        qs_ = uh_conv(runoff_im + rss, conv_uh)\n        for i in range(inputs.shape[0]):\n            if i == 0:\n                qi = linear_reservoir(ris_[i], ci, qi0)\n                qg = linear_reservoir(rgs_[i], cg, qg0)\n            else:\n                qi = linear_reservoir(ris_[i], ci, qi)\n                qg = linear_reservoir(rgs_[i], cg, qg)\n            qs[i, :] = qs_[i, :, 0] + qi + qg\n    else:\n        raise NotImplementedError(\n            \"We don't provide this route method now! Please use 'CS' or 'MZ'!\"\n        )\n\n    # seq, batch, feature\n    q_sim = np.expand_dims(qs, axis=2)\n    if return_state:\n        result = (q_sim, es, *w, s, fr, qi, qg)\n        # If warmup states are requested, add them as the last element\n        if return_warmup_states and warmup_states is not None:\n            return result + (warmup_states,)\n        else:\n            return result\n    else:\n        # For non-state return, only return warmup states if specifically requested\n        if return_warmup_states and warmup_states is not None:\n            return (q_sim, es), warmup_states\n        else:\n            return q_sim, es\n</code></pre>"}]}